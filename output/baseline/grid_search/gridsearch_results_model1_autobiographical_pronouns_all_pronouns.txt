
Run from 2023-04-02 10:59:53.135590
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.4896273910999298, val_loss: 0.9489397406578064

('adam', 0.01, 50, 200)
train_loss: 0.482624351978302, val_loss: 0.8796496987342834

('adam', 0.01, 50, 300)
train_loss: 0.4792307913303375, val_loss: 0.9412951469421387

('adam', 0.01, 100, 100)
train_loss: 0.4894230365753174, val_loss: 0.9548073410987854

('adam', 0.01, 100, 200)
train_loss: 0.482484370470047, val_loss: 0.8846715092658997

('adam', 0.01, 100, 300)
train_loss: 0.4813118577003479, val_loss: 0.9719187617301941

('adam', 0.01, 200, 100)
train_loss: 0.48994049429893494, val_loss: 0.9413642883300781

('adam', 0.01, 200, 200)
train_loss: 0.4821079671382904, val_loss: 0.9186830520629883

('adam', 0.01, 200, 300)
train_loss: 0.4817739427089691, val_loss: 0.9776591658592224

('adam', 0.1, 50, 100)
train_loss: 0.4790610671043396, val_loss: 0.9568166732788086

('adam', 0.1, 50, 200)
train_loss: 0.4715902507305145, val_loss: 0.878778874874115

('adam', 0.1, 50, 300)
train_loss: 0.46764761209487915, val_loss: 0.8355288505554199

('adam', 0.1, 100, 100)
train_loss: 0.4751419723033905, val_loss: 0.9160512089729309

('adam', 0.1, 100, 200)
train_loss: 0.4715721607208252, val_loss: 0.8785870671272278

('adam', 0.1, 100, 300)
train_loss: 0.46618813276290894, val_loss: 0.8186354041099548

('adam', 0.1, 200, 100)
train_loss: 0.47518619894981384, val_loss: 0.9163835048675537

('adam', 0.1, 200, 200)
train_loss: 0.47204264998435974, val_loss: 0.8835474848747253

('adam', 0.1, 200, 300)
train_loss: 0.4661328196525574, val_loss: 0.8179890513420105

('adam', 0.2, 50, 100)
train_loss: 0.47232943773269653, val_loss: 0.8829866051673889

('adam', 0.2, 50, 200)
train_loss: 0.46496421098709106, val_loss: 0.8039939403533936

('adam', 0.2, 50, 300)
train_loss: 0.45936864614486694, val_loss: 0.7314119338989258

('adam', 0.2, 100, 100)
train_loss: 0.4737480580806732, val_loss: 0.8985374569892883

('adam', 0.2, 100, 200)
train_loss: 0.4634115695953369, val_loss: 0.7848888039588928

('adam', 0.2, 100, 300)
train_loss: 0.4586111009120941, val_loss: 0.720238208770752

('adam', 0.2, 200, 100)
train_loss: 0.47372427582740784, val_loss: 0.9011515974998474

('adam', 0.2, 200, 200)
train_loss: 0.4646145701408386, val_loss: 0.799715518951416

('adam', 0.2, 200, 300)
train_loss: 0.45846471190452576, val_loss: 0.7180275321006775


---------
BEST MODEL
('adam', 0.2, 200, 300)
val_loss: 0.7180275321006775
---------

Run from 2023-04-05 15:30:42.277563
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7846271395683289, val_loss: 0.6756126284599304

('adam', 0.001, 8, 50)
train_loss: 0.638815701007843, val_loss: 0.6940911412239075

('adam', 0.001, 8, 100)
train_loss: 0.5237196683883667, val_loss: 0.8273966908454895

('adam', 0.001, 16, 10)
train_loss: 0.6391358375549316, val_loss: 0.7149924635887146

('adam', 0.001, 16, 50)
train_loss: 0.6583403944969177, val_loss: 0.6924875378608704

('adam', 0.001, 16, 100)
train_loss: 0.5421958565711975, val_loss: 0.7991349101066589

('adam', 0.001, 32, 10)
train_loss: 0.7634273171424866, val_loss: 0.6787829399108887

('adam', 0.001, 32, 50)
train_loss: 0.666113555431366, val_loss: 0.6937806010246277

('adam', 0.001, 32, 100)
train_loss: 0.6613407135009766, val_loss: 0.6866648197174072

('adam', 0.001, 64, 10)
train_loss: 0.6323458552360535, val_loss: 0.7213807106018066

('adam', 0.001, 64, 50)
train_loss: 0.7104122042655945, val_loss: 0.6837455630302429

('adam', 0.001, 64, 100)
train_loss: 0.5939404368400574, val_loss: 0.7432696223258972

('adam', 0.01, 8, 10)
train_loss: 0.6495124697685242, val_loss: 0.6677908301353455

('adam', 0.01, 8, 50)
train_loss: 0.48105114698410034, val_loss: 0.9069907665252686

('adam', 0.01, 8, 100)
train_loss: 0.47677692770957947, val_loss: 0.9260147213935852

('adam', 0.01, 16, 10)
train_loss: 0.6287639737129211, val_loss: 0.6969466209411621

('adam', 0.01, 16, 50)
train_loss: 0.4870850443840027, val_loss: 0.9189373850822449

('adam', 0.01, 16, 100)
train_loss: 0.4792432188987732, val_loss: 0.8902933597564697

('adam', 0.01, 32, 10)
train_loss: 0.6002723574638367, val_loss: 0.7299518585205078

('adam', 0.01, 32, 50)
train_loss: 0.4932868480682373, val_loss: 0.9007025361061096

('adam', 0.01, 32, 100)
train_loss: 0.4865001440048218, val_loss: 0.8477632403373718

('adam', 0.01, 64, 10)
train_loss: 0.6124043464660645, val_loss: 0.728015661239624

('adam', 0.01, 64, 50)
train_loss: 0.5238255858421326, val_loss: 0.8142761588096619

('adam', 0.01, 64, 100)
train_loss: 0.5183524489402771, val_loss: 0.7700701355934143

('adam', 0.1, 8, 10)
train_loss: 0.48592516779899597, val_loss: 0.9592167735099792

('adam', 0.1, 8, 50)
train_loss: 0.48142126202583313, val_loss: 0.9360256195068359

('adam', 0.1, 8, 100)
train_loss: 0.47160521149635315, val_loss: 0.8803994059562683

('adam', 0.1, 16, 10)
train_loss: 0.4934979975223541, val_loss: 1.1176406145095825

('adam', 0.1, 16, 50)
train_loss: 0.47486788034439087, val_loss: 0.9349480271339417

('adam', 0.1, 16, 100)
train_loss: 0.4735989272594452, val_loss: 0.8633415102958679

('adam', 0.1, 32, 10)
train_loss: 0.4890545904636383, val_loss: 1.1090930700302124

('adam', 0.1, 32, 50)
train_loss: 0.4774806797504425, val_loss: 0.9118382930755615

('adam', 0.1, 32, 100)
train_loss: 0.47517675161361694, val_loss: 0.8933035731315613

('adam', 0.1, 64, 10)
train_loss: 0.5007486939430237, val_loss: 0.8512975573539734

('adam', 0.1, 64, 50)
train_loss: 0.48205527663230896, val_loss: 0.9444051384925842

('adam', 0.1, 64, 100)
train_loss: 0.47629618644714355, val_loss: 0.9249672889709473


---------
BEST MODEL
('adam', 0.01, 8, 10)
val_loss: 0.6677908301353455
---------

Run from 2023-04-05 16:32:05.970312
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7897641658782959, val_loss: 0.6755602955818176

('adam', 0.001, 8, 50)
train_loss: 0.5480839014053345, val_loss: 0.7963132262229919

('adam', 0.001, 8, 100)
train_loss: 0.5977199673652649, val_loss: 0.7011335492134094

('adam', 0.001, 16, 10)
train_loss: 0.6068117022514343, val_loss: 0.7397597432136536

('adam', 0.001, 16, 50)
train_loss: 0.6700853109359741, val_loss: 0.6875516772270203

('adam', 0.001, 16, 100)
train_loss: 0.6411125659942627, val_loss: 0.6877344250679016

('adam', 0.001, 32, 10)
train_loss: 0.7167482972145081, val_loss: 0.685314953327179

('adam', 0.001, 32, 50)
train_loss: 0.6075030565261841, val_loss: 0.7312794327735901

('adam', 0.001, 32, 100)
train_loss: 0.6814628839492798, val_loss: 0.6791095733642578

('adam', 0.001, 64, 10)
train_loss: 0.6362903714179993, val_loss: 0.718758761882782

('adam', 0.001, 64, 50)
train_loss: 0.7700440287590027, val_loss: 0.6758387684822083

('adam', 0.001, 64, 100)
train_loss: 0.6720356941223145, val_loss: 0.6908535361289978

('adam', 0.01, 8, 10)
train_loss: 0.5552344918251038, val_loss: 0.758856475353241

('adam', 0.01, 8, 50)
train_loss: 0.4814084768295288, val_loss: 0.8837725520133972

('adam', 0.01, 8, 100)
train_loss: 0.47880783677101135, val_loss: 0.9506147503852844

('adam', 0.01, 16, 10)
train_loss: 0.598088800907135, val_loss: 0.7210207581520081

('adam', 0.01, 16, 50)
train_loss: 0.49731311202049255, val_loss: 0.8065431714057922

('adam', 0.01, 16, 100)
train_loss: 0.479876309633255, val_loss: 0.9179372787475586

('adam', 0.01, 32, 10)
train_loss: 0.5909257531166077, val_loss: 0.7409601211547852

('adam', 0.01, 32, 50)
train_loss: 0.5135301351547241, val_loss: 0.8022074699401855

('adam', 0.01, 32, 100)
train_loss: 0.48509737849235535, val_loss: 0.852019727230072

('adam', 0.01, 64, 10)
train_loss: 0.577304482460022, val_loss: 0.7654008865356445

('adam', 0.01, 64, 50)
train_loss: 0.5941367745399475, val_loss: 0.6998398900032043

('adam', 0.01, 64, 100)
train_loss: 0.49280261993408203, val_loss: 0.8990388512611389

('adam', 0.1, 8, 10)
train_loss: 0.4853608012199402, val_loss: 0.9643401503562927

('adam', 0.1, 8, 50)
train_loss: 0.48046836256980896, val_loss: 0.9351779818534851

('adam', 0.1, 8, 100)
train_loss: 0.4748362898826599, val_loss: 0.8945872783660889

('adam', 0.1, 16, 10)
train_loss: 0.4821857213973999, val_loss: 1.0087895393371582

('adam', 0.1, 16, 50)
train_loss: 0.47775915265083313, val_loss: 0.9069268703460693

('adam', 0.1, 16, 100)
train_loss: 0.4772816598415375, val_loss: 0.9901692271232605

('adam', 0.1, 32, 10)
train_loss: 0.49328458309173584, val_loss: 1.171554446220398

('adam', 0.1, 32, 50)
train_loss: 0.47788310050964355, val_loss: 0.9671812653541565

('adam', 0.1, 32, 100)
train_loss: 0.48041921854019165, val_loss: 1.0348891019821167

('adam', 0.1, 64, 10)
train_loss: 0.49111688137054443, val_loss: 0.9177260398864746

('adam', 0.1, 64, 50)
train_loss: 0.483295738697052, val_loss: 0.9788083434104919

('adam', 0.1, 64, 100)
train_loss: 0.47770676016807556, val_loss: 0.9392011165618896


---------
BEST MODEL
('adam', 0.001, 8, 10)
val_loss: 0.6755602955818176
---------

Run from 2023-04-10 10:07:24.428368
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.690131425857544, val_loss: 0.6897737383842468

('adam', 0.001, 8, 50)
train_loss: 0.582807183265686, val_loss: 0.7412409782409668

('adam', 0.001, 8, 100)
train_loss: 0.5223811864852905, val_loss: 0.8320034146308899

('adam', 0.001, 16, 10)
train_loss: 0.645936131477356, val_loss: 0.7111213803291321

('adam', 0.001, 16, 50)
train_loss: 0.5944157838821411, val_loss: 0.7382252812385559

('adam', 0.001, 16, 100)
train_loss: 0.5915156006813049, val_loss: 0.7250682711601257

('adam', 0.001, 32, 10)
train_loss: 0.6352940797805786, val_loss: 0.7185471653938293

('adam', 0.001, 32, 50)
train_loss: 0.6328336596488953, val_loss: 0.7116210460662842

('adam', 0.001, 32, 100)
train_loss: 0.5599778890609741, val_loss: 0.777991771697998

('adam', 0.001, 64, 10)
train_loss: 0.6758090853691101, val_loss: 0.698418915271759

('adam', 0.001, 64, 50)
train_loss: 0.6755631566047668, val_loss: 0.6944344639778137

('adam', 0.001, 64, 100)
train_loss: 0.6995890736579895, val_loss: 0.6818273663520813

('adam', 0.01, 8, 10)
train_loss: 0.6074424386024475, val_loss: 0.6948698163032532

('adam', 0.01, 8, 50)
train_loss: 0.48182833194732666, val_loss: 0.9427674412727356

('adam', 0.01, 8, 100)
train_loss: 0.4820985198020935, val_loss: 0.985630214214325

('adam', 0.01, 16, 10)
train_loss: 0.6114189028739929, val_loss: 0.7094411849975586

('adam', 0.01, 16, 50)
train_loss: 0.4877083897590637, val_loss: 0.942936897277832

('adam', 0.01, 16, 100)
train_loss: 0.4796707332134247, val_loss: 0.94549161195755

('adam', 0.01, 32, 10)
train_loss: 0.5654252767562866, val_loss: 0.7737836837768555

('adam', 0.01, 32, 50)
train_loss: 0.4982575476169586, val_loss: 0.8683421015739441

('adam', 0.01, 32, 100)
train_loss: 0.4827786087989807, val_loss: 0.8808608055114746

('adam', 0.01, 64, 10)
train_loss: 0.6485502123832703, val_loss: 0.7031855583190918

('adam', 0.01, 64, 50)
train_loss: 0.5544788241386414, val_loss: 0.7480594515800476

('adam', 0.01, 64, 100)
train_loss: 0.5212397575378418, val_loss: 0.7619755864143372

('adam', 0.1, 8, 10)
train_loss: 0.48581796884536743, val_loss: 0.9875743985176086

('adam', 0.1, 8, 50)
train_loss: 0.47738295793533325, val_loss: 0.9511892199516296

('adam', 0.1, 8, 100)
train_loss: 0.4732584059238434, val_loss: 0.8651048541069031

('adam', 0.1, 16, 10)
train_loss: 0.48500633239746094, val_loss: 1.035071611404419

('adam', 0.1, 16, 50)
train_loss: 0.47883930802345276, val_loss: 0.8965651392936707

('adam', 0.1, 16, 100)
train_loss: 0.47871288657188416, val_loss: 0.992553174495697

('adam', 0.1, 32, 10)
train_loss: 0.4796231985092163, val_loss: 1.027614951133728

('adam', 0.1, 32, 50)
train_loss: 0.4785851836204529, val_loss: 1.014414668083191

('adam', 0.1, 32, 100)
train_loss: 0.4763854742050171, val_loss: 0.9689081311225891

('adam', 0.1, 64, 10)
train_loss: 0.4865482747554779, val_loss: 1.004227876663208

('adam', 0.1, 64, 50)
train_loss: 0.4799734354019165, val_loss: 0.9300288558006287

('adam', 0.1, 64, 100)
train_loss: 0.47986552119255066, val_loss: 0.9620981812477112


---------
BEST MODEL
('adam', 0.001, 64, 100)
val_loss: 0.6818273663520813
---------

Run from 2023-04-10 13:04:21.272064
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.8288193345069885, val_loss: 0.7059624791145325

('adam', 0.001, 8, 50)
train_loss: 0.5253514051437378, val_loss: 0.7972486615180969

('adam', 0.001, 8, 100)
train_loss: 0.5225551724433899, val_loss: 0.7900330424308777

('adam', 0.001, 16, 10)
train_loss: 0.908089816570282, val_loss: 0.7264335751533508

('adam', 0.001, 16, 50)
train_loss: 0.5817455053329468, val_loss: 0.7244122624397278

('adam', 0.001, 16, 100)
train_loss: 0.5556070804595947, val_loss: 0.7434319853782654

('adam', 0.001, 32, 10)
train_loss: 0.9798770546913147, val_loss: 0.749703586101532

('adam', 0.001, 32, 50)
train_loss: 0.9079472422599792, val_loss: 0.7260053753852844

('adam', 0.001, 32, 100)
train_loss: 0.8224958777427673, val_loss: 0.7034750580787659

('adam', 0.001, 64, 10)
train_loss: 0.7659103274345398, val_loss: 0.6960033774375916

('adam', 0.001, 64, 50)
train_loss: 0.5963361263275146, val_loss: 0.7174832224845886

('adam', 0.001, 64, 100)
train_loss: 0.539480984210968, val_loss: 0.7724012732505798

('adam', 0.01, 8, 10)
train_loss: 0.5448310971260071, val_loss: 0.7557684779167175

('adam', 0.01, 8, 50)
train_loss: 0.48310351371765137, val_loss: 0.9052082896232605

('adam', 0.01, 8, 100)
train_loss: 0.4828612208366394, val_loss: 0.9393852353096008

('adam', 0.01, 16, 10)
train_loss: 0.6804956197738647, val_loss: 0.6901791095733643

('adam', 0.01, 16, 50)
train_loss: 0.49586713314056396, val_loss: 0.9415533542633057

('adam', 0.01, 16, 100)
train_loss: 0.4950040578842163, val_loss: 0.9364445209503174

('adam', 0.01, 32, 10)
train_loss: 0.7162044644355774, val_loss: 0.690049946308136

('adam', 0.01, 32, 50)
train_loss: 0.5056906342506409, val_loss: 0.8113054633140564

('adam', 0.01, 32, 100)
train_loss: 0.4960643947124481, val_loss: 0.9500927329063416

('adam', 0.01, 64, 10)
train_loss: 0.5859837532043457, val_loss: 0.7241647839546204

('adam', 0.01, 64, 50)
train_loss: 0.5080593228340149, val_loss: 0.8308346271514893

('adam', 0.01, 64, 100)
train_loss: 0.49672216176986694, val_loss: 0.8482246398925781

('adam', 0.1, 8, 10)
train_loss: 0.49199193716049194, val_loss: 0.958122968673706

('adam', 0.1, 8, 50)
train_loss: 0.47321268916130066, val_loss: 0.9208405613899231

('adam', 0.1, 8, 100)
train_loss: 0.45235905051231384, val_loss: 0.91245436668396

('adam', 0.1, 16, 10)
train_loss: 0.4880531132221222, val_loss: 0.9961959719657898

('adam', 0.1, 16, 50)
train_loss: 0.4757775366306305, val_loss: 0.931046724319458

('adam', 0.1, 16, 100)
train_loss: 0.4626486897468567, val_loss: 0.9715216159820557

('adam', 0.1, 32, 10)
train_loss: 0.5042223930358887, val_loss: 1.1090326309204102

('adam', 0.1, 32, 50)
train_loss: 0.4888106882572174, val_loss: 0.9772065281867981

('adam', 0.1, 32, 100)
train_loss: 0.4721182882785797, val_loss: 0.9473905563354492

('adam', 0.1, 64, 10)
train_loss: 0.49267011880874634, val_loss: 0.9971308708190918

('adam', 0.1, 64, 50)
train_loss: 0.4833349287509918, val_loss: 0.9250752329826355

('adam', 0.1, 64, 100)
train_loss: 0.4655075967311859, val_loss: 0.9249294400215149


---------
BEST MODEL
('adam', 0.01, 32, 10)
val_loss: 0.690049946308136
---------

Run from 2023-04-10 14:27:06.037869
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.5693815350532532, val_loss: 0.7530128359794617

('adam', 0.001, 8, 50)
train_loss: 0.5513085722923279, val_loss: 0.7638006210327148

('adam', 0.001, 8, 100)
train_loss: 0.5196986198425293, val_loss: 0.8118193745613098

('adam', 0.001, 16, 10)
train_loss: 0.5733044147491455, val_loss: 0.7496039271354675

('adam', 0.001, 16, 50)
train_loss: 0.7139675617218018, val_loss: 0.6845064163208008

('adam', 0.001, 16, 100)
train_loss: 0.5186242461204529, val_loss: 0.8319651484489441

('adam', 0.001, 32, 10)
train_loss: 0.6857216954231262, val_loss: 0.693594753742218

('adam', 0.001, 32, 50)
train_loss: 0.9195634722709656, val_loss: 0.7115412354469299

('adam', 0.001, 32, 100)
train_loss: 0.5557052493095398, val_loss: 0.759850025177002

('adam', 0.001, 64, 10)
train_loss: 0.554875373840332, val_loss: 0.7733321189880371

('adam', 0.001, 64, 50)
train_loss: 0.532574474811554, val_loss: 0.8103587627410889

('adam', 0.001, 64, 100)
train_loss: 0.5342280268669128, val_loss: 0.8019773960113525

('adam', 0.01, 8, 10)
train_loss: 0.5937759876251221, val_loss: 0.7106552124023438

('adam', 0.01, 8, 50)
train_loss: 0.48663976788520813, val_loss: 0.9531393051147461

('adam', 0.01, 8, 100)
train_loss: 0.479729562997818, val_loss: 0.9405671954154968

('adam', 0.01, 16, 10)
train_loss: 0.6524396538734436, val_loss: 0.6903676390647888

('adam', 0.01, 16, 50)
train_loss: 0.4916971027851105, val_loss: 0.8516125679016113

('adam', 0.01, 16, 100)
train_loss: 0.49073103070259094, val_loss: 0.9725378155708313

('adam', 0.01, 32, 10)
train_loss: 0.5225443243980408, val_loss: 0.8284108638763428

('adam', 0.01, 32, 50)
train_loss: 0.49341222643852234, val_loss: 0.9679167866706848

('adam', 0.01, 32, 100)
train_loss: 0.4864684045314789, val_loss: 0.959805428981781

('adam', 0.01, 64, 10)
train_loss: 0.5759265422821045, val_loss: 0.7441527843475342

('adam', 0.01, 64, 50)
train_loss: 0.4988391399383545, val_loss: 0.9048551917076111

('adam', 0.01, 64, 100)
train_loss: 0.49360892176628113, val_loss: 0.9804792404174805

('adam', 0.1, 8, 10)
train_loss: 0.4861547648906708, val_loss: 0.9483380317687988

('adam', 0.1, 8, 50)
train_loss: 0.46908819675445557, val_loss: 0.9041173458099365

('adam', 0.1, 8, 100)
train_loss: 0.45824679732322693, val_loss: 0.9084836840629578

('adam', 0.1, 16, 10)
train_loss: 0.48484039306640625, val_loss: 1.0121841430664062

('adam', 0.1, 16, 50)
train_loss: 0.47360700368881226, val_loss: 0.9145910143852234

('adam', 0.1, 16, 100)
train_loss: 0.46251291036605835, val_loss: 0.8649857044219971

('adam', 0.1, 32, 10)
train_loss: 0.49679264426231384, val_loss: 0.9462165832519531

('adam', 0.1, 32, 50)
train_loss: 0.48382988572120667, val_loss: 1.054911494255066

('adam', 0.1, 32, 100)
train_loss: 0.47667473554611206, val_loss: 0.9943558573722839

('adam', 0.1, 64, 10)
train_loss: 0.489619642496109, val_loss: 1.0110682249069214

('adam', 0.1, 64, 50)
train_loss: 0.48478463292121887, val_loss: 0.9460482597351074

('adam', 0.1, 64, 100)
train_loss: 0.47395333647727966, val_loss: 0.9207084774971008


---------
BEST MODEL
('adam', 0.001, 16, 50)
val_loss: 0.6845064163208008
---------

Run from 2023-04-17 09:56:56.087920
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6350514888763428, val_loss: 0.7019885182380676

('adam', 0.001, 8, 50)
train_loss: 0.7015671730041504, val_loss: 0.6936439275741577

('adam', 0.001, 8, 100)
train_loss: 0.5491164922714233, val_loss: 0.7915560007095337

('adam', 0.001, 16, 10)
train_loss: 0.6715628504753113, val_loss: 0.6942130327224731

('adam', 0.001, 16, 50)
train_loss: 0.8816637992858887, val_loss: 0.7402780652046204

('adam', 0.001, 16, 100)
train_loss: 0.7420729398727417, val_loss: 0.6991876363754272

('adam', 0.001, 32, 10)
train_loss: 0.8680521845817566, val_loss: 0.7327582240104675

('adam', 0.001, 32, 50)
train_loss: 0.5743697285652161, val_loss: 0.7496001720428467

('adam', 0.001, 32, 100)
train_loss: 0.6547510027885437, val_loss: 0.6955262422561646

('adam', 0.001, 64, 10)
train_loss: 0.9252633452415466, val_loss: 0.7558797001838684

('adam', 0.001, 64, 50)
train_loss: 0.7703692317008972, val_loss: 0.703095018863678

('adam', 0.001, 64, 100)
train_loss: 0.636489987373352, val_loss: 0.7006744146347046

('adam', 0.01, 8, 10)
train_loss: 0.5661991834640503, val_loss: 0.7495161890983582

('adam', 0.01, 8, 50)
train_loss: 0.5287919640541077, val_loss: 0.8890905380249023

('adam', 0.01, 8, 100)
train_loss: 0.5228284001350403, val_loss: 0.8852512240409851

('adam', 0.01, 16, 10)
train_loss: 0.5653845071792603, val_loss: 0.7600269317626953

('adam', 0.01, 16, 50)
train_loss: 0.5284340977668762, val_loss: 0.8352203369140625

('adam', 0.01, 16, 100)
train_loss: 0.5307667851448059, val_loss: 0.8859182596206665

('adam', 0.01, 32, 10)
train_loss: 0.5926581025123596, val_loss: 0.7262710332870483

('adam', 0.01, 32, 50)
train_loss: 0.5338238477706909, val_loss: 0.8591955900192261

('adam', 0.01, 32, 100)
train_loss: 0.525734007358551, val_loss: 0.8482858538627625

('adam', 0.01, 64, 10)
train_loss: 0.5757588148117065, val_loss: 0.7491541504859924

('adam', 0.01, 64, 50)
train_loss: 0.676433801651001, val_loss: 0.6932086944580078

('adam', 0.01, 64, 100)
train_loss: 0.5346262454986572, val_loss: 0.8850128054618835

('adam', 0.1, 8, 10)
train_loss: 0.5315060615539551, val_loss: 0.9066972732543945

('adam', 0.1, 8, 50)
train_loss: 0.5162353515625, val_loss: 0.8399718403816223

('adam', 0.1, 8, 100)
train_loss: 0.48628348112106323, val_loss: 0.8478600382804871

('adam', 0.1, 16, 10)
train_loss: 0.5330454111099243, val_loss: 0.9410715103149414

('adam', 0.1, 16, 50)
train_loss: 0.5160762071609497, val_loss: 0.8579505681991577

('adam', 0.1, 16, 100)
train_loss: 0.4951540529727936, val_loss: 0.8679879307746887

('adam', 0.1, 32, 10)
train_loss: 0.5461066961288452, val_loss: 1.0019030570983887

('adam', 0.1, 32, 50)
train_loss: 0.5205867886543274, val_loss: 0.8269637823104858

('adam', 0.1, 32, 100)
train_loss: 0.505408525466919, val_loss: 0.8997417688369751

('adam', 0.1, 64, 10)
train_loss: 0.530635416507721, val_loss: 0.9263311624526978

('adam', 0.1, 64, 50)
train_loss: 0.521107017993927, val_loss: 0.8740043640136719

('adam', 0.1, 64, 100)
train_loss: 0.5106489658355713, val_loss: 0.8731417655944824

('sgd', 0.001, 8, 10)
train_loss: 0.5666049122810364, val_loss: 0.7695665955543518

('sgd', 0.001, 8, 50)
train_loss: 0.6896712183952332, val_loss: 0.693097710609436

('sgd', 0.001, 8, 100)
train_loss: 0.6477397084236145, val_loss: 0.6970013380050659

('sgd', 0.001, 16, 10)
train_loss: 0.6128152012825012, val_loss: 0.7131229639053345

('sgd', 0.001, 16, 50)
train_loss: 0.8892137408256531, val_loss: 0.7420722246170044

('sgd', 0.001, 16, 100)
train_loss: 0.5928212404251099, val_loss: 0.7275038957595825

('sgd', 0.001, 32, 10)
train_loss: 0.5721209645271301, val_loss: 0.7579863667488098

('sgd', 0.001, 32, 50)
train_loss: 0.6485412120819092, val_loss: 0.6980536580085754

('sgd', 0.001, 32, 100)
train_loss: 0.6672663688659668, val_loss: 0.6944876909255981

('sgd', 0.001, 64, 10)
train_loss: 0.6260859966278076, val_loss: 0.7060640454292297

('sgd', 0.001, 64, 50)
train_loss: 0.7073571085929871, val_loss: 0.6935238242149353

('sgd', 0.001, 64, 100)
train_loss: 0.8737353086471558, val_loss: 0.7356374263763428

('sgd', 0.01, 8, 10)
train_loss: 0.6166561841964722, val_loss: 0.7088903188705444

('sgd', 0.01, 8, 50)
train_loss: 0.5872177481651306, val_loss: 0.7125452160835266

('sgd', 0.01, 8, 100)
train_loss: 0.5293951034545898, val_loss: 0.7793719172477722

('sgd', 0.01, 16, 10)
train_loss: 0.7003653645515442, val_loss: 0.693226158618927

('sgd', 0.01, 16, 50)
train_loss: 0.6794729232788086, val_loss: 0.6933322548866272

('sgd', 0.01, 16, 100)
train_loss: 0.5516852736473083, val_loss: 0.7744488716125488

('sgd', 0.01, 32, 10)
train_loss: 0.7215473055839539, val_loss: 0.6946535110473633

('sgd', 0.01, 32, 50)
train_loss: 0.5871500372886658, val_loss: 0.7290940880775452

('sgd', 0.01, 32, 100)
train_loss: 0.6110012531280518, val_loss: 0.7042266130447388

('sgd', 0.01, 64, 10)
train_loss: 0.5655272603034973, val_loss: 0.7718982100486755

('sgd', 0.01, 64, 50)
train_loss: 0.8315657377243042, val_loss: 0.7226408123970032

('sgd', 0.01, 64, 100)
train_loss: 0.5636762976646423, val_loss: 0.7676254510879517

('sgd', 0.1, 8, 10)
train_loss: 0.5394670963287354, val_loss: 0.8752032518386841

('sgd', 0.1, 8, 50)
train_loss: 0.534369170665741, val_loss: 0.9001243710517883

('sgd', 0.1, 8, 100)
train_loss: 0.5146673917770386, val_loss: 0.8872720003128052

('sgd', 0.1, 16, 10)
train_loss: 0.5859229564666748, val_loss: 0.7179971933364868

('sgd', 0.1, 16, 50)
train_loss: 0.5356037616729736, val_loss: 0.894110381603241

('sgd', 0.1, 16, 100)
train_loss: 0.5215703248977661, val_loss: 0.8803062438964844

('sgd', 0.1, 32, 10)
train_loss: 0.6130263805389404, val_loss: 0.7072389721870422

('sgd', 0.1, 32, 50)
train_loss: 0.527928352355957, val_loss: 0.8635188341140747

('sgd', 0.1, 32, 100)
train_loss: 0.5266895294189453, val_loss: 0.9036279916763306

('sgd', 0.1, 64, 10)
train_loss: 0.5555522441864014, val_loss: 0.7973091006278992

('sgd', 0.1, 64, 50)
train_loss: 0.5395345091819763, val_loss: 0.8360942006111145

('sgd', 0.1, 64, 100)
train_loss: 0.5161846876144409, val_loss: 0.8364347219467163


---------
BEST MODEL
('sgd', 0.001, 8, 50)
val_loss: 0.693097710609436
---------

Run from 2023-04-17 16:39:09.292799
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6345289349555969, val_loss: 0.7097803950309753

('adam', 0.001, 8, 50)
train_loss: 0.7246576547622681, val_loss: 0.6791650652885437

('adam', 0.001, 8, 100)
train_loss: 0.6039809584617615, val_loss: 0.7106461524963379

('adam', 0.001, 16, 10)
train_loss: 0.6334753632545471, val_loss: 0.7111520171165466

('adam', 0.001, 16, 50)
train_loss: 0.5937308669090271, val_loss: 0.7377837300300598

('adam', 0.001, 16, 100)
train_loss: 0.5901901125907898, val_loss: 0.7350121140480042

('adam', 0.001, 32, 10)
train_loss: 0.6451169848442078, val_loss: 0.7059111595153809

('adam', 0.001, 32, 50)
train_loss: 0.5635275840759277, val_loss: 0.7998430728912354

('adam', 0.001, 32, 100)
train_loss: 0.6556224226951599, val_loss: 0.6944016814231873

('adam', 0.001, 64, 10)
train_loss: 0.8334551453590393, val_loss: 0.6973772048950195

('adam', 0.001, 64, 50)
train_loss: 0.7609054446220398, val_loss: 0.6878998875617981

('adam', 0.001, 64, 100)
train_loss: 0.5903586745262146, val_loss: 0.7432456612586975

('adam', 0.01, 8, 10)
train_loss: 0.5615763664245605, val_loss: 0.7860064506530762

('adam', 0.01, 8, 50)
train_loss: 0.542322039604187, val_loss: 0.8765103816986084

('adam', 0.01, 8, 100)
train_loss: 0.5326186418533325, val_loss: 0.848456084728241

('adam', 0.01, 16, 10)
train_loss: 0.554316520690918, val_loss: 0.8479628562927246

('adam', 0.01, 16, 50)
train_loss: 0.5444535613059998, val_loss: 0.8681135177612305

('adam', 0.01, 16, 100)
train_loss: 0.5383745431900024, val_loss: 0.8670271039009094

('adam', 0.01, 32, 10)
train_loss: 0.6413444876670837, val_loss: 0.7016733288764954

('adam', 0.01, 32, 50)
train_loss: 0.5724928379058838, val_loss: 0.7317045331001282

('adam', 0.01, 32, 100)
train_loss: 0.5466743111610413, val_loss: 0.8851468563079834

('adam', 0.01, 64, 10)
train_loss: 0.6295068860054016, val_loss: 0.7112706303596497

('adam', 0.01, 64, 50)
train_loss: 0.6197502017021179, val_loss: 0.7000945210456848

('adam', 0.01, 64, 100)
train_loss: 0.5563115477561951, val_loss: 0.7640946507453918

('adam', 0.1, 8, 10)
train_loss: 0.5343717932701111, val_loss: 0.8575366139411926

('adam', 0.1, 8, 50)
train_loss: 0.5358971953392029, val_loss: 0.8808664679527283

('adam', 0.1, 8, 100)
train_loss: 0.5235439538955688, val_loss: 0.8171791434288025

('adam', 0.1, 16, 10)
train_loss: 0.5474361181259155, val_loss: 0.8727404475212097

('adam', 0.1, 16, 50)
train_loss: 0.5320166349411011, val_loss: 0.8461551666259766

('adam', 0.1, 16, 100)
train_loss: 0.5246467590332031, val_loss: 0.8513872027397156

('adam', 0.1, 32, 10)
train_loss: 0.5551899075508118, val_loss: 0.9980537295341492

('adam', 0.1, 32, 50)
train_loss: 0.5374006032943726, val_loss: 0.8523944020271301

('adam', 0.1, 32, 100)
train_loss: 0.5288020968437195, val_loss: 0.8752568364143372

('adam', 0.1, 64, 10)
train_loss: 0.5449123978614807, val_loss: 0.8289176821708679

('adam', 0.1, 64, 50)
train_loss: 0.5390117168426514, val_loss: 0.8707571029663086

('adam', 0.1, 64, 100)
train_loss: 0.5325414538383484, val_loss: 0.8568598628044128

('sgd', 0.001, 8, 10)
train_loss: 0.5715389847755432, val_loss: 0.7818386554718018

('sgd', 0.001, 8, 50)
train_loss: 0.851304829120636, val_loss: 0.6958653926849365

('sgd', 0.001, 8, 100)
train_loss: 0.7187181711196899, val_loss: 0.6822319626808167

('sgd', 0.001, 16, 10)
train_loss: 0.7409281134605408, val_loss: 0.6891738772392273

('sgd', 0.001, 16, 50)
train_loss: 0.5743192434310913, val_loss: 0.7746272087097168

('sgd', 0.001, 16, 100)
train_loss: 0.5752081871032715, val_loss: 0.7714490294456482

('sgd', 0.001, 32, 10)
train_loss: 0.8109529614448547, val_loss: 0.6941537261009216

('sgd', 0.001, 32, 50)
train_loss: 0.8659007549285889, val_loss: 0.7021138072013855

('sgd', 0.001, 32, 100)
train_loss: 0.7987986207008362, val_loss: 0.6899660229682922

('sgd', 0.001, 64, 10)
train_loss: 0.8589387536048889, val_loss: 0.7022969126701355

('sgd', 0.001, 64, 50)
train_loss: 0.6908868551254272, val_loss: 0.6929416656494141

('sgd', 0.001, 64, 100)
train_loss: 0.7389139533042908, val_loss: 0.6882574558258057

('sgd', 0.01, 8, 10)
train_loss: 0.7916806936264038, val_loss: 0.6823146939277649

('sgd', 0.01, 8, 50)
train_loss: 0.5685322284698486, val_loss: 0.7514088153839111

('sgd', 0.01, 8, 100)
train_loss: 0.5470743775367737, val_loss: 0.8148331046104431

('sgd', 0.01, 16, 10)
train_loss: 0.8790116906166077, val_loss: 0.7005274891853333

('sgd', 0.01, 16, 50)
train_loss: 0.7207255959510803, val_loss: 0.667400598526001

('sgd', 0.01, 16, 100)
train_loss: 0.5649517178535461, val_loss: 0.7621880173683167

('sgd', 0.01, 32, 10)
train_loss: 0.5663672685623169, val_loss: 0.7957337498664856

('sgd', 0.01, 32, 50)
train_loss: 0.5833545923233032, val_loss: 0.7501612305641174

('sgd', 0.01, 32, 100)
train_loss: 0.6211472153663635, val_loss: 0.6943435072898865

('sgd', 0.01, 64, 10)
train_loss: 0.5873090028762817, val_loss: 0.7523610591888428

('sgd', 0.01, 64, 50)
train_loss: 0.6022549867630005, val_loss: 0.7306513786315918

('sgd', 0.01, 64, 100)
train_loss: 0.574012041091919, val_loss: 0.7674455642700195

('sgd', 0.1, 8, 10)
train_loss: 0.550010085105896, val_loss: 0.8407343029975891

('sgd', 0.1, 8, 50)
train_loss: 0.5387155413627625, val_loss: 0.858725368976593

('sgd', 0.1, 8, 100)
train_loss: 0.547630250453949, val_loss: 0.8923969268798828

('sgd', 0.1, 16, 10)
train_loss: 0.5755874514579773, val_loss: 0.743934154510498

('sgd', 0.1, 16, 50)
train_loss: 0.5320044159889221, val_loss: 0.8390975594520569

('sgd', 0.1, 16, 100)
train_loss: 0.5481448173522949, val_loss: 0.8859572410583496

('sgd', 0.1, 32, 10)
train_loss: 0.665801465511322, val_loss: 0.6748061180114746

('sgd', 0.1, 32, 50)
train_loss: 0.5365687608718872, val_loss: 0.8194336891174316

('sgd', 0.1, 32, 100)
train_loss: 0.5410346984863281, val_loss: 0.8659358024597168

('sgd', 0.1, 64, 10)
train_loss: 0.6197425723075867, val_loss: 0.7127373218536377

('sgd', 0.1, 64, 50)
train_loss: 0.5497278571128845, val_loss: 0.8166959285736084

('sgd', 0.1, 64, 100)
train_loss: 0.5437995791435242, val_loss: 0.8568997979164124


---------
BEST MODEL
('sgd', 0.01, 16, 50)
val_loss: 0.667400598526001
---------
