
Run from 2023-04-02 10:47:26.779622
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6280547380447388, val_loss: 0.7499977946281433

('adam', 0.01, 50, 200)
train_loss: 0.6154491901397705, val_loss: 0.763362467288971

('adam', 0.01, 50, 300)
train_loss: 0.6077077984809875, val_loss: 0.7612702250480652

('adam', 0.01, 100, 100)
train_loss: 0.6215071678161621, val_loss: 0.7511099576950073

('adam', 0.01, 100, 200)
train_loss: 0.6131054759025574, val_loss: 0.7627376317977905

('adam', 0.01, 100, 300)
train_loss: 0.619187593460083, val_loss: 0.7643663287162781

('adam', 0.01, 200, 100)
train_loss: 0.6273762583732605, val_loss: 0.750098705291748

('adam', 0.01, 200, 200)
train_loss: 0.6175755262374878, val_loss: 0.763961136341095

('adam', 0.01, 200, 300)
train_loss: 0.6184107065200806, val_loss: 0.7641186714172363

('adam', 0.1, 50, 100)
train_loss: 0.5933013558387756, val_loss: 0.7592526078224182

('adam', 0.1, 50, 200)
train_loss: 0.5693913698196411, val_loss: 0.7647948861122131

('adam', 0.1, 50, 300)
train_loss: 0.552358090877533, val_loss: 0.7814150452613831

('adam', 0.1, 100, 100)
train_loss: 0.5910820364952087, val_loss: 0.7594927549362183

('adam', 0.1, 100, 200)
train_loss: 0.566857099533081, val_loss: 0.7663136720657349

('adam', 0.1, 100, 300)
train_loss: 0.5509178638458252, val_loss: 0.7838073372840881

('adam', 0.1, 200, 100)
train_loss: 0.5915408730506897, val_loss: 0.7594361901283264

('adam', 0.1, 200, 200)
train_loss: 0.5643070936203003, val_loss: 0.7681021094322205

('adam', 0.1, 200, 300)
train_loss: 0.5508083701133728, val_loss: 0.7839987874031067

('adam', 0.2, 50, 100)
train_loss: 0.5620772838592529, val_loss: 0.769987940788269

('adam', 0.2, 50, 200)
train_loss: 0.5441676378250122, val_loss: 0.7992850542068481

('adam', 0.2, 50, 300)
train_loss: 0.5374094843864441, val_loss: 0.8303473591804504

('adam', 0.2, 100, 100)
train_loss: 0.5641001462936401, val_loss: 0.7679386734962463

('adam', 0.2, 100, 200)
train_loss: 0.5423352122306824, val_loss: 0.8051980137825012

('adam', 0.2, 100, 300)
train_loss: 0.5368899703025818, val_loss: 0.8348246812820435

('adam', 0.2, 200, 100)
train_loss: 0.5649693608283997, val_loss: 0.7671782374382019

('adam', 0.2, 200, 200)
train_loss: 0.5444453358650208, val_loss: 0.7984752655029297

('adam', 0.2, 200, 300)
train_loss: 0.537849485874176, val_loss: 0.8270468711853027


---------
BEST MODEL
('adam', 0.01, 50, 100)
val_loss: 0.7499977946281433
---------

Run from 2023-04-05 15:05:04.777856
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6852225065231323, val_loss: 0.692530632019043

('adam', 0.001, 8, 50)
train_loss: 0.6714597344398499, val_loss: 0.6974859833717346

('adam', 0.001, 8, 100)
train_loss: 0.6573610305786133, val_loss: 0.7049072980880737

('adam', 0.001, 16, 10)
train_loss: 0.6908239722251892, val_loss: 0.6938015818595886

('adam', 0.001, 16, 50)
train_loss: 0.6712195873260498, val_loss: 0.694564163684845

('adam', 0.001, 16, 100)
train_loss: 0.6458925604820251, val_loss: 0.6990540623664856

('adam', 0.001, 32, 10)
train_loss: 0.683197021484375, val_loss: 0.6912210583686829

('adam', 0.001, 32, 50)
train_loss: 0.6840762495994568, val_loss: 0.6954137086868286

('adam', 0.001, 32, 100)
train_loss: 0.6680898070335388, val_loss: 0.6967499852180481

('adam', 0.001, 64, 10)
train_loss: 0.6899933218955994, val_loss: 0.6927202939987183

('adam', 0.001, 64, 50)
train_loss: 0.6879127621650696, val_loss: 0.6943559050559998

('adam', 0.001, 64, 100)
train_loss: 0.6641089916229248, val_loss: 0.6919248700141907

('adam', 0.01, 8, 10)
train_loss: 0.649077832698822, val_loss: 0.704955518245697

('adam', 0.01, 8, 50)
train_loss: 0.6157693862915039, val_loss: 0.7687566876411438

('adam', 0.01, 8, 100)
train_loss: 0.6170297861099243, val_loss: 0.7632225751876831

('adam', 0.01, 16, 10)
train_loss: 0.6588879823684692, val_loss: 0.7007551193237305

('adam', 0.01, 16, 50)
train_loss: 0.612023115158081, val_loss: 0.7492810487747192

('adam', 0.01, 16, 100)
train_loss: 0.620378315448761, val_loss: 0.7713489532470703

('adam', 0.01, 32, 10)
train_loss: 0.6608160138130188, val_loss: 0.6961771249771118

('adam', 0.01, 32, 50)
train_loss: 0.6136471629142761, val_loss: 0.7431992888450623

('adam', 0.01, 32, 100)
train_loss: 0.6251372694969177, val_loss: 0.7551531195640564

('adam', 0.01, 64, 10)
train_loss: 0.6781060695648193, val_loss: 0.6944398880004883

('adam', 0.01, 64, 50)
train_loss: 0.6297779679298401, val_loss: 0.7188282012939453

('adam', 0.01, 64, 100)
train_loss: 0.6294300556182861, val_loss: 0.7498038411140442

('adam', 0.1, 8, 10)
train_loss: 0.6167619824409485, val_loss: 0.7312445044517517

('adam', 0.1, 8, 50)
train_loss: 0.5793273448944092, val_loss: 0.7709430456161499

('adam', 0.1, 8, 100)
train_loss: 0.5517436265945435, val_loss: 0.8421486616134644

('adam', 0.1, 16, 10)
train_loss: 0.6150882244110107, val_loss: 0.7797942757606506

('adam', 0.1, 16, 50)
train_loss: 0.5833699107170105, val_loss: 0.7663816809654236

('adam', 0.1, 16, 100)
train_loss: 0.5619071125984192, val_loss: 0.7712744474411011

('adam', 0.1, 32, 10)
train_loss: 0.6174936294555664, val_loss: 0.7940769791603088

('adam', 0.1, 32, 50)
train_loss: 0.5878857970237732, val_loss: 0.7721298336982727

('adam', 0.1, 32, 100)
train_loss: 0.5698603391647339, val_loss: 0.7742348313331604

('adam', 0.1, 64, 10)
train_loss: 0.6155102252960205, val_loss: 0.7912231087684631

('adam', 0.1, 64, 50)
train_loss: 0.6072409749031067, val_loss: 0.7705000042915344

('adam', 0.1, 64, 100)
train_loss: 0.5930583477020264, val_loss: 0.7592746019363403


---------
BEST MODEL
('adam', 0.001, 32, 10)
val_loss: 0.6912210583686829
---------

Run from 2023-04-05 16:05:36.872716
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6899228692054749, val_loss: 0.6941089034080505

('adam', 0.001, 8, 50)
train_loss: 0.6636801958084106, val_loss: 0.6966695189476013

('adam', 0.001, 8, 100)
train_loss: 0.6482012867927551, val_loss: 0.7049274444580078

('adam', 0.001, 16, 10)
train_loss: 0.6743447184562683, val_loss: 0.6896230578422546

('adam', 0.001, 16, 50)
train_loss: 0.6712461113929749, val_loss: 0.6947407126426697

('adam', 0.001, 16, 100)
train_loss: 0.6458501815795898, val_loss: 0.6983945965766907

('adam', 0.001, 32, 10)
train_loss: 0.6953546404838562, val_loss: 0.6947641372680664

('adam', 0.001, 32, 50)
train_loss: 0.6751638650894165, val_loss: 0.693352997303009

('adam', 0.001, 32, 100)
train_loss: 0.6710988283157349, val_loss: 0.6972107291221619

('adam', 0.001, 64, 10)
train_loss: 0.7019013166427612, val_loss: 0.6963887810707092

('adam', 0.001, 64, 50)
train_loss: 0.6736941337585449, val_loss: 0.6908847689628601

('adam', 0.001, 64, 100)
train_loss: 0.666321873664856, val_loss: 0.6922871470451355

('adam', 0.01, 8, 10)
train_loss: 0.6391205787658691, val_loss: 0.7039455771446228

('adam', 0.01, 8, 50)
train_loss: 0.6184127926826477, val_loss: 0.7486383318901062

('adam', 0.01, 8, 100)
train_loss: 0.6067049503326416, val_loss: 0.7656348347663879

('adam', 0.01, 16, 10)
train_loss: 0.6616020202636719, val_loss: 0.701241135597229

('adam', 0.01, 16, 50)
train_loss: 0.6198639273643494, val_loss: 0.7523751258850098

('adam', 0.01, 16, 100)
train_loss: 0.6120754480361938, val_loss: 0.765063464641571

('adam', 0.01, 32, 10)
train_loss: 0.6809349060058594, val_loss: 0.6996968984603882

('adam', 0.01, 32, 50)
train_loss: 0.6335721015930176, val_loss: 0.7321361899375916

('adam', 0.01, 32, 100)
train_loss: 0.6209381818771362, val_loss: 0.7582744359970093

('adam', 0.01, 64, 10)
train_loss: 0.6915571093559265, val_loss: 0.6978554725646973

('adam', 0.01, 64, 50)
train_loss: 0.628182590007782, val_loss: 0.7191497683525085

('adam', 0.01, 64, 100)
train_loss: 0.6139293909072876, val_loss: 0.7527604699134827

('adam', 0.1, 8, 10)
train_loss: 0.6160448789596558, val_loss: 0.7697874307632446

('adam', 0.1, 8, 50)
train_loss: 0.580417811870575, val_loss: 0.7276595234870911

('adam', 0.1, 8, 100)
train_loss: 0.5540212392807007, val_loss: 0.7679932713508606

('adam', 0.1, 16, 10)
train_loss: 0.6234070658683777, val_loss: 0.7640026211738586

('adam', 0.1, 16, 50)
train_loss: 0.5823549628257751, val_loss: 0.7658936381340027

('adam', 0.1, 16, 100)
train_loss: 0.5587124824523926, val_loss: 0.7726271748542786

('adam', 0.1, 32, 10)
train_loss: 0.6266052722930908, val_loss: 0.8168649077415466

('adam', 0.1, 32, 50)
train_loss: 0.5991762280464172, val_loss: 0.7504013776779175

('adam', 0.1, 32, 100)
train_loss: 0.5706998109817505, val_loss: 0.7403091788291931

('adam', 0.1, 64, 10)
train_loss: 0.6255273818969727, val_loss: 0.7847386598587036

('adam', 0.1, 64, 50)
train_loss: 0.6027477979660034, val_loss: 0.7693430781364441

('adam', 0.1, 64, 100)
train_loss: 0.5857023596763611, val_loss: 0.7603505253791809


---------
BEST MODEL
('adam', 0.001, 16, 10)
val_loss: 0.6896230578422546
---------

Run from 2023-04-10 09:23:19.863645
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6781920194625854, val_loss: 0.6912017464637756

('adam', 0.001, 8, 50)
train_loss: 0.6666581630706787, val_loss: 0.6975522637367249

('adam', 0.001, 8, 100)
train_loss: 0.6441405415534973, val_loss: 0.7056112289428711

('adam', 0.001, 16, 10)
train_loss: 0.6811790466308594, val_loss: 0.6911833882331848

('adam', 0.001, 16, 50)
train_loss: 0.6813910007476807, val_loss: 0.6969529390335083

('adam', 0.001, 16, 100)
train_loss: 0.6671361923217773, val_loss: 0.7008116841316223

('adam', 0.001, 32, 10)
train_loss: 0.6920061111450195, val_loss: 0.6937407851219177

('adam', 0.001, 32, 50)
train_loss: 0.6749751567840576, val_loss: 0.6931355595588684

('adam', 0.001, 32, 100)
train_loss: 0.6567099094390869, val_loss: 0.6948179006576538

('adam', 0.001, 64, 10)
train_loss: 0.6943521499633789, val_loss: 0.6939983367919922

('adam', 0.001, 64, 50)
train_loss: 0.6684538722038269, val_loss: 0.6898579001426697

('adam', 0.001, 64, 100)
train_loss: 0.6818974018096924, val_loss: 0.6955825090408325

('adam', 0.01, 8, 10)
train_loss: 0.6590632200241089, val_loss: 0.7050236463546753

('adam', 0.01, 8, 50)
train_loss: 0.6185519099235535, val_loss: 0.7739816904067993

('adam', 0.01, 8, 100)
train_loss: 0.6049515604972839, val_loss: 0.7771018147468567

('adam', 0.01, 16, 10)
train_loss: 0.6635607481002808, val_loss: 0.7003542184829712

('adam', 0.01, 16, 50)
train_loss: 0.6117562651634216, val_loss: 0.7510432004928589

('adam', 0.01, 16, 100)
train_loss: 0.6108478903770447, val_loss: 0.765354573726654

('adam', 0.01, 32, 10)
train_loss: 0.6729953289031982, val_loss: 0.6979762315750122

('adam', 0.01, 32, 50)
train_loss: 0.6256566643714905, val_loss: 0.736404299736023

('adam', 0.01, 32, 100)
train_loss: 0.6244372725486755, val_loss: 0.7583393454551697

('adam', 0.01, 64, 10)
train_loss: 0.6794472336769104, val_loss: 0.6947427988052368

('adam', 0.01, 64, 50)
train_loss: 0.6236111521720886, val_loss: 0.7202494740486145

('adam', 0.01, 64, 100)
train_loss: 0.6228383779525757, val_loss: 0.7508590817451477

('adam', 0.1, 8, 10)
train_loss: 0.619198739528656, val_loss: 0.8131591081619263

('adam', 0.1, 8, 50)
train_loss: 0.5713608264923096, val_loss: 0.7778388857841492

('adam', 0.1, 8, 100)
train_loss: 0.5586689114570618, val_loss: 0.8334713578224182

('adam', 0.1, 16, 10)
train_loss: 0.6177279949188232, val_loss: 0.7690505981445312

('adam', 0.1, 16, 50)
train_loss: 0.5803217887878418, val_loss: 0.7614882588386536

('adam', 0.1, 16, 100)
train_loss: 0.5555127859115601, val_loss: 0.7803574800491333

('adam', 0.1, 32, 10)
train_loss: 0.6308974027633667, val_loss: 0.8391088843345642

('adam', 0.1, 32, 50)
train_loss: 0.5981429219245911, val_loss: 0.7383784055709839

('adam', 0.1, 32, 100)
train_loss: 0.5708407759666443, val_loss: 0.7761865854263306

('adam', 0.1, 64, 10)
train_loss: 0.6226816177368164, val_loss: 0.7864589691162109

('adam', 0.1, 64, 50)
train_loss: 0.6105148792266846, val_loss: 0.7710141539573669

('adam', 0.1, 64, 100)
train_loss: 0.5880458950996399, val_loss: 0.7599417567253113


---------
BEST MODEL
('adam', 0.001, 64, 50)
val_loss: 0.6898579001426697
---------

Run from 2023-04-10 12:29:08.267604
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6875284314155579, val_loss: 0.6942381858825684

('adam', 0.001, 8, 50)
train_loss: 0.6871941089630127, val_loss: 0.7144330143928528

('adam', 0.001, 8, 100)
train_loss: 0.6449270844459534, val_loss: 0.7038304209709167

('adam', 0.001, 16, 10)
train_loss: 0.716049075126648, val_loss: 0.7167196869850159

('adam', 0.001, 16, 50)
train_loss: 0.6768405437469482, val_loss: 0.6983048319816589

('adam', 0.001, 16, 100)
train_loss: 0.6658254265785217, val_loss: 0.7075487375259399

('adam', 0.001, 32, 10)
train_loss: 0.669363796710968, val_loss: 0.6758187413215637

('adam', 0.001, 32, 50)
train_loss: 0.6576860547065735, val_loss: 0.6766196489334106

('adam', 0.001, 32, 100)
train_loss: 0.6696293354034424, val_loss: 0.6989244818687439

('adam', 0.001, 64, 10)
train_loss: 0.6722105741500854, val_loss: 0.6770154237747192

('adam', 0.001, 64, 50)
train_loss: 0.7030842304229736, val_loss: 0.7092456221580505

('adam', 0.001, 64, 100)
train_loss: 0.6687426567077637, val_loss: 0.6878043413162231

('adam', 0.01, 8, 10)
train_loss: 0.6525663733482361, val_loss: 0.7211190462112427

('adam', 0.01, 8, 50)
train_loss: 0.6213210225105286, val_loss: 0.7394389510154724

('adam', 0.01, 8, 100)
train_loss: 0.6192185878753662, val_loss: 0.7344024777412415

('adam', 0.01, 16, 10)
train_loss: 0.6731683611869812, val_loss: 0.71349036693573

('adam', 0.01, 16, 50)
train_loss: 0.6190903782844543, val_loss: 0.7331010103225708

('adam', 0.01, 16, 100)
train_loss: 0.6212326288223267, val_loss: 0.7547522187232971

('adam', 0.01, 32, 10)
train_loss: 0.6813779473304749, val_loss: 0.7058177590370178

('adam', 0.01, 32, 50)
train_loss: 0.6250098347663879, val_loss: 0.7376238703727722

('adam', 0.01, 32, 100)
train_loss: 0.6217631101608276, val_loss: 0.76124507188797

('adam', 0.01, 64, 10)
train_loss: 0.6706117987632751, val_loss: 0.6884726285934448

('adam', 0.01, 64, 50)
train_loss: 0.6344426274299622, val_loss: 0.7157711982727051

('adam', 0.01, 64, 100)
train_loss: 0.6213648915290833, val_loss: 0.7427306771278381

('adam', 0.1, 8, 10)
train_loss: 0.6263195276260376, val_loss: 0.723617672920227

('adam', 0.1, 8, 50)
train_loss: 0.6182860732078552, val_loss: 0.7443600296974182

('adam', 0.1, 8, 100)
train_loss: 0.6343395113945007, val_loss: 0.6757470369338989

('adam', 0.1, 16, 10)
train_loss: 0.6224303245544434, val_loss: 0.7638524174690247

('adam', 0.1, 16, 50)
train_loss: 0.6200908422470093, val_loss: 0.7589759230613708

('adam', 0.1, 16, 100)
train_loss: 0.6199209690093994, val_loss: 0.7475133538246155

('adam', 0.1, 32, 10)
train_loss: 0.6290228962898254, val_loss: 0.8079692721366882

('adam', 0.1, 32, 50)
train_loss: 0.6202336549758911, val_loss: 0.7064547538757324

('adam', 0.1, 32, 100)
train_loss: 0.6199067234992981, val_loss: 0.7271748781204224

('adam', 0.1, 64, 10)
train_loss: 0.625918447971344, val_loss: 0.7858773469924927

('adam', 0.1, 64, 50)
train_loss: 0.6204800605773926, val_loss: 0.7570614218711853

('adam', 0.1, 64, 100)
train_loss: 0.6182277202606201, val_loss: 0.7303616404533386


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6757470369338989
---------

Run from 2023-04-10 13:44:57.486782
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6974324584007263, val_loss: 0.7100868225097656

('adam', 0.001, 8, 50)
train_loss: 0.6620988845825195, val_loss: 0.6858258247375488

('adam', 0.001, 8, 100)
train_loss: 0.6404756903648376, val_loss: 0.6829535365104675

('adam', 0.001, 16, 10)
train_loss: 0.6718032360076904, val_loss: 0.6639730334281921

('adam', 0.001, 16, 50)
train_loss: 0.656434953212738, val_loss: 0.6670435667037964

('adam', 0.001, 16, 100)
train_loss: 0.6449921131134033, val_loss: 0.675949215888977

('adam', 0.001, 32, 10)
train_loss: 0.682878315448761, val_loss: 0.6813632845878601

('adam', 0.001, 32, 50)
train_loss: 0.6692885160446167, val_loss: 0.677388072013855

('adam', 0.001, 32, 100)
train_loss: 0.6789624094963074, val_loss: 0.7206760048866272

('adam', 0.001, 64, 10)
train_loss: 0.669835090637207, val_loss: 0.6579623222351074

('adam', 0.001, 64, 50)
train_loss: 0.7005058526992798, val_loss: 0.7212312817573547

('adam', 0.001, 64, 100)
train_loss: 0.658928394317627, val_loss: 0.6651554107666016

('adam', 0.01, 8, 10)
train_loss: 0.6366088390350342, val_loss: 0.6772044897079468

('adam', 0.01, 8, 50)
train_loss: 0.6249526143074036, val_loss: 0.7428490519523621

('adam', 0.01, 8, 100)
train_loss: 0.6246681213378906, val_loss: 0.7728012800216675

('adam', 0.01, 16, 10)
train_loss: 0.6733927726745605, val_loss: 0.7232868075370789

('adam', 0.01, 16, 50)
train_loss: 0.6209205985069275, val_loss: 0.7380288243293762

('adam', 0.01, 16, 100)
train_loss: 0.6183661222457886, val_loss: 0.7247509956359863

('adam', 0.01, 32, 10)
train_loss: 0.6872475147247314, val_loss: 0.7325996160507202

('adam', 0.01, 32, 50)
train_loss: 0.6208484172821045, val_loss: 0.7120436429977417

('adam', 0.01, 32, 100)
train_loss: 0.6199823021888733, val_loss: 0.7303816676139832

('adam', 0.01, 64, 10)
train_loss: 0.6661630272865295, val_loss: 0.6750604510307312

('adam', 0.01, 64, 50)
train_loss: 0.6272022724151611, val_loss: 0.6993058323860168

('adam', 0.01, 64, 100)
train_loss: 0.6226198673248291, val_loss: 0.7403773069381714

('adam', 0.1, 8, 10)
train_loss: 0.6254414916038513, val_loss: 0.7207147479057312

('adam', 0.1, 8, 50)
train_loss: 0.6234710216522217, val_loss: 0.7599238753318787

('adam', 0.1, 8, 100)
train_loss: 0.6188889741897583, val_loss: 0.6870341897010803

('adam', 0.1, 16, 10)
train_loss: 0.6228562593460083, val_loss: 0.7566103339195251

('adam', 0.1, 16, 50)
train_loss: 0.6179478764533997, val_loss: 0.7274709343910217

('adam', 0.1, 16, 100)
train_loss: 0.6173747181892395, val_loss: 0.7141615748405457

('adam', 0.1, 32, 10)
train_loss: 0.6244415640830994, val_loss: 0.775323212146759

('adam', 0.1, 32, 50)
train_loss: 0.6190263628959656, val_loss: 0.6938874125480652

('adam', 0.1, 32, 100)
train_loss: 0.6175175905227661, val_loss: 0.7002694010734558

('adam', 0.1, 64, 10)
train_loss: 0.6206647753715515, val_loss: 0.768938422203064

('adam', 0.1, 64, 50)
train_loss: 0.6172595620155334, val_loss: 0.717981219291687

('adam', 0.1, 64, 100)
train_loss: 0.6172327995300293, val_loss: 0.7147973775863647


---------
BEST MODEL
('adam', 0.001, 64, 10)
val_loss: 0.6579623222351074
---------

Run from 2023-04-17 09:34:02.371581
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6696937680244446, val_loss: 0.6914438009262085

('adam', 0.001, 8, 50)
train_loss: 0.6532277464866638, val_loss: 0.6967962384223938

('adam', 0.001, 8, 100)
train_loss: 0.6418522596359253, val_loss: 0.7053361535072327

('adam', 0.001, 16, 10)
train_loss: 0.6831362843513489, val_loss: 0.6922218203544617

('adam', 0.001, 16, 50)
train_loss: 0.6591660976409912, val_loss: 0.6941580176353455

('adam', 0.001, 16, 100)
train_loss: 0.6751326322555542, val_loss: 0.7011486887931824

('adam', 0.001, 32, 10)
train_loss: 0.7198231220245361, val_loss: 0.7003656029701233

('adam', 0.001, 32, 50)
train_loss: 0.6614614129066467, val_loss: 0.692666232585907

('adam', 0.001, 32, 100)
train_loss: 0.6840616464614868, val_loss: 0.6990669965744019

('adam', 0.001, 64, 10)
train_loss: 0.6895623803138733, val_loss: 0.6928005218505859

('adam', 0.001, 64, 50)
train_loss: 0.7056500315666199, val_loss: 0.697811484336853

('adam', 0.001, 64, 100)
train_loss: 0.6790980696678162, val_loss: 0.694645881652832

('adam', 0.01, 8, 10)
train_loss: 0.6243969202041626, val_loss: 0.7149912714958191

('adam', 0.01, 8, 50)
train_loss: 0.6111040711402893, val_loss: 0.7584899067878723

('adam', 0.01, 8, 100)
train_loss: 0.6000176668167114, val_loss: 0.7577739357948303

('adam', 0.01, 16, 10)
train_loss: 0.6292265057563782, val_loss: 0.7052386403083801

('adam', 0.01, 16, 50)
train_loss: 0.6045153141021729, val_loss: 0.7556658983230591

('adam', 0.01, 16, 100)
train_loss: 0.6103582382202148, val_loss: 0.7640355229377747

('adam', 0.01, 32, 10)
train_loss: 0.6711685657501221, val_loss: 0.6975437998771667

('adam', 0.01, 32, 50)
train_loss: 0.6117228269577026, val_loss: 0.7414715886116028

('adam', 0.01, 32, 100)
train_loss: 0.6242204308509827, val_loss: 0.7628130316734314

('adam', 0.01, 64, 10)
train_loss: 0.6872592568397522, val_loss: 0.6957236528396606

('adam', 0.01, 64, 50)
train_loss: 0.6505685448646545, val_loss: 0.7157813310623169

('adam', 0.01, 64, 100)
train_loss: 0.6207668781280518, val_loss: 0.7517483830451965

('adam', 0.1, 8, 10)
train_loss: 0.611329197883606, val_loss: 0.7444237470626831

('adam', 0.1, 8, 50)
train_loss: 0.5822349190711975, val_loss: 0.8376813530921936

('adam', 0.1, 8, 100)
train_loss: 0.5894930958747864, val_loss: 0.764054000377655

('adam', 0.1, 16, 10)
train_loss: 0.6200602650642395, val_loss: 0.7792697548866272

('adam', 0.1, 16, 50)
train_loss: 0.58286052942276, val_loss: 0.7972801923751831

('adam', 0.1, 16, 100)
train_loss: 0.5803751945495605, val_loss: 0.8182583451271057

('adam', 0.1, 32, 10)
train_loss: 0.6179659366607666, val_loss: 0.8154227137565613

('adam', 0.1, 32, 50)
train_loss: 0.5975555777549744, val_loss: 0.734959602355957

('adam', 0.1, 32, 100)
train_loss: 0.5808778405189514, val_loss: 0.8065272569656372

('adam', 0.1, 64, 10)
train_loss: 0.6115994453430176, val_loss: 0.7977401614189148

('adam', 0.1, 64, 50)
train_loss: 0.5977171063423157, val_loss: 0.774480402469635

('adam', 0.1, 64, 100)
train_loss: 0.5850002765655518, val_loss: 0.7768839597702026

('sgd', 0.001, 8, 10)
train_loss: 0.6974310278892517, val_loss: 0.694392204284668

('sgd', 0.001, 8, 50)
train_loss: 0.6787341237068176, val_loss: 0.6926926970481873

('sgd', 0.001, 8, 100)
train_loss: 0.6593359708786011, val_loss: 0.6930438280105591

('sgd', 0.001, 16, 10)
train_loss: 0.6887496113777161, val_loss: 0.6925157308578491

('sgd', 0.001, 16, 50)
train_loss: 0.7179862260818481, val_loss: 0.7004037499427795

('sgd', 0.001, 16, 100)
train_loss: 0.7168189287185669, val_loss: 0.7013219594955444

('sgd', 0.001, 32, 10)
train_loss: 0.694341778755188, val_loss: 0.6935157179832458

('sgd', 0.001, 32, 50)
train_loss: 0.661637008190155, val_loss: 0.6902702450752258

('sgd', 0.001, 32, 100)
train_loss: 0.6979281902313232, val_loss: 0.6954998970031738

('sgd', 0.001, 64, 10)
train_loss: 0.6870908737182617, val_loss: 0.6921148300170898

('sgd', 0.001, 64, 50)
train_loss: 0.6766172051429749, val_loss: 0.6908999085426331

('sgd', 0.001, 64, 100)
train_loss: 0.7181126475334167, val_loss: 0.7000296711921692

('sgd', 0.01, 8, 10)
train_loss: 0.6948617696762085, val_loss: 0.6980605125427246

('sgd', 0.01, 8, 50)
train_loss: 0.6587260365486145, val_loss: 0.7139326930046082

('sgd', 0.01, 8, 100)
train_loss: 0.628432035446167, val_loss: 0.7354339957237244

('sgd', 0.01, 16, 10)
train_loss: 0.6759462952613831, val_loss: 0.6921668648719788

('sgd', 0.01, 16, 50)
train_loss: 0.6504924893379211, val_loss: 0.6993589997291565

('sgd', 0.01, 16, 100)
train_loss: 0.6291292309761047, val_loss: 0.7120145559310913

('sgd', 0.01, 32, 10)
train_loss: 0.6940879821777344, val_loss: 0.6947723031044006

('sgd', 0.01, 32, 50)
train_loss: 0.642785906791687, val_loss: 0.6956471800804138

('sgd', 0.01, 32, 100)
train_loss: 0.6380434632301331, val_loss: 0.7025051712989807

('sgd', 0.01, 64, 10)
train_loss: 0.6612242460250854, val_loss: 0.69022136926651

('sgd', 0.01, 64, 50)
train_loss: 0.6739762425422668, val_loss: 0.6934536099433899

('sgd', 0.01, 64, 100)
train_loss: 0.6903674602508545, val_loss: 0.700581967830658

('sgd', 0.1, 8, 10)
train_loss: 0.6147177815437317, val_loss: 0.7486953139305115

('sgd', 0.1, 8, 50)
train_loss: 0.6231021285057068, val_loss: 0.7731438875198364

('sgd', 0.1, 8, 100)
train_loss: 0.6105864644050598, val_loss: 0.7655203938484192

('sgd', 0.1, 16, 10)
train_loss: 0.6359128952026367, val_loss: 0.7111441493034363

('sgd', 0.1, 16, 50)
train_loss: 0.6218277812004089, val_loss: 0.7623211741447449

('sgd', 0.1, 16, 100)
train_loss: 0.623307466506958, val_loss: 0.7591296434402466

('sgd', 0.1, 32, 10)
train_loss: 0.6717768907546997, val_loss: 0.7045012712478638

('sgd', 0.1, 32, 50)
train_loss: 0.6118836402893066, val_loss: 0.7543500661849976

('sgd', 0.1, 32, 100)
train_loss: 0.6269721984863281, val_loss: 0.767119824886322

('sgd', 0.1, 64, 10)
train_loss: 0.6763429045677185, val_loss: 0.6975013017654419

('sgd', 0.1, 64, 50)
train_loss: 0.646226167678833, val_loss: 0.73040771484375

('sgd', 0.1, 64, 100)
train_loss: 0.6310107111930847, val_loss: 0.7533658146858215


---------
BEST MODEL
('sgd', 0.01, 64, 10)
val_loss: 0.69022136926651
---------

Run from 2023-04-17 09:40:22.452691
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6863236427307129, val_loss: 0.692165732383728

('adam', 0.001, 8, 50)
train_loss: 0.658531904220581, val_loss: 0.6871399283409119

('adam', 0.001, 8, 100)
train_loss: 0.6453700661659241, val_loss: 0.6888301968574524

('adam', 0.001, 16, 10)
train_loss: 0.6699930429458618, val_loss: 0.6807292103767395

('adam', 0.001, 16, 50)
train_loss: 0.6885919570922852, val_loss: 0.6968793869018555

('adam', 0.001, 16, 100)
train_loss: 0.6816878318786621, val_loss: 0.7051663398742676

('adam', 0.001, 32, 10)
train_loss: 0.7213654518127441, val_loss: 0.712790846824646

('adam', 0.001, 32, 50)
train_loss: 0.6770678758621216, val_loss: 0.6915815472602844

('adam', 0.001, 32, 100)
train_loss: 0.6900548934936523, val_loss: 0.7081090211868286

('adam', 0.001, 64, 10)
train_loss: 0.7126734256744385, val_loss: 0.7066158056259155

('adam', 0.001, 64, 50)
train_loss: 0.6912551522254944, val_loss: 0.6968355178833008

('adam', 0.001, 64, 100)
train_loss: 0.7007805109024048, val_loss: 0.7075997591018677

('adam', 0.01, 8, 10)
train_loss: 0.6475085616111755, val_loss: 0.6967812776565552

('adam', 0.01, 8, 50)
train_loss: 0.6388569474220276, val_loss: 0.7233961224555969

('adam', 0.01, 8, 100)
train_loss: 0.639076292514801, val_loss: 0.709811806678772

('adam', 0.01, 16, 10)
train_loss: 0.6804139018058777, val_loss: 0.7074055671691895

('adam', 0.01, 16, 50)
train_loss: 0.6410461068153381, val_loss: 0.7238804697990417

('adam', 0.01, 16, 100)
train_loss: 0.6410598158836365, val_loss: 0.7177267074584961

('adam', 0.01, 32, 10)
train_loss: 0.6694956421852112, val_loss: 0.6959730386734009

('adam', 0.01, 32, 50)
train_loss: 0.6459221839904785, val_loss: 0.7347774505615234

('adam', 0.01, 32, 100)
train_loss: 0.6373181343078613, val_loss: 0.7165776491165161

('adam', 0.01, 64, 10)
train_loss: 0.685705840587616, val_loss: 0.6976006031036377

('adam', 0.01, 64, 50)
train_loss: 0.6531248092651367, val_loss: 0.7177770733833313

('adam', 0.01, 64, 100)
train_loss: 0.6376421451568604, val_loss: 0.7188845872879028

('adam', 0.1, 8, 10)
train_loss: 0.6516422629356384, val_loss: 0.6947558522224426

('adam', 0.1, 8, 50)
train_loss: 0.6354596018791199, val_loss: 0.6945626139640808

('adam', 0.1, 8, 100)
train_loss: 0.6383432149887085, val_loss: 0.6671571731567383

('adam', 0.1, 16, 10)
train_loss: 0.64302659034729, val_loss: 0.7427385449409485

('adam', 0.1, 16, 50)
train_loss: 0.6772561073303223, val_loss: 0.671945333480835

('adam', 0.1, 16, 100)
train_loss: 0.6356105804443359, val_loss: 0.6999143362045288

('adam', 0.1, 32, 10)
train_loss: 0.6362385749816895, val_loss: 0.7134868502616882

('adam', 0.1, 32, 50)
train_loss: 0.6352782249450684, val_loss: 0.7014616131782532

('adam', 0.1, 32, 100)
train_loss: 0.6357118487358093, val_loss: 0.7087717056274414

('adam', 0.1, 64, 10)
train_loss: 0.6451578140258789, val_loss: 0.7688558101654053

('adam', 0.1, 64, 50)
train_loss: 0.6354647874832153, val_loss: 0.6995357275009155

('adam', 0.1, 64, 100)
train_loss: 0.6351086497306824, val_loss: 0.6959845423698425

('sgd', 0.001, 8, 10)
train_loss: 0.7092058658599854, val_loss: 0.7047997713088989

('sgd', 0.001, 8, 50)
train_loss: 0.6787717342376709, val_loss: 0.6885721683502197

('sgd', 0.001, 8, 100)
train_loss: 0.6745399236679077, val_loss: 0.6906743049621582

('sgd', 0.001, 16, 10)
train_loss: 0.6798392534255981, val_loss: 0.6847246289253235

('sgd', 0.001, 16, 50)
train_loss: 0.6903543472290039, val_loss: 0.6941598057746887

('sgd', 0.001, 16, 100)
train_loss: 0.6739158630371094, val_loss: 0.6859441995620728

('sgd', 0.001, 32, 10)
train_loss: 0.7045795917510986, val_loss: 0.7009116411209106

('sgd', 0.001, 32, 50)
train_loss: 0.6932575702667236, val_loss: 0.6947489976882935

('sgd', 0.001, 32, 100)
train_loss: 0.712099015712738, val_loss: 0.7086929678916931

('sgd', 0.001, 64, 10)
train_loss: 0.6725623607635498, val_loss: 0.6791582703590393

('sgd', 0.001, 64, 50)
train_loss: 0.7187327742576599, val_loss: 0.710435152053833

('sgd', 0.001, 64, 100)
train_loss: 0.6988197565078735, val_loss: 0.6984424591064453

('sgd', 0.01, 8, 10)
train_loss: 0.7012463808059692, val_loss: 0.7101489305496216

('sgd', 0.01, 8, 50)
train_loss: 0.6669899821281433, val_loss: 0.7224594354629517

('sgd', 0.01, 8, 100)
train_loss: 0.6420446038246155, val_loss: 0.7183062434196472

('sgd', 0.01, 16, 10)
train_loss: 0.6921388506889343, val_loss: 0.6993705034255981

('sgd', 0.01, 16, 50)
train_loss: 0.6509488224983215, val_loss: 0.6904846429824829

('sgd', 0.01, 16, 100)
train_loss: 0.6500602960586548, val_loss: 0.7164713144302368

('sgd', 0.01, 32, 10)
train_loss: 0.6669141054153442, val_loss: 0.6776747703552246

('sgd', 0.01, 32, 50)
train_loss: 0.6897374391555786, val_loss: 0.7061875462532043

('sgd', 0.01, 32, 100)
train_loss: 0.6527084112167358, val_loss: 0.6888920068740845

('sgd', 0.01, 64, 10)
train_loss: 0.6780965328216553, val_loss: 0.684287428855896

('sgd', 0.01, 64, 50)
train_loss: 0.6996283531188965, val_loss: 0.7052736282348633

('sgd', 0.01, 64, 100)
train_loss: 0.6587719917297363, val_loss: 0.6819857954978943

('sgd', 0.1, 8, 10)
train_loss: 0.6475690603256226, val_loss: 0.707861065864563

('sgd', 0.1, 8, 50)
train_loss: 0.6454769968986511, val_loss: 0.7547564506530762

('sgd', 0.1, 8, 100)
train_loss: 0.6467517018318176, val_loss: 0.728037416934967

('sgd', 0.1, 16, 10)
train_loss: 0.6505894064903259, val_loss: 0.7113401293754578

('sgd', 0.1, 16, 50)
train_loss: 0.6398886442184448, val_loss: 0.7370771169662476

('sgd', 0.1, 16, 100)
train_loss: 0.6591705679893494, val_loss: 0.7199820876121521

('sgd', 0.1, 32, 10)
train_loss: 0.6563315987586975, val_loss: 0.6927099227905273

('sgd', 0.1, 32, 50)
train_loss: 0.6403185725212097, val_loss: 0.7159239649772644

('sgd', 0.1, 32, 100)
train_loss: 0.6536354422569275, val_loss: 0.7576882839202881

('sgd', 0.1, 64, 10)
train_loss: 0.6660491824150085, val_loss: 0.6876220107078552

('sgd', 0.1, 64, 50)
train_loss: 0.6604695916175842, val_loss: 0.7325661778450012

('sgd', 0.1, 64, 100)
train_loss: 0.6424340009689331, val_loss: 0.725416362285614


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6671571731567383
---------

Run from 2023-04-17 16:03:11.607941
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6639623641967773, val_loss: 0.7038869857788086

('adam', 0.001, 8, 50)
train_loss: 0.6727012395858765, val_loss: 0.6951996088027954

('adam', 0.001, 8, 100)
train_loss: 0.6520818471908569, val_loss: 0.7083991765975952

('adam', 0.001, 16, 10)
train_loss: 0.7067238092422485, val_loss: 0.6890023946762085

('adam', 0.001, 16, 50)
train_loss: 0.6945759654045105, val_loss: 0.6886269450187683

('adam', 0.001, 16, 100)
train_loss: 0.6509072184562683, val_loss: 0.7085562944412231

('adam', 0.001, 32, 10)
train_loss: 0.6803568005561829, val_loss: 0.6967483162879944

('adam', 0.001, 32, 50)
train_loss: 0.6873599886894226, val_loss: 0.6915812492370605

('adam', 0.001, 32, 100)
train_loss: 0.6746871471405029, val_loss: 0.6944994926452637

('adam', 0.001, 64, 10)
train_loss: 0.6838425993919373, val_loss: 0.6957457065582275

('adam', 0.001, 64, 50)
train_loss: 0.693752110004425, val_loss: 0.6910284757614136

('adam', 0.001, 64, 100)
train_loss: 0.6929877996444702, val_loss: 0.6897428631782532

('adam', 0.01, 8, 10)
train_loss: 0.6660856604576111, val_loss: 0.7010255455970764

('adam', 0.01, 8, 50)
train_loss: 0.628758430480957, val_loss: 0.7570431232452393

('adam', 0.01, 8, 100)
train_loss: 0.6257899403572083, val_loss: 0.7618024349212646

('adam', 0.01, 16, 10)
train_loss: 0.6637641191482544, val_loss: 0.7004380822181702

('adam', 0.01, 16, 50)
train_loss: 0.6411988139152527, val_loss: 0.7437172532081604

('adam', 0.01, 16, 100)
train_loss: 0.6274906396865845, val_loss: 0.7594459056854248

('adam', 0.01, 32, 10)
train_loss: 0.66599440574646, val_loss: 0.6993898153305054

('adam', 0.01, 32, 50)
train_loss: 0.6382743120193481, val_loss: 0.7423583269119263

('adam', 0.01, 32, 100)
train_loss: 0.6424731016159058, val_loss: 0.748053789138794

('adam', 0.01, 64, 10)
train_loss: 0.6994848251342773, val_loss: 0.6884536743164062

('adam', 0.01, 64, 50)
train_loss: 0.6385095715522766, val_loss: 0.7272731065750122

('adam', 0.01, 64, 100)
train_loss: 0.6412566304206848, val_loss: 0.742013692855835

('adam', 0.1, 8, 10)
train_loss: 0.6284210681915283, val_loss: 0.7628863453865051

('adam', 0.1, 8, 50)
train_loss: 0.620823860168457, val_loss: 0.7822930812835693

('adam', 0.1, 8, 100)
train_loss: 0.6202433109283447, val_loss: 0.7919602990150452

('adam', 0.1, 16, 10)
train_loss: 0.6307448744773865, val_loss: 0.7485506534576416

('adam', 0.1, 16, 50)
train_loss: 0.6208125352859497, val_loss: 0.7773019075393677

('adam', 0.1, 16, 100)
train_loss: 0.6201860308647156, val_loss: 0.7845700979232788

('adam', 0.1, 32, 10)
train_loss: 0.6455484628677368, val_loss: 0.792121946811676

('adam', 0.1, 32, 50)
train_loss: 0.6207917928695679, val_loss: 0.783821702003479

('adam', 0.1, 32, 100)
train_loss: 0.6214941143989563, val_loss: 0.7703003883361816

('adam', 0.1, 64, 10)
train_loss: 0.635430097579956, val_loss: 0.7922266721725464

('adam', 0.1, 64, 50)
train_loss: 0.6238032579421997, val_loss: 0.7661288976669312

('adam', 0.1, 64, 100)
train_loss: 0.6204366683959961, val_loss: 0.7793067693710327

('sgd', 0.001, 8, 10)
train_loss: 0.6844435930252075, val_loss: 0.6955429315567017

('sgd', 0.001, 8, 50)
train_loss: 0.6677546501159668, val_loss: 0.7016209363937378

('sgd', 0.001, 8, 100)
train_loss: 0.6689363718032837, val_loss: 0.6993501782417297

('sgd', 0.001, 16, 10)
train_loss: 0.6790489554405212, val_loss: 0.6978317499160767

('sgd', 0.001, 16, 50)
train_loss: 0.6812863349914551, val_loss: 0.6961140632629395

('sgd', 0.001, 16, 100)
train_loss: 0.665285050868988, val_loss: 0.7030602097511292

('sgd', 0.001, 32, 10)
train_loss: 0.6906130313873291, val_loss: 0.6937514543533325

('sgd', 0.001, 32, 50)
train_loss: 0.6997302174568176, val_loss: 0.6907206773757935

('sgd', 0.001, 32, 100)
train_loss: 0.668677031993866, val_loss: 0.701686441898346

('sgd', 0.001, 64, 10)
train_loss: 0.6673828959465027, val_loss: 0.7036949396133423

('sgd', 0.001, 64, 50)
train_loss: 0.6815385818481445, val_loss: 0.6966779232025146

('sgd', 0.001, 64, 100)
train_loss: 0.7216052412986755, val_loss: 0.6870282888412476

('sgd', 0.01, 8, 10)
train_loss: 0.6768400073051453, val_loss: 0.6957530975341797

('sgd', 0.01, 8, 50)
train_loss: 0.6446207165718079, val_loss: 0.7150343656539917

('sgd', 0.01, 8, 100)
train_loss: 0.6343348622322083, val_loss: 0.7335878610610962

('sgd', 0.01, 16, 10)
train_loss: 0.6731136441230774, val_loss: 0.6989383697509766

('sgd', 0.01, 16, 50)
train_loss: 0.667766273021698, val_loss: 0.6974552869796753

('sgd', 0.01, 16, 100)
train_loss: 0.6668537855148315, val_loss: 0.699912428855896

('sgd', 0.01, 32, 10)
train_loss: 0.7042048573493958, val_loss: 0.6892334818840027

('sgd', 0.01, 32, 50)
train_loss: 0.6947399973869324, val_loss: 0.6879605054855347

('sgd', 0.01, 32, 100)
train_loss: 0.6747141480445862, val_loss: 0.6941417455673218

('sgd', 0.01, 64, 10)
train_loss: 0.6872819066047668, val_loss: 0.6943327188491821

('sgd', 0.01, 64, 50)
train_loss: 0.6595705151557922, val_loss: 0.7057870626449585

('sgd', 0.01, 64, 100)
train_loss: 0.6848171353340149, val_loss: 0.6909391283988953

('sgd', 0.1, 8, 10)
train_loss: 0.6548900604248047, val_loss: 0.7195104360580444

('sgd', 0.1, 8, 50)
train_loss: 0.6337901949882507, val_loss: 0.751660943031311

('sgd', 0.1, 8, 100)
train_loss: 0.6375753283500671, val_loss: 0.750035285949707

('sgd', 0.1, 16, 10)
train_loss: 0.6551200151443481, val_loss: 0.7072429060935974

('sgd', 0.1, 16, 50)
train_loss: 0.6430279016494751, val_loss: 0.7434134483337402

('sgd', 0.1, 16, 100)
train_loss: 0.6426834464073181, val_loss: 0.7464814186096191

('sgd', 0.1, 32, 10)
train_loss: 0.6504138112068176, val_loss: 0.7096691131591797

('sgd', 0.1, 32, 50)
train_loss: 0.6332603693008423, val_loss: 0.7427828311920166

('sgd', 0.1, 32, 100)
train_loss: 0.6361182928085327, val_loss: 0.7537426948547363

('sgd', 0.1, 64, 10)
train_loss: 0.6930150985717773, val_loss: 0.6889133453369141

('sgd', 0.1, 64, 50)
train_loss: 0.6521943807601929, val_loss: 0.7158128023147583

('sgd', 0.1, 64, 100)
train_loss: 0.6383535265922546, val_loss: 0.7395228743553162


---------
BEST MODEL
('sgd', 0.001, 64, 100)
val_loss: 0.6870282888412476
---------
