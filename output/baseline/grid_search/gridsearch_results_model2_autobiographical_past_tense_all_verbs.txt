
Run from 2023-04-02 11:01:58.365675
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.49742433428764343, val_loss: 0.7983219623565674

('adam', 0.01, 50, 200)
train_loss: 0.46705758571624756, val_loss: 0.9077591896057129

('adam', 0.01, 50, 300)
train_loss: 0.470468133687973, val_loss: 0.9224643707275391

('adam', 0.01, 100, 100)
train_loss: 0.4646352529525757, val_loss: 0.8958533406257629

('adam', 0.01, 100, 200)
train_loss: 0.47087934613227844, val_loss: 0.9062337875366211

('adam', 0.01, 100, 300)
train_loss: 0.46761777997016907, val_loss: 0.9115961194038391

('adam', 0.01, 200, 100)
train_loss: 0.5600713491439819, val_loss: 0.7426531910896301

('adam', 0.01, 200, 200)
train_loss: 0.49241557717323303, val_loss: 0.8506565093994141

('adam', 0.01, 200, 300)
train_loss: 0.464769572019577, val_loss: 0.8984823226928711

('adam', 0.1, 50, 100)
train_loss: 0.46256232261657715, val_loss: 0.8866497874259949

('adam', 0.1, 50, 200)
train_loss: 0.46231263875961304, val_loss: 0.887091338634491

('adam', 0.1, 50, 300)
train_loss: 0.46091073751449585, val_loss: 0.8805723190307617

('adam', 0.1, 100, 100)
train_loss: 0.47357550263404846, val_loss: 0.9396727681159973

('adam', 0.1, 100, 200)
train_loss: 0.4651966691017151, val_loss: 0.9004926085472107

('adam', 0.1, 100, 300)
train_loss: 0.4632542133331299, val_loss: 0.8915022015571594

('adam', 0.1, 200, 100)
train_loss: 0.4620937407016754, val_loss: 0.8846127390861511

('adam', 0.1, 200, 200)
train_loss: 0.46181589365005493, val_loss: 0.884783923625946

('adam', 0.1, 200, 300)
train_loss: 0.4560021460056305, val_loss: 0.8570146560668945

('adam', 0.2, 50, 100)
train_loss: 0.4625316858291626, val_loss: 0.8891117572784424

('adam', 0.2, 50, 200)
train_loss: 0.4581853449344635, val_loss: 0.8676436543464661

('adam', 0.2, 50, 300)
train_loss: 0.4547271728515625, val_loss: 0.84964519739151

('adam', 0.2, 100, 100)
train_loss: 0.4596908390522003, val_loss: 0.874854564666748

('adam', 0.2, 100, 200)
train_loss: 0.45481863617897034, val_loss: 0.8505363464355469

('adam', 0.2, 100, 300)
train_loss: 0.4561440348625183, val_loss: 0.8577280044555664

('adam', 0.2, 200, 100)
train_loss: 0.46670016646385193, val_loss: 0.9047573208808899

('adam', 0.2, 200, 200)
train_loss: 0.4607531428337097, val_loss: 0.8797731995582581

('adam', 0.2, 200, 300)
train_loss: 0.45581376552581787, val_loss: 0.8560600280761719


---------
BEST MODEL
('adam', 0.01, 200, 100)
val_loss: 0.7426531910896301
---------

Run from 2023-04-05 15:34:53.909109
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.5180652141571045, val_loss: 0.7374243140220642

('adam', 0.001, 8, 50)
train_loss: 0.6875049471855164, val_loss: 0.6951319575309753

('adam', 0.001, 8, 100)
train_loss: 0.7423515319824219, val_loss: 0.6986749172210693

('adam', 0.001, 16, 10)
train_loss: 0.5024803280830383, val_loss: 0.7514284253120422

('adam', 0.001, 16, 50)
train_loss: 0.7259907722473145, val_loss: 0.6950907707214355

('adam', 0.001, 16, 100)
train_loss: 0.5262503623962402, val_loss: 0.7378813624382019

('adam', 0.001, 32, 10)
train_loss: 0.7375295758247375, val_loss: 0.6953256726264954

('adam', 0.001, 32, 50)
train_loss: 0.8784502744674683, val_loss: 0.7148378491401672

('adam', 0.001, 32, 100)
train_loss: 0.7812119126319885, val_loss: 0.6997928619384766

('adam', 0.001, 64, 10)
train_loss: 0.5151016116142273, val_loss: 0.7386391758918762

('adam', 0.001, 64, 50)
train_loss: 0.8004817962646484, val_loss: 0.7022067904472351

('adam', 0.001, 64, 100)
train_loss: 0.5376409888267517, val_loss: 0.725529670715332

('adam', 0.01, 8, 10)
train_loss: 0.7578868865966797, val_loss: 0.6988511681556702

('adam', 0.01, 8, 50)
train_loss: 0.48296648263931274, val_loss: 0.883676290512085

('adam', 0.01, 8, 100)
train_loss: 0.4637151062488556, val_loss: 0.892279326915741

('adam', 0.01, 16, 10)
train_loss: 0.5873292684555054, val_loss: 0.7111803889274597

('adam', 0.01, 16, 50)
train_loss: 0.514071524143219, val_loss: 0.7981918454170227

('adam', 0.01, 16, 100)
train_loss: 0.47466841340065, val_loss: 0.9158423542976379

('adam', 0.01, 32, 10)
train_loss: 0.57851243019104, val_loss: 0.7115432620048523

('adam', 0.01, 32, 50)
train_loss: 0.46639928221702576, val_loss: 0.8826484084129333

('adam', 0.01, 32, 100)
train_loss: 0.47395777702331543, val_loss: 0.8951253890991211

('adam', 0.01, 64, 10)
train_loss: 0.49627721309661865, val_loss: 0.763272225856781

('adam', 0.01, 64, 50)
train_loss: 0.6557068228721619, val_loss: 0.701525866985321

('adam', 0.01, 64, 100)
train_loss: 0.4906383454799652, val_loss: 0.8095847964286804

('adam', 0.1, 8, 10)
train_loss: 0.46355322003364563, val_loss: 0.8663375973701477

('adam', 0.1, 8, 50)
train_loss: 0.46286091208457947, val_loss: 0.859363853931427

('adam', 0.1, 8, 100)
train_loss: 0.4646136164665222, val_loss: 0.8840048909187317

('adam', 0.1, 16, 10)
train_loss: 0.48268479108810425, val_loss: 1.0485929250717163

('adam', 0.1, 16, 50)
train_loss: 0.4652694761753082, val_loss: 0.8579394221305847

('adam', 0.1, 16, 100)
train_loss: 0.45962467789649963, val_loss: 0.9017059206962585

('adam', 0.1, 32, 10)
train_loss: 0.4760213792324066, val_loss: 1.018930435180664

('adam', 0.1, 32, 50)
train_loss: 0.46629467606544495, val_loss: 0.8477630615234375

('adam', 0.1, 32, 100)
train_loss: 0.47224587202072144, val_loss: 0.8560085296630859

('adam', 0.1, 64, 10)
train_loss: 0.4803245961666107, val_loss: 0.8673748970031738

('adam', 0.1, 64, 50)
train_loss: 0.4619802236557007, val_loss: 0.8927273750305176

('adam', 0.1, 64, 100)
train_loss: 0.4734022319316864, val_loss: 0.9405179023742676


---------
BEST MODEL
('adam', 0.001, 16, 50)
val_loss: 0.6950907707214355
---------

Run from 2023-04-05 16:36:19.138386
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.5081486701965332, val_loss: 0.7463094592094421

('adam', 0.001, 8, 50)
train_loss: 0.4853549301624298, val_loss: 0.7802529335021973

('adam', 0.001, 8, 100)
train_loss: 0.5448607206344604, val_loss: 0.7315213084220886

('adam', 0.001, 16, 10)
train_loss: 0.5318164825439453, val_loss: 0.7270665168762207

('adam', 0.001, 16, 50)
train_loss: 0.6776805520057678, val_loss: 0.6943348050117493

('adam', 0.001, 16, 100)
train_loss: 0.7250364422798157, val_loss: 0.6961305737495422

('adam', 0.001, 32, 10)
train_loss: 0.6265231966972351, val_loss: 0.6964810490608215

('adam', 0.001, 32, 50)
train_loss: 0.6216220259666443, val_loss: 0.698276698589325

('adam', 0.001, 32, 100)
train_loss: 0.5068888664245605, val_loss: 0.7517198920249939

('adam', 0.001, 64, 10)
train_loss: 0.6981350183486938, val_loss: 0.6932974457740784

('adam', 0.001, 64, 50)
train_loss: 0.754787266254425, val_loss: 0.6968805193901062

('adam', 0.001, 64, 100)
train_loss: 0.5093010067939758, val_loss: 0.7468547821044922

('adam', 0.01, 8, 10)
train_loss: 0.49294331669807434, val_loss: 0.7846525311470032

('adam', 0.01, 8, 50)
train_loss: 0.4812537133693695, val_loss: 0.8891190886497498

('adam', 0.01, 8, 100)
train_loss: 0.46898746490478516, val_loss: 0.9203548431396484

('adam', 0.01, 16, 10)
train_loss: 0.6150280237197876, val_loss: 0.7041636109352112

('adam', 0.01, 16, 50)
train_loss: 0.49186787009239197, val_loss: 0.829249382019043

('adam', 0.01, 16, 100)
train_loss: 0.4772585332393646, val_loss: 0.9336560368537903

('adam', 0.01, 32, 10)
train_loss: 0.5883672833442688, val_loss: 0.7081804275512695

('adam', 0.01, 32, 50)
train_loss: 0.46690329909324646, val_loss: 0.8758401870727539

('adam', 0.01, 32, 100)
train_loss: 0.471540242433548, val_loss: 0.8934407234191895

('adam', 0.01, 64, 10)
train_loss: 0.5610982775688171, val_loss: 0.7151792645454407

('adam', 0.01, 64, 50)
train_loss: 0.47056493163108826, val_loss: 0.8373207449913025

('adam', 0.01, 64, 100)
train_loss: 0.4835309088230133, val_loss: 0.8238838315010071

('adam', 0.1, 8, 10)
train_loss: 0.4656910002231598, val_loss: 0.8793394565582275

('adam', 0.1, 8, 50)
train_loss: 0.47238993644714355, val_loss: 0.9481108784675598

('adam', 0.1, 8, 100)
train_loss: 0.45943135023117065, val_loss: 0.8864390254020691

('adam', 0.1, 16, 10)
train_loss: 0.4703858494758606, val_loss: 0.9129898548126221

('adam', 0.1, 16, 50)
train_loss: 0.4660702645778656, val_loss: 0.9050345420837402

('adam', 0.1, 16, 100)
train_loss: 0.4626597464084625, val_loss: 0.8943716883659363

('adam', 0.1, 32, 10)
train_loss: 0.4794216752052307, val_loss: 0.9770901799201965

('adam', 0.1, 32, 50)
train_loss: 0.4717904031276703, val_loss: 0.9081435203552246

('adam', 0.1, 32, 100)
train_loss: 0.4668276309967041, val_loss: 0.8508660197257996

('adam', 0.1, 64, 10)
train_loss: 0.46746325492858887, val_loss: 0.9739730954170227

('adam', 0.1, 64, 50)
train_loss: 0.4669768810272217, val_loss: 0.9017267227172852

('adam', 0.1, 64, 100)
train_loss: 0.4593265950679779, val_loss: 0.8796517252922058


---------
BEST MODEL
('adam', 0.001, 64, 10)
val_loss: 0.6932974457740784
---------

Run from 2023-04-10 10:12:29.812886
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6245985627174377, val_loss: 0.6971413493156433

('adam', 0.001, 8, 50)
train_loss: 0.5132235884666443, val_loss: 0.7467395663261414

('adam', 0.001, 8, 100)
train_loss: 0.5260215401649475, val_loss: 0.7437122464179993

('adam', 0.001, 16, 10)
train_loss: 0.932468593120575, val_loss: 0.7266300320625305

('adam', 0.001, 16, 50)
train_loss: 0.9027841687202454, val_loss: 0.7192156314849854

('adam', 0.001, 16, 100)
train_loss: 0.6868847012519836, val_loss: 0.6957632899284363

('adam', 0.001, 32, 10)
train_loss: 0.6214886903762817, val_loss: 0.6971551775932312

('adam', 0.001, 32, 50)
train_loss: 0.9434655904769897, val_loss: 0.728384256362915

('adam', 0.001, 32, 100)
train_loss: 0.5247155427932739, val_loss: 0.7363996505737305

('adam', 0.001, 64, 10)
train_loss: 0.7461916208267212, val_loss: 0.6960521340370178

('adam', 0.001, 64, 50)
train_loss: 0.5150313973426819, val_loss: 0.7400000691413879

('adam', 0.001, 64, 100)
train_loss: 0.5946090817451477, val_loss: 0.7034589648246765

('adam', 0.01, 8, 10)
train_loss: 0.5654972195625305, val_loss: 0.724616289138794

('adam', 0.01, 8, 50)
train_loss: 0.4698180556297302, val_loss: 0.9098103046417236

('adam', 0.01, 8, 100)
train_loss: 0.4667169153690338, val_loss: 0.9026945233345032

('adam', 0.01, 16, 10)
train_loss: 0.5104939937591553, val_loss: 0.7546539902687073

('adam', 0.01, 16, 50)
train_loss: 0.47291314601898193, val_loss: 0.8717654347419739

('adam', 0.01, 16, 100)
train_loss: 0.4655385911464691, val_loss: 0.9084323048591614

('adam', 0.01, 32, 10)
train_loss: 0.4921039342880249, val_loss: 0.7742168307304382

('adam', 0.01, 32, 50)
train_loss: 0.5819342732429504, val_loss: 0.7314419150352478

('adam', 0.01, 32, 100)
train_loss: 0.47702422738075256, val_loss: 0.8793843388557434

('adam', 0.01, 64, 10)
train_loss: 0.5152952075004578, val_loss: 0.7430745959281921

('adam', 0.01, 64, 50)
train_loss: 0.4806174635887146, val_loss: 0.8078968524932861

('adam', 0.01, 64, 100)
train_loss: 0.4799257516860962, val_loss: 0.8325944542884827

('adam', 0.1, 8, 10)
train_loss: 0.46481844782829285, val_loss: 0.8895575404167175

('adam', 0.1, 8, 50)
train_loss: 0.46298137307167053, val_loss: 0.878193199634552

('adam', 0.1, 8, 100)
train_loss: 0.46090927720069885, val_loss: 0.8898608684539795

('adam', 0.1, 16, 10)
train_loss: 0.46831417083740234, val_loss: 0.9257137179374695

('adam', 0.1, 16, 50)
train_loss: 0.46240100264549255, val_loss: 0.8809377551078796

('adam', 0.1, 16, 100)
train_loss: 0.461942583322525, val_loss: 0.9033686518669128

('adam', 0.1, 32, 10)
train_loss: 0.47026920318603516, val_loss: 0.9359493851661682

('adam', 0.1, 32, 50)
train_loss: 0.4670984745025635, val_loss: 0.9628276228904724

('adam', 0.1, 32, 100)
train_loss: 0.4704013764858246, val_loss: 0.9571720957756042

('adam', 0.1, 64, 10)
train_loss: 0.46707233786582947, val_loss: 0.9562650322914124

('adam', 0.1, 64, 50)
train_loss: 0.4649668335914612, val_loss: 0.916201651096344

('adam', 0.1, 64, 100)
train_loss: 0.46404361724853516, val_loss: 0.900820255279541


---------
BEST MODEL
('adam', 0.001, 16, 100)
val_loss: 0.6957632899284363
---------

Run from 2023-04-10 13:08:08.442375
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6878905296325684, val_loss: 0.6932263374328613

('adam', 0.001, 8, 50)
train_loss: 0.8054879903793335, val_loss: 0.7018840312957764

('adam', 0.001, 8, 100)
train_loss: 0.4775245487689972, val_loss: 0.8277309536933899

('adam', 0.001, 16, 10)
train_loss: 0.8211614489555359, val_loss: 0.7056794166564941

('adam', 0.001, 16, 50)
train_loss: 0.6618852019309998, val_loss: 0.694683849811554

('adam', 0.001, 16, 100)
train_loss: 0.5298822522163391, val_loss: 0.7446163296699524

('adam', 0.001, 32, 10)
train_loss: 0.5493114590644836, val_loss: 0.7263287901878357

('adam', 0.001, 32, 50)
train_loss: 0.7670800685882568, val_loss: 0.6974494457244873

('adam', 0.001, 32, 100)
train_loss: 0.6777448654174805, val_loss: 0.6937925219535828

('adam', 0.001, 64, 10)
train_loss: 0.5316599607467651, val_loss: 0.7385969161987305

('adam', 0.001, 64, 50)
train_loss: 0.994331955909729, val_loss: 0.7479241490364075

('adam', 0.001, 64, 100)
train_loss: 0.6565311551094055, val_loss: 0.6949463486671448

('adam', 0.01, 8, 10)
train_loss: 0.5407366156578064, val_loss: 0.742210865020752

('adam', 0.01, 8, 50)
train_loss: 0.46826401352882385, val_loss: 0.9338006377220154

('adam', 0.01, 8, 100)
train_loss: 0.4658193588256836, val_loss: 0.9424123764038086

('adam', 0.01, 16, 10)
train_loss: 0.5081383585929871, val_loss: 0.7708349823951721

('adam', 0.01, 16, 50)
train_loss: 0.4653242528438568, val_loss: 0.9248821139335632

('adam', 0.01, 16, 100)
train_loss: 0.4749300181865692, val_loss: 0.9311947822570801

('adam', 0.01, 32, 10)
train_loss: 0.49765661358833313, val_loss: 0.7843649387359619

('adam', 0.01, 32, 50)
train_loss: 0.4830201268196106, val_loss: 0.8298479914665222

('adam', 0.01, 32, 100)
train_loss: 0.4759865701198578, val_loss: 0.8810270428657532

('adam', 0.01, 64, 10)
train_loss: 0.4982311725616455, val_loss: 0.7797183394432068

('adam', 0.01, 64, 50)
train_loss: 0.49735209345817566, val_loss: 0.791165292263031

('adam', 0.01, 64, 100)
train_loss: 0.5236976146697998, val_loss: 0.766730546951294

('adam', 0.1, 8, 10)
train_loss: 0.46594882011413574, val_loss: 0.9329845309257507

('adam', 0.1, 8, 50)
train_loss: 0.4642609655857086, val_loss: 0.9259791970252991

('adam', 0.1, 8, 100)
train_loss: 0.4567835032939911, val_loss: 0.9267470240592957

('adam', 0.1, 16, 10)
train_loss: 0.46705105900764465, val_loss: 0.9792799353599548

('adam', 0.1, 16, 50)
train_loss: 0.46809637546539307, val_loss: 0.9798145294189453

('adam', 0.1, 16, 100)
train_loss: 0.46054530143737793, val_loss: 0.9203630089759827

('adam', 0.1, 32, 10)
train_loss: 0.47455185651779175, val_loss: 1.027215600013733

('adam', 0.1, 32, 50)
train_loss: 0.46118736267089844, val_loss: 0.9462828040122986

('adam', 0.1, 32, 100)
train_loss: 0.4624153971672058, val_loss: 0.9405953288078308

('adam', 0.1, 64, 10)
train_loss: 0.46748843789100647, val_loss: 0.9551852345466614

('adam', 0.1, 64, 50)
train_loss: 0.4625604450702667, val_loss: 0.9609783291816711

('adam', 0.1, 64, 100)
train_loss: 0.46508753299713135, val_loss: 0.9435498714447021


---------
BEST MODEL
('adam', 0.001, 8, 10)
val_loss: 0.6932263374328613
---------

Run from 2023-04-10 14:32:44.582415
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7492775917053223, val_loss: 0.702610194683075

('adam', 0.001, 8, 50)
train_loss: 0.7816370129585266, val_loss: 0.7145264744758606

('adam', 0.001, 8, 100)
train_loss: 0.5658941268920898, val_loss: 0.719498336315155

('adam', 0.001, 16, 10)
train_loss: 0.9515021443367004, val_loss: 0.7563347816467285

('adam', 0.001, 16, 50)
train_loss: 0.735222578048706, val_loss: 0.7032982707023621

('adam', 0.001, 16, 100)
train_loss: 0.5315071940422058, val_loss: 0.7281894683837891

('adam', 0.001, 32, 10)
train_loss: 0.6788356900215149, val_loss: 0.6923732161521912

('adam', 0.001, 32, 50)
train_loss: 0.8010907769203186, val_loss: 0.7148818969726562

('adam', 0.001, 32, 100)
train_loss: 0.8112642168998718, val_loss: 0.7199857234954834

('adam', 0.001, 64, 10)
train_loss: 0.7003418207168579, val_loss: 0.6942417025566101

('adam', 0.001, 64, 50)
train_loss: 0.7511489987373352, val_loss: 0.7032694220542908

('adam', 0.001, 64, 100)
train_loss: 0.5008434653282166, val_loss: 0.7475563883781433

('adam', 0.01, 8, 10)
train_loss: 0.5397312641143799, val_loss: 0.7338398098945618

('adam', 0.01, 8, 50)
train_loss: 0.48238322138786316, val_loss: 0.8844594359397888

('adam', 0.01, 8, 100)
train_loss: 0.47216135263442993, val_loss: 0.9120104908943176

('adam', 0.01, 16, 10)
train_loss: 0.611544132232666, val_loss: 0.7042391896247864

('adam', 0.01, 16, 50)
train_loss: 0.48484620451927185, val_loss: 0.8407197594642639

('adam', 0.01, 16, 100)
train_loss: 0.4713954031467438, val_loss: 0.9319446086883545

('adam', 0.01, 32, 10)
train_loss: 0.6588134765625, val_loss: 0.6976475119590759

('adam', 0.01, 32, 50)
train_loss: 0.46916529536247253, val_loss: 0.8828757405281067

('adam', 0.01, 32, 100)
train_loss: 0.4740726947784424, val_loss: 0.8921201825141907

('adam', 0.01, 64, 10)
train_loss: 0.5150800943374634, val_loss: 0.7340614199638367

('adam', 0.01, 64, 50)
train_loss: 0.7274790406227112, val_loss: 0.7136160731315613

('adam', 0.01, 64, 100)
train_loss: 0.5317474007606506, val_loss: 0.762211799621582

('adam', 0.1, 8, 10)
train_loss: 0.4743715524673462, val_loss: 0.9169462323188782

('adam', 0.1, 8, 50)
train_loss: 0.4719022512435913, val_loss: 0.9303507804870605

('adam', 0.1, 8, 100)
train_loss: 0.46863240003585815, val_loss: 0.885312020778656

('adam', 0.1, 16, 10)
train_loss: 0.4843696653842926, val_loss: 1.043642282485962

('adam', 0.1, 16, 50)
train_loss: 0.47450387477874756, val_loss: 0.8897560238838196

('adam', 0.1, 16, 100)
train_loss: 0.47100505232810974, val_loss: 0.8661827445030212

('adam', 0.1, 32, 10)
train_loss: 0.48735547065734863, val_loss: 1.0738964080810547

('adam', 0.1, 32, 50)
train_loss: 0.4743026793003082, val_loss: 0.8032053112983704

('adam', 0.1, 32, 100)
train_loss: 0.47151851654052734, val_loss: 0.8523421287536621

('adam', 0.1, 64, 10)
train_loss: 0.5075688362121582, val_loss: 0.8115518093109131

('adam', 0.1, 64, 50)
train_loss: 0.47449883818626404, val_loss: 0.9026269912719727

('adam', 0.1, 64, 100)
train_loss: 0.46804186701774597, val_loss: 0.8725359439849854


---------
BEST MODEL
('adam', 0.001, 32, 10)
val_loss: 0.6923732161521912
---------

Run from 2023-04-17 09:58:58.887194
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6668168902397156, val_loss: 0.6938619017601013

('adam', 0.001, 8, 50)
train_loss: 0.6770915985107422, val_loss: 0.6957328915596008

('adam', 0.001, 8, 100)
train_loss: 0.5845966935157776, val_loss: 0.7208379507064819

('adam', 0.001, 16, 10)
train_loss: 0.6865700483322144, val_loss: 0.6932589411735535

('adam', 0.001, 16, 50)
train_loss: 0.6781663298606873, val_loss: 0.6944190263748169

('adam', 0.001, 16, 100)
train_loss: 0.5336878299713135, val_loss: 0.7626951932907104

('adam', 0.001, 32, 10)
train_loss: 0.7466073632240295, val_loss: 0.6974228620529175

('adam', 0.001, 32, 50)
train_loss: 0.5642011761665344, val_loss: 0.7249085307121277

('adam', 0.001, 32, 100)
train_loss: 0.6108009815216064, val_loss: 0.7044247388839722

('adam', 0.001, 64, 10)
train_loss: 0.9826133251190186, val_loss: 0.7546374201774597

('adam', 0.001, 64, 50)
train_loss: 0.9135066270828247, val_loss: 0.7333257794380188

('adam', 0.001, 64, 100)
train_loss: 0.5619456171989441, val_loss: 0.7267042994499207

('adam', 0.01, 8, 10)
train_loss: 0.5212564468383789, val_loss: 0.7976831197738647

('adam', 0.01, 8, 50)
train_loss: 0.5202097296714783, val_loss: 0.869250476360321

('adam', 0.01, 8, 100)
train_loss: 0.5192445516586304, val_loss: 0.8982440829277039

('adam', 0.01, 16, 10)
train_loss: 0.5298756957054138, val_loss: 0.7714823484420776

('adam', 0.01, 16, 50)
train_loss: 0.5130300521850586, val_loss: 0.8560649156570435

('adam', 0.01, 16, 100)
train_loss: 0.5156323909759521, val_loss: 0.8788606524467468

('adam', 0.01, 32, 10)
train_loss: 0.6679916381835938, val_loss: 0.6957438588142395

('adam', 0.01, 32, 50)
train_loss: 0.5545968413352966, val_loss: 0.7566455602645874

('adam', 0.01, 32, 100)
train_loss: 0.5149745345115662, val_loss: 0.8615933656692505

('adam', 0.01, 64, 10)
train_loss: 0.8923783898353577, val_loss: 0.7258189916610718

('adam', 0.01, 64, 50)
train_loss: 0.5645527839660645, val_loss: 0.7353739738464355

('adam', 0.01, 64, 100)
train_loss: 0.5146521329879761, val_loss: 0.8497376441955566

('adam', 0.1, 8, 10)
train_loss: 0.5161300897598267, val_loss: 0.861605167388916

('adam', 0.1, 8, 50)
train_loss: 0.5164927244186401, val_loss: 0.8717249035835266

('adam', 0.1, 8, 100)
train_loss: 0.5177507996559143, val_loss: 0.9103635549545288

('adam', 0.1, 16, 10)
train_loss: 0.5293546319007874, val_loss: 1.0072450637817383

('adam', 0.1, 16, 50)
train_loss: 0.5148114562034607, val_loss: 0.8605577349662781

('adam', 0.1, 16, 100)
train_loss: 0.5139991044998169, val_loss: 0.8844717741012573

('adam', 0.1, 32, 10)
train_loss: 0.5255407691001892, val_loss: 0.9860098958015442

('adam', 0.1, 32, 50)
train_loss: 0.5143581032752991, val_loss: 0.8769403696060181

('adam', 0.1, 32, 100)
train_loss: 0.5126835703849792, val_loss: 0.8819828033447266

('adam', 0.1, 64, 10)
train_loss: 0.5187349915504456, val_loss: 0.8625289797782898

('adam', 0.1, 64, 50)
train_loss: 0.5125014185905457, val_loss: 0.8679758906364441

('adam', 0.1, 64, 100)
train_loss: 0.5112544298171997, val_loss: 0.8566719889640808

('sgd', 0.001, 8, 10)
train_loss: 0.5715495347976685, val_loss: 0.7180579304695129

('sgd', 0.001, 8, 50)
train_loss: 0.5685340166091919, val_loss: 0.7208648920059204

('sgd', 0.001, 8, 100)
train_loss: 0.7465121746063232, val_loss: 0.6984415054321289

('sgd', 0.001, 16, 10)
train_loss: 0.9917439222335815, val_loss: 0.7576001882553101

('sgd', 0.001, 16, 50)
train_loss: 0.6875206828117371, val_loss: 0.6933960914611816

('sgd', 0.001, 16, 100)
train_loss: 0.7668519616127014, val_loss: 0.7005676031112671

('sgd', 0.001, 32, 10)
train_loss: 0.59126216173172, val_loss: 0.7079290151596069

('sgd', 0.001, 32, 50)
train_loss: 0.5813989639282227, val_loss: 0.7126896977424622

('sgd', 0.001, 32, 100)
train_loss: 0.6637647747993469, val_loss: 0.6940000057220459

('sgd', 0.001, 64, 10)
train_loss: 0.6168793439865112, val_loss: 0.6997413635253906

('sgd', 0.001, 64, 50)
train_loss: 0.9008700847625732, val_loss: 0.7300605773925781

('sgd', 0.001, 64, 100)
train_loss: 0.9593561291694641, val_loss: 0.7470873594284058

('sgd', 0.01, 8, 10)
train_loss: 0.8136330842971802, val_loss: 0.7078925371170044

('sgd', 0.01, 8, 50)
train_loss: 0.6035086512565613, val_loss: 0.7251883745193481

('sgd', 0.01, 8, 100)
train_loss: 0.5293117761611938, val_loss: 0.7993303537368774

('sgd', 0.01, 16, 10)
train_loss: 0.7907681465148926, val_loss: 0.7039231657981873

('sgd', 0.01, 16, 50)
train_loss: 0.60700923204422, val_loss: 0.7096868753433228

('sgd', 0.01, 16, 100)
train_loss: 0.5324726104736328, val_loss: 0.7705423831939697

('sgd', 0.01, 32, 10)
train_loss: 0.7532564997673035, val_loss: 0.6982806921005249

('sgd', 0.01, 32, 50)
train_loss: 0.6853666305541992, val_loss: 0.6962493658065796

('sgd', 0.01, 32, 100)
train_loss: 0.6206921935081482, val_loss: 0.7100937962532043

('sgd', 0.01, 64, 10)
train_loss: 0.7137031555175781, val_loss: 0.6942960023880005

('sgd', 0.01, 64, 50)
train_loss: 0.8130394816398621, val_loss: 0.7084507942199707

('sgd', 0.01, 64, 100)
train_loss: 0.5498397350311279, val_loss: 0.7386717796325684

('sgd', 0.1, 8, 10)
train_loss: 0.5200580954551697, val_loss: 0.8113036155700684

('sgd', 0.1, 8, 50)
train_loss: 0.5218570232391357, val_loss: 0.9103239178657532

('sgd', 0.1, 8, 100)
train_loss: 0.5160006284713745, val_loss: 0.9041898846626282

('sgd', 0.1, 16, 10)
train_loss: 0.6086633801460266, val_loss: 0.7267157435417175

('sgd', 0.1, 16, 50)
train_loss: 0.5133396983146667, val_loss: 0.8680191040039062

('sgd', 0.1, 16, 100)
train_loss: 0.520980715751648, val_loss: 0.8912757039070129

('sgd', 0.1, 32, 10)
train_loss: 0.5957295298576355, val_loss: 0.7198594212532043

('sgd', 0.1, 32, 50)
train_loss: 0.5139129757881165, val_loss: 0.8460357785224915

('sgd', 0.1, 32, 100)
train_loss: 0.5196794271469116, val_loss: 0.8855738639831543

('sgd', 0.1, 64, 10)
train_loss: 0.6436964869499207, val_loss: 0.6999754905700684

('sgd', 0.1, 64, 50)
train_loss: 0.5393974781036377, val_loss: 0.7844773530960083

('sgd', 0.1, 64, 100)
train_loss: 0.517693281173706, val_loss: 0.8495560884475708


---------
BEST MODEL
('adam', 0.001, 16, 10)
val_loss: 0.6932589411735535
---------

Run from 2023-04-17 16:44:35.750730
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6492567658424377, val_loss: 0.7003477215766907

('adam', 0.001, 8, 50)
train_loss: 0.5718241333961487, val_loss: 0.7401034235954285

('adam', 0.001, 8, 100)
train_loss: 0.5596917271614075, val_loss: 0.7540125846862793

('adam', 0.001, 16, 10)
train_loss: 0.6587653756141663, val_loss: 0.6983963847160339

('adam', 0.001, 16, 50)
train_loss: 0.6133676767349243, val_loss: 0.7119151949882507

('adam', 0.001, 16, 100)
train_loss: 0.7056043744087219, val_loss: 0.6833724975585938

('adam', 0.001, 32, 10)
train_loss: 0.6907348036766052, val_loss: 0.692918598651886

('adam', 0.001, 32, 50)
train_loss: 0.5747690796852112, val_loss: 0.7381508946418762

('adam', 0.001, 32, 100)
train_loss: 0.5366187691688538, val_loss: 0.789664089679718

('adam', 0.001, 64, 10)
train_loss: 0.5586515069007874, val_loss: 0.7548131942749023

('adam', 0.001, 64, 50)
train_loss: 0.6013128161430359, val_loss: 0.7196515202522278

('adam', 0.001, 64, 100)
train_loss: 0.8836829662322998, val_loss: 0.6988328099250793

('adam', 0.01, 8, 10)
train_loss: 0.5538747906684875, val_loss: 0.7658575177192688

('adam', 0.01, 8, 50)
train_loss: 0.5261908173561096, val_loss: 0.878349781036377

('adam', 0.01, 8, 100)
train_loss: 0.5275706648826599, val_loss: 0.8892856240272522

('adam', 0.01, 16, 10)
train_loss: 0.5499275326728821, val_loss: 0.7695145010948181

('adam', 0.01, 16, 50)
train_loss: 0.5162041187286377, val_loss: 0.8944390416145325

('adam', 0.01, 16, 100)
train_loss: 0.5202471613883972, val_loss: 0.8949071764945984

('adam', 0.01, 32, 10)
train_loss: 0.7453382015228271, val_loss: 0.6836283802986145

('adam', 0.01, 32, 50)
train_loss: 0.5190010666847229, val_loss: 0.8838503360748291

('adam', 0.01, 32, 100)
train_loss: 0.537195086479187, val_loss: 0.8344063758850098

('adam', 0.01, 64, 10)
train_loss: 0.571036159992218, val_loss: 0.7430598139762878

('adam', 0.01, 64, 50)
train_loss: 0.5502528548240662, val_loss: 0.7696962356567383

('adam', 0.01, 64, 100)
train_loss: 0.5206747055053711, val_loss: 0.8829447627067566

('adam', 0.1, 8, 10)
train_loss: 0.517845094203949, val_loss: 0.8990399837493896

('adam', 0.1, 8, 50)
train_loss: 0.5123189687728882, val_loss: 0.885966956615448

('adam', 0.1, 8, 100)
train_loss: 0.5046553611755371, val_loss: 0.9212077260017395

('adam', 0.1, 16, 10)
train_loss: 0.5427834987640381, val_loss: 0.9885621070861816

('adam', 0.1, 16, 50)
train_loss: 0.5193977952003479, val_loss: 0.8965970873832703

('adam', 0.1, 16, 100)
train_loss: 0.5038807988166809, val_loss: 0.9216229319572449

('adam', 0.1, 32, 10)
train_loss: 0.5389273166656494, val_loss: 0.9532778263092041

('adam', 0.1, 32, 50)
train_loss: 0.5169165134429932, val_loss: 0.9627626538276672

('adam', 0.1, 32, 100)
train_loss: 0.5150165557861328, val_loss: 0.9135718941688538

('adam', 0.1, 64, 10)
train_loss: 0.533457338809967, val_loss: 0.8554190993309021

('adam', 0.1, 64, 50)
train_loss: 0.5167170166969299, val_loss: 0.9067556858062744

('adam', 0.1, 64, 100)
train_loss: 0.522178053855896, val_loss: 0.887887716293335

('sgd', 0.001, 8, 10)
train_loss: 0.6396633982658386, val_loss: 0.7038936614990234

('sgd', 0.001, 8, 50)
train_loss: 0.7715205550193787, val_loss: 0.6867232918739319

('sgd', 0.001, 8, 100)
train_loss: 0.7666868567466736, val_loss: 0.6825557351112366

('sgd', 0.001, 16, 10)
train_loss: 0.5544784069061279, val_loss: 0.7596890926361084

('sgd', 0.001, 16, 50)
train_loss: 0.9126067757606506, val_loss: 0.705711305141449

('sgd', 0.001, 16, 100)
train_loss: 0.598859965801239, val_loss: 0.7208750247955322

('sgd', 0.001, 32, 10)
train_loss: 0.7292537093162537, val_loss: 0.6903212666511536

('sgd', 0.001, 32, 50)
train_loss: 0.8626527786254883, val_loss: 0.6983463168144226

('sgd', 0.001, 32, 100)
train_loss: 0.8696691393852234, val_loss: 0.6975342631340027

('sgd', 0.001, 64, 10)
train_loss: 0.5402572154998779, val_loss: 0.7807424664497375

('sgd', 0.001, 64, 50)
train_loss: 0.741641104221344, val_loss: 0.6897074580192566

('sgd', 0.001, 64, 100)
train_loss: 0.5602461695671082, val_loss: 0.7527963519096375

('sgd', 0.01, 8, 10)
train_loss: 0.6264407634735107, val_loss: 0.7067196369171143

('sgd', 0.01, 8, 50)
train_loss: 0.530353307723999, val_loss: 0.8076262474060059

('sgd', 0.01, 8, 100)
train_loss: 0.5354680418968201, val_loss: 0.8110947012901306

('sgd', 0.01, 16, 10)
train_loss: 0.7627036571502686, val_loss: 0.6866013407707214

('sgd', 0.01, 16, 50)
train_loss: 0.5820087194442749, val_loss: 0.7305433750152588

('sgd', 0.01, 16, 100)
train_loss: 0.6246969699859619, val_loss: 0.6972866654396057

('sgd', 0.01, 32, 10)
train_loss: 0.5946249961853027, val_loss: 0.7240554690361023

('sgd', 0.01, 32, 50)
train_loss: 0.6173641681671143, val_loss: 0.7083343863487244

('sgd', 0.01, 32, 100)
train_loss: 0.5383424162864685, val_loss: 0.7867177128791809

('sgd', 0.01, 64, 10)
train_loss: 0.8515397906303406, val_loss: 0.6965451240539551

('sgd', 0.01, 64, 50)
train_loss: 0.5711081027984619, val_loss: 0.7414584159851074

('sgd', 0.01, 64, 100)
train_loss: 0.5642417073249817, val_loss: 0.7479212880134583

('sgd', 0.1, 8, 10)
train_loss: 0.5239095687866211, val_loss: 0.849901020526886

('sgd', 0.1, 8, 50)
train_loss: 0.5186215043067932, val_loss: 0.8971483707427979

('sgd', 0.1, 8, 100)
train_loss: 0.5355226993560791, val_loss: 0.8788936734199524

('sgd', 0.1, 16, 10)
train_loss: 0.5375089049339294, val_loss: 0.7946710586547852

('sgd', 0.1, 16, 50)
train_loss: 0.5268226265907288, val_loss: 0.8695012927055359

('sgd', 0.1, 16, 100)
train_loss: 0.5191434621810913, val_loss: 0.8855136036872864

('sgd', 0.1, 32, 10)
train_loss: 0.5839797258377075, val_loss: 0.7323100566864014

('sgd', 0.1, 32, 50)
train_loss: 0.5391502976417542, val_loss: 0.8368106484413147

('sgd', 0.1, 32, 100)
train_loss: 0.5233305096626282, val_loss: 0.8907206654548645

('sgd', 0.1, 64, 10)
train_loss: 0.7984049320220947, val_loss: 0.6763893961906433

('sgd', 0.1, 64, 50)
train_loss: 0.5465406179428101, val_loss: 0.7852314114570618

('sgd', 0.1, 64, 100)
train_loss: 0.5200343728065491, val_loss: 0.8685469627380371


---------
BEST MODEL
('sgd', 0.1, 64, 10)
val_loss: 0.6763893961906433
---------
