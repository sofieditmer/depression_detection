
Run from 2023-04-02 10:49:30.831244
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6312788724899292, val_loss: 0.738612711429596

('adam', 0.01, 50, 200)
train_loss: 0.6240350008010864, val_loss: 0.7636374235153198

('adam', 0.01, 50, 300)
train_loss: 0.6219000220298767, val_loss: 0.7843880653381348

('adam', 0.01, 100, 100)
train_loss: 0.6287937164306641, val_loss: 0.7418955564498901

('adam', 0.01, 100, 200)
train_loss: 0.6226949095726013, val_loss: 0.7739866375923157

('adam', 0.01, 100, 300)
train_loss: 0.6221765875816345, val_loss: 0.7797335386276245

('adam', 0.01, 200, 100)
train_loss: 0.6247991323471069, val_loss: 0.7568097710609436

('adam', 0.01, 200, 200)
train_loss: 0.622032880783081, val_loss: 0.7815083265304565

('adam', 0.01, 200, 300)
train_loss: 0.622150719165802, val_loss: 0.7806167602539062

('adam', 0.1, 50, 100)
train_loss: 0.6215870380401611, val_loss: 0.7973394989967346

('adam', 0.1, 50, 200)
train_loss: 0.6215865015983582, val_loss: 0.7967057228088379

('adam', 0.1, 50, 300)
train_loss: 0.6215865612030029, val_loss: 0.7967047095298767

('adam', 0.1, 100, 100)
train_loss: 0.621586799621582, val_loss: 0.7963194847106934

('adam', 0.1, 100, 200)
train_loss: 0.6215865015983582, val_loss: 0.7967045903205872

('adam', 0.1, 100, 300)
train_loss: 0.6215865612030029, val_loss: 0.7967045903205872

('adam', 0.1, 200, 100)
train_loss: 0.6215882897377014, val_loss: 0.7977896928787231

('adam', 0.1, 200, 200)
train_loss: 0.6215865612030029, val_loss: 0.7967033386230469

('adam', 0.1, 200, 300)
train_loss: 0.6215865612030029, val_loss: 0.7967047095298767

('adam', 0.2, 50, 100)
train_loss: 0.6215869188308716, val_loss: 0.7970170974731445

('adam', 0.2, 50, 200)
train_loss: 0.6215865612030029, val_loss: 0.7967063188552856

('adam', 0.2, 50, 300)
train_loss: 0.6215865612030029, val_loss: 0.7967045903205872

('adam', 0.2, 100, 100)
train_loss: 0.6215870380401611, val_loss: 0.7970741987228394

('adam', 0.2, 100, 200)
train_loss: 0.6215865612030029, val_loss: 0.7967046499252319

('adam', 0.2, 100, 300)
train_loss: 0.6215865015983582, val_loss: 0.7967046499252319

('adam', 0.2, 200, 100)
train_loss: 0.6215872168540955, val_loss: 0.7973097562789917

('adam', 0.2, 200, 200)
train_loss: 0.6215865612030029, val_loss: 0.7967067956924438

('adam', 0.2, 200, 300)
train_loss: 0.6215865612030029, val_loss: 0.7967045903205872


---------
BEST MODEL
('adam', 0.01, 50, 100)
val_loss: 0.738612711429596
---------

Run from 2023-04-05 15:09:05.231991
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6735828518867493, val_loss: 0.6758843660354614

('adam', 0.001, 8, 50)
train_loss: 0.6582942008972168, val_loss: 0.6809849143028259

('adam', 0.001, 8, 100)
train_loss: 0.6476289629936218, val_loss: 0.6989141702651978

('adam', 0.001, 16, 10)
train_loss: 0.6738712787628174, val_loss: 0.6729752421379089

('adam', 0.001, 16, 50)
train_loss: 0.6601758599281311, val_loss: 0.6721266508102417

('adam', 0.001, 16, 100)
train_loss: 0.6627346873283386, val_loss: 0.708590567111969

('adam', 0.001, 32, 10)
train_loss: 0.6801174283027649, val_loss: 0.6799556612968445

('adam', 0.001, 32, 50)
train_loss: 0.6652558445930481, val_loss: 0.6752718091011047

('adam', 0.001, 32, 100)
train_loss: 0.6879624724388123, val_loss: 0.7283777594566345

('adam', 0.001, 64, 10)
train_loss: 0.6726699471473694, val_loss: 0.6680803298950195

('adam', 0.001, 64, 50)
train_loss: 0.6693042516708374, val_loss: 0.6729097962379456

('adam', 0.001, 64, 100)
train_loss: 0.6713535189628601, val_loss: 0.687015950679779

('adam', 0.01, 8, 10)
train_loss: 0.6476436257362366, val_loss: 0.6928521990776062

('adam', 0.01, 8, 50)
train_loss: 0.6216029524803162, val_loss: 0.797673761844635

('adam', 0.01, 8, 100)
train_loss: 0.6298898458480835, val_loss: 0.737901508808136

('adam', 0.01, 16, 10)
train_loss: 0.6510240435600281, val_loss: 0.6924464106559753

('adam', 0.01, 16, 50)
train_loss: 0.6330483555793762, val_loss: 0.7360664010047913

('adam', 0.01, 16, 100)
train_loss: 0.630530834197998, val_loss: 0.7417191863059998

('adam', 0.01, 32, 10)
train_loss: 0.6591678857803345, val_loss: 0.683862030506134

('adam', 0.01, 32, 50)
train_loss: 0.6387260556221008, val_loss: 0.7240690588951111

('adam', 0.01, 32, 100)
train_loss: 0.6265119910240173, val_loss: 0.7696985602378845

('adam', 0.01, 64, 10)
train_loss: 0.6606590151786804, val_loss: 0.6716267466545105

('adam', 0.01, 64, 50)
train_loss: 0.635735809803009, val_loss: 0.7127049565315247

('adam', 0.01, 64, 100)
train_loss: 0.6275597810745239, val_loss: 0.744172990322113

('adam', 0.1, 8, 10)
train_loss: 0.6244372129440308, val_loss: 0.8117843270301819

('adam', 0.1, 8, 50)
train_loss: 0.6229673027992249, val_loss: 0.7813076376914978

('adam', 0.1, 8, 100)
train_loss: 0.6261743307113647, val_loss: 0.760825514793396

('adam', 0.1, 16, 10)
train_loss: 0.6310657858848572, val_loss: 0.7473987936973572

('adam', 0.1, 16, 50)
train_loss: 0.6218379139900208, val_loss: 0.7951011657714844

('adam', 0.1, 16, 100)
train_loss: 0.6217560768127441, val_loss: 0.7915807366371155

('adam', 0.1, 32, 10)
train_loss: 0.6366578340530396, val_loss: 0.8300355076789856

('adam', 0.1, 32, 50)
train_loss: 0.6207338571548462, val_loss: 0.7868252992630005

('adam', 0.1, 32, 100)
train_loss: 0.6221528649330139, val_loss: 0.807586669921875

('adam', 0.1, 64, 10)
train_loss: 0.6234215497970581, val_loss: 0.7856047749519348

('adam', 0.1, 64, 50)
train_loss: 0.6216320395469666, val_loss: 0.8015584945678711

('adam', 0.1, 64, 100)
train_loss: 0.6212647557258606, val_loss: 0.7987046241760254


---------
BEST MODEL
('adam', 0.001, 64, 10)
val_loss: 0.6680803298950195
---------

Run from 2023-04-05 16:09:35.688781
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7035292387008667, val_loss: 0.717998206615448

('adam', 0.001, 8, 50)
train_loss: 0.6708077788352966, val_loss: 0.7103571891784668

('adam', 0.001, 8, 100)
train_loss: 0.6496577858924866, val_loss: 0.6851526498794556

('adam', 0.001, 16, 10)
train_loss: 0.6906110048294067, val_loss: 0.6954790353775024

('adam', 0.001, 16, 50)
train_loss: 0.66617351770401, val_loss: 0.68470299243927

('adam', 0.001, 16, 100)
train_loss: 0.6546180248260498, val_loss: 0.6942740082740784

('adam', 0.001, 32, 10)
train_loss: 0.7184061408042908, val_loss: 0.7313453555107117

('adam', 0.001, 32, 50)
train_loss: 0.6680371761322021, val_loss: 0.6797409653663635

('adam', 0.001, 32, 100)
train_loss: 0.6845836639404297, val_loss: 0.7208715081214905

('adam', 0.001, 64, 10)
train_loss: 0.6788689494132996, val_loss: 0.6762882471084595

('adam', 0.001, 64, 50)
train_loss: 0.7157055139541626, val_loss: 0.7353622317314148

('adam', 0.001, 64, 100)
train_loss: 0.6643294095993042, val_loss: 0.6773524880409241

('adam', 0.01, 8, 10)
train_loss: 0.6591047048568726, val_loss: 0.7211480736732483

('adam', 0.01, 8, 50)
train_loss: 0.6278507709503174, val_loss: 0.7484975457191467

('adam', 0.01, 8, 100)
train_loss: 0.6300386786460876, val_loss: 0.7448230385780334

('adam', 0.01, 16, 10)
train_loss: 0.653056800365448, val_loss: 0.6891905665397644

('adam', 0.01, 16, 50)
train_loss: 0.627156674861908, val_loss: 0.7462162375450134

('adam', 0.01, 16, 100)
train_loss: 0.629851758480072, val_loss: 0.7454055547714233

('adam', 0.01, 32, 10)
train_loss: 0.6643151640892029, val_loss: 0.693665087223053

('adam', 0.01, 32, 50)
train_loss: 0.6264822483062744, val_loss: 0.746073305606842

('adam', 0.01, 32, 100)
train_loss: 0.6306972503662109, val_loss: 0.7523444890975952

('adam', 0.01, 64, 10)
train_loss: 0.722790002822876, val_loss: 0.7542372941970825

('adam', 0.01, 64, 50)
train_loss: 0.6390950083732605, val_loss: 0.721496045589447

('adam', 0.01, 64, 100)
train_loss: 0.6275351047515869, val_loss: 0.7442252039909363

('adam', 0.1, 8, 10)
train_loss: 0.622472882270813, val_loss: 0.788120687007904

('adam', 0.1, 8, 50)
train_loss: 0.6224806308746338, val_loss: 0.7923313975334167

('adam', 0.1, 8, 100)
train_loss: 0.6204677820205688, val_loss: 0.7809044718742371

('adam', 0.1, 16, 10)
train_loss: 0.6273980736732483, val_loss: 0.7441064715385437

('adam', 0.1, 16, 50)
train_loss: 0.6219452619552612, val_loss: 0.7917557954788208

('adam', 0.1, 16, 100)
train_loss: 0.621559739112854, val_loss: 0.7901929020881653

('adam', 0.1, 32, 10)
train_loss: 0.6262926459312439, val_loss: 0.8106425404548645

('adam', 0.1, 32, 50)
train_loss: 0.6217005848884583, val_loss: 0.7864169478416443

('adam', 0.1, 32, 100)
train_loss: 0.6218682527542114, val_loss: 0.8158223032951355

('adam', 0.1, 64, 10)
train_loss: 0.637130856513977, val_loss: 0.7761768102645874

('adam', 0.1, 64, 50)
train_loss: 0.6215946674346924, val_loss: 0.8015228509902954

('adam', 0.1, 64, 100)
train_loss: 0.6212644577026367, val_loss: 0.79939204454422


---------
BEST MODEL
('adam', 0.001, 64, 10)
val_loss: 0.6762882471084595
---------

Run from 2023-04-10 09:29:25.161650
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6686705350875854, val_loss: 0.6642668843269348

('adam', 0.001, 8, 50)
train_loss: 0.6791165471076965, val_loss: 0.7216157913208008

('adam', 0.001, 8, 100)
train_loss: 0.6457445621490479, val_loss: 0.6984643936157227

('adam', 0.001, 16, 10)
train_loss: 0.681827187538147, val_loss: 0.6839128732681274

('adam', 0.001, 16, 50)
train_loss: 0.6755279898643494, val_loss: 0.6987618803977966

('adam', 0.001, 16, 100)
train_loss: 0.6526617407798767, val_loss: 0.6810697317123413

('adam', 0.001, 32, 10)
train_loss: 0.6694164276123047, val_loss: 0.6651172041893005

('adam', 0.001, 32, 50)
train_loss: 0.6668862104415894, val_loss: 0.6773242354393005

('adam', 0.001, 32, 100)
train_loss: 0.6869605183601379, val_loss: 0.7259112596511841

('adam', 0.001, 64, 10)
train_loss: 0.6997112035751343, val_loss: 0.7040883898735046

('adam', 0.001, 64, 50)
train_loss: 0.6799154877662659, val_loss: 0.6871750950813293

('adam', 0.001, 64, 100)
train_loss: 0.696388840675354, val_loss: 0.7211353182792664

('adam', 0.01, 8, 10)
train_loss: 0.6585186719894409, val_loss: 0.7180272936820984

('adam', 0.01, 8, 50)
train_loss: 0.6222102642059326, val_loss: 0.7751182913780212

('adam', 0.01, 8, 100)
train_loss: 0.6283212304115295, val_loss: 0.7523329854011536

('adam', 0.01, 16, 10)
train_loss: 0.656834602355957, val_loss: 0.7019310593605042

('adam', 0.01, 16, 50)
train_loss: 0.6263302564620972, val_loss: 0.7467869520187378

('adam', 0.01, 16, 100)
train_loss: 0.6267308592796326, val_loss: 0.7527707815170288

('adam', 0.01, 32, 10)
train_loss: 0.6665559411048889, val_loss: 0.6959308981895447

('adam', 0.01, 32, 50)
train_loss: 0.6366686224937439, val_loss: 0.7203398942947388

('adam', 0.01, 32, 100)
train_loss: 0.6274767518043518, val_loss: 0.7502325773239136

('adam', 0.01, 64, 10)
train_loss: 0.6773508191108704, val_loss: 0.6935842633247375

('adam', 0.01, 64, 50)
train_loss: 0.6385717988014221, val_loss: 0.7118218541145325

('adam', 0.01, 64, 100)
train_loss: 0.6301581263542175, val_loss: 0.7388550043106079

('adam', 0.1, 8, 10)
train_loss: 0.6238718628883362, val_loss: 0.8067918419837952

('adam', 0.1, 8, 50)
train_loss: 0.6220042109489441, val_loss: 0.8097477555274963

('adam', 0.1, 8, 100)
train_loss: 0.6231957674026489, val_loss: 0.8001990914344788

('adam', 0.1, 16, 10)
train_loss: 0.629075288772583, val_loss: 0.7412574887275696

('adam', 0.1, 16, 50)
train_loss: 0.6221511960029602, val_loss: 0.81053626537323

('adam', 0.1, 16, 100)
train_loss: 0.6218973994255066, val_loss: 0.7917149662971497

('adam', 0.1, 32, 10)
train_loss: 0.6469481587409973, val_loss: 0.8138260841369629

('adam', 0.1, 32, 50)
train_loss: 0.622668445110321, val_loss: 0.7958311438560486

('adam', 0.1, 32, 100)
train_loss: 0.6217366456985474, val_loss: 0.7911600470542908

('adam', 0.1, 64, 10)
train_loss: 0.622787356376648, val_loss: 0.7860516309738159

('adam', 0.1, 64, 50)
train_loss: 0.6213810443878174, val_loss: 0.7976323366165161

('adam', 0.1, 64, 100)
train_loss: 0.6212655901908875, val_loss: 0.7991631627082825


---------
BEST MODEL
('adam', 0.001, 8, 10)
val_loss: 0.6642668843269348
---------

Run from 2023-04-10 12:34:32.766568
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7035074234008789, val_loss: 0.6975679397583008

('adam', 0.001, 8, 50)
train_loss: 0.6574268341064453, val_loss: 0.6957690119743347

('adam', 0.001, 8, 100)
train_loss: 0.6325820088386536, val_loss: 0.7074628472328186

('adam', 0.001, 16, 10)
train_loss: 0.6639576554298401, val_loss: 0.6891874670982361

('adam', 0.001, 16, 50)
train_loss: 0.6634048223495483, val_loss: 0.6941973567008972

('adam', 0.001, 16, 100)
train_loss: 0.6683123707771301, val_loss: 0.701014518737793

('adam', 0.001, 32, 10)
train_loss: 0.7184150218963623, val_loss: 0.7010761499404907

('adam', 0.001, 32, 50)
train_loss: 0.6515448689460754, val_loss: 0.6919686198234558

('adam', 0.001, 32, 100)
train_loss: 0.6570470929145813, val_loss: 0.6959668397903442

('adam', 0.001, 64, 10)
train_loss: 0.7629880905151367, val_loss: 0.7169876098632812

('adam', 0.001, 64, 50)
train_loss: 0.7210969924926758, val_loss: 0.7034732103347778

('adam', 0.001, 64, 100)
train_loss: 0.7305105328559875, val_loss: 0.7088195085525513

('adam', 0.01, 8, 10)
train_loss: 0.630585253238678, val_loss: 0.7089787721633911

('adam', 0.01, 8, 50)
train_loss: 0.6204074025154114, val_loss: 0.7566043138504028

('adam', 0.01, 8, 100)
train_loss: 0.6215670704841614, val_loss: 0.7720797657966614

('adam', 0.01, 16, 10)
train_loss: 0.6398081183433533, val_loss: 0.7011418342590332

('adam', 0.01, 16, 50)
train_loss: 0.6206958889961243, val_loss: 0.7475391626358032

('adam', 0.01, 16, 100)
train_loss: 0.6215595602989197, val_loss: 0.7554548382759094

('adam', 0.01, 32, 10)
train_loss: 0.6393518447875977, val_loss: 0.699086606502533

('adam', 0.01, 32, 50)
train_loss: 0.6214964389801025, val_loss: 0.73753422498703

('adam', 0.01, 32, 100)
train_loss: 0.6219527125358582, val_loss: 0.7640029191970825

('adam', 0.01, 64, 10)
train_loss: 0.66849285364151, val_loss: 0.6931485533714294

('adam', 0.01, 64, 50)
train_loss: 0.6288456916809082, val_loss: 0.7216771245002747

('adam', 0.01, 64, 100)
train_loss: 0.6262226700782776, val_loss: 0.7494841814041138

('adam', 0.1, 8, 10)
train_loss: 0.6264168620109558, val_loss: 0.7924472093582153

('adam', 0.1, 8, 50)
train_loss: 0.6227422952651978, val_loss: 0.7730028033256531

('adam', 0.1, 8, 100)
train_loss: 0.6256439089775085, val_loss: 0.7158673405647278

('adam', 0.1, 16, 10)
train_loss: 0.62907874584198, val_loss: 0.7914085388183594

('adam', 0.1, 16, 50)
train_loss: 0.6209046244621277, val_loss: 0.7614294290542603

('adam', 0.1, 16, 100)
train_loss: 0.620580792427063, val_loss: 0.7516288757324219

('adam', 0.1, 32, 10)
train_loss: 0.6341608762741089, val_loss: 0.8421713709831238

('adam', 0.1, 32, 50)
train_loss: 0.6208899617195129, val_loss: 0.7565056681632996

('adam', 0.1, 32, 100)
train_loss: 0.6220998764038086, val_loss: 0.7779768705368042

('adam', 0.1, 64, 10)
train_loss: 0.6260450482368469, val_loss: 0.7940208315849304

('adam', 0.1, 64, 50)
train_loss: 0.6204930543899536, val_loss: 0.7544177174568176

('adam', 0.1, 64, 100)
train_loss: 0.6204571723937988, val_loss: 0.7530723810195923


---------
BEST MODEL
('adam', 0.001, 16, 10)
val_loss: 0.6891874670982361
---------

Run from 2023-04-10 13:50:19.204784
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7273136377334595, val_loss: 0.7372443079948425

('adam', 0.001, 8, 50)
train_loss: 0.6725026369094849, val_loss: 0.7038441896438599

('adam', 0.001, 8, 100)
train_loss: 0.6495469808578491, val_loss: 0.7134221792221069

('adam', 0.001, 16, 10)
train_loss: 0.7301492691040039, val_loss: 0.7375721335411072

('adam', 0.001, 16, 50)
train_loss: 0.668687105178833, val_loss: 0.6901487708091736

('adam', 0.001, 16, 100)
train_loss: 0.6869919300079346, val_loss: 0.7336762547492981

('adam', 0.001, 32, 10)
train_loss: 0.6704041361808777, val_loss: 0.6756049394607544

('adam', 0.001, 32, 50)
train_loss: 0.7163848280906677, val_loss: 0.7351457476615906

('adam', 0.001, 32, 100)
train_loss: 0.6765817999839783, val_loss: 0.7068637609481812

('adam', 0.001, 64, 10)
train_loss: 0.6729658246040344, val_loss: 0.6767026782035828

('adam', 0.001, 64, 50)
train_loss: 0.7314549684524536, val_loss: 0.744190514087677

('adam', 0.001, 64, 100)
train_loss: 0.6740019917488098, val_loss: 0.6917514801025391

('adam', 0.01, 8, 10)
train_loss: 0.6406075954437256, val_loss: 0.7090393304824829

('adam', 0.01, 8, 50)
train_loss: 0.6231108903884888, val_loss: 0.7674000859260559

('adam', 0.01, 8, 100)
train_loss: 0.6276047825813293, val_loss: 0.7541093826293945

('adam', 0.01, 16, 10)
train_loss: 0.6488712430000305, val_loss: 0.6963366270065308

('adam', 0.01, 16, 50)
train_loss: 0.6270415186882019, val_loss: 0.7460044026374817

('adam', 0.01, 16, 100)
train_loss: 0.6273378133773804, val_loss: 0.7524467706680298

('adam', 0.01, 32, 10)
train_loss: 0.666939914226532, val_loss: 0.696760356426239

('adam', 0.01, 32, 50)
train_loss: 0.6328383684158325, val_loss: 0.7293105721473694

('adam', 0.01, 32, 100)
train_loss: 0.629715621471405, val_loss: 0.7469492554664612

('adam', 0.01, 64, 10)
train_loss: 0.6799356937408447, val_loss: 0.6962728500366211

('adam', 0.01, 64, 50)
train_loss: 0.6358172297477722, val_loss: 0.7166088223457336

('adam', 0.01, 64, 100)
train_loss: 0.6304292678833008, val_loss: 0.7433907389640808

('adam', 0.1, 8, 10)
train_loss: 0.6252555251121521, val_loss: 0.7752447128295898

('adam', 0.1, 8, 50)
train_loss: 0.6289777159690857, val_loss: 0.8258021473884583

('adam', 0.1, 8, 100)
train_loss: 0.6173226237297058, val_loss: 0.7656723856925964

('adam', 0.1, 16, 10)
train_loss: 0.6244361400604248, val_loss: 0.7899444699287415

('adam', 0.1, 16, 50)
train_loss: 0.6240454912185669, val_loss: 0.7699432969093323

('adam', 0.1, 16, 100)
train_loss: 0.6224305033683777, val_loss: 0.7871033549308777

('adam', 0.1, 32, 10)
train_loss: 0.6264541149139404, val_loss: 0.8188349008560181

('adam', 0.1, 32, 50)
train_loss: 0.6240818500518799, val_loss: 0.784844696521759

('adam', 0.1, 32, 100)
train_loss: 0.6227319836616516, val_loss: 0.8137801289558411

('adam', 0.1, 64, 10)
train_loss: 0.6268966197967529, val_loss: 0.7869612574577332

('adam', 0.1, 64, 50)
train_loss: 0.6225293874740601, val_loss: 0.7888323664665222

('adam', 0.1, 64, 100)
train_loss: 0.622236967086792, val_loss: 0.7850532531738281


---------
BEST MODEL
('adam', 0.001, 32, 10)
val_loss: 0.6756049394607544
---------

Run from 2023-04-17 09:36:04.056693
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7146199941635132, val_loss: 0.686448872089386

('adam', 0.001, 8, 50)
train_loss: 0.6952248811721802, val_loss: 0.682961106300354

('adam', 0.001, 8, 100)
train_loss: 0.6468636393547058, val_loss: 0.7083892822265625

('adam', 0.001, 16, 10)
train_loss: 0.6705290675163269, val_loss: 0.7053632140159607

('adam', 0.001, 16, 50)
train_loss: 0.6635799407958984, val_loss: 0.7026280164718628

('adam', 0.001, 16, 100)
train_loss: 0.6507066488265991, val_loss: 0.7105236053466797

('adam', 0.001, 32, 10)
train_loss: 0.688014030456543, val_loss: 0.6941004395484924

('adam', 0.001, 32, 50)
train_loss: 0.6840511560440063, val_loss: 0.6912195086479187

('adam', 0.001, 32, 100)
train_loss: 0.6889433264732361, val_loss: 0.685433566570282


Run from 2023-04-17 09:42:24.888660
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7473728656768799, val_loss: 0.6795629262924194

('adam', 0.001, 8, 50)
train_loss: 0.6642313599586487, val_loss: 0.7053224444389343

('adam', 0.001, 8, 100)
train_loss: 0.6593368649482727, val_loss: 0.7027880549430847

('adam', 0.001, 16, 10)
train_loss: 0.6866279244422913, val_loss: 0.694869339466095

('adam', 0.001, 16, 50)
train_loss: 0.7292122840881348, val_loss: 0.6776569485664368

('adam', 0.001, 16, 100)
train_loss: 0.6912448406219482, val_loss: 0.6836117506027222

('adam', 0.001, 32, 10)
train_loss: 0.6617380380630493, val_loss: 0.7241944074630737

('adam', 0.001, 32, 50)
train_loss: 0.6611018180847168, val_loss: 0.7168561220169067

('adam', 0.001, 32, 100)
train_loss: 0.6530789136886597, val_loss: 0.7251490354537964

('adam', 0.001, 64, 10)
train_loss: 0.7190566658973694, val_loss: 0.6837412118911743

('adam', 0.001, 64, 50)
train_loss: 0.711030125617981, val_loss: 0.6832130551338196

('adam', 0.001, 64, 100)
train_loss: 0.6751487851142883, val_loss: 0.6979619264602661

('adam', 0.01, 8, 10)
train_loss: 0.6735272407531738, val_loss: 0.6892610788345337

('adam', 0.01, 8, 50)
train_loss: 0.641892671585083, val_loss: 0.7787520289421082

('adam', 0.01, 8, 100)
train_loss: 0.6428071856498718, val_loss: 0.7627987265586853

('adam', 0.01, 16, 10)
train_loss: 0.683302104473114, val_loss: 0.6861693859100342

('adam', 0.01, 16, 50)
train_loss: 0.6450872421264648, val_loss: 0.7325345277786255

('adam', 0.01, 16, 100)
train_loss: 0.6494695544242859, val_loss: 0.7508499026298523

('adam', 0.01, 32, 10)
train_loss: 0.6893936991691589, val_loss: 0.6859217882156372

('adam', 0.01, 32, 50)
train_loss: 0.6431717872619629, val_loss: 0.746272087097168

('adam', 0.01, 32, 100)
train_loss: 0.6427918076515198, val_loss: 0.756269633769989

('adam', 0.01, 64, 10)
train_loss: 0.7382955551147461, val_loss: 0.6779594421386719

('adam', 0.01, 64, 50)
train_loss: 0.6500362753868103, val_loss: 0.7175342440605164

('adam', 0.01, 64, 100)
train_loss: 0.6418122053146362, val_loss: 0.775918185710907

('adam', 0.1, 8, 10)
train_loss: 0.6434473991394043, val_loss: 0.7635383605957031

('adam', 0.1, 8, 50)
train_loss: 0.644399106502533, val_loss: 0.7447311282157898

('adam', 0.1, 8, 100)
train_loss: 0.6536067128181458, val_loss: 0.8245689272880554

('adam', 0.1, 16, 10)
train_loss: 0.6458904147148132, val_loss: 0.7856087684631348

('adam', 0.1, 16, 50)
train_loss: 0.6420614719390869, val_loss: 0.7613106369972229

('adam', 0.1, 16, 100)
train_loss: 0.6427441835403442, val_loss: 0.7613679766654968

('adam', 0.1, 32, 10)
train_loss: 0.6494166851043701, val_loss: 0.7661438584327698

('adam', 0.1, 32, 50)
train_loss: 0.643794596195221, val_loss: 0.7761548757553101

('adam', 0.1, 32, 100)
train_loss: 0.6415683031082153, val_loss: 0.767497718334198

('adam', 0.1, 64, 10)
train_loss: 0.6481714248657227, val_loss: 0.7748717069625854

('adam', 0.1, 64, 50)
train_loss: 0.6417405605316162, val_loss: 0.7643154859542847

('adam', 0.1, 64, 100)
train_loss: 0.6415590643882751, val_loss: 0.7677964568138123

('sgd', 0.001, 8, 10)
train_loss: 0.6635918021202087, val_loss: 0.7217415571212769

('sgd', 0.001, 8, 50)
train_loss: 0.7009298205375671, val_loss: 0.685602068901062

('sgd', 0.001, 8, 100)
train_loss: 0.6646654009819031, val_loss: 0.7106752991676331

('sgd', 0.001, 16, 10)
train_loss: 0.7564851641654968, val_loss: 0.6815691590309143

('sgd', 0.001, 16, 50)
train_loss: 0.6688603162765503, val_loss: 0.712166428565979

('sgd', 0.001, 16, 100)
train_loss: 0.6579737067222595, val_loss: 0.7282764315605164

('sgd', 0.001, 32, 10)
train_loss: 0.7048751711845398, val_loss: 0.6878167986869812

('sgd', 0.001, 32, 50)
train_loss: 0.7204963564872742, val_loss: 0.6829484701156616

('sgd', 0.001, 32, 100)
train_loss: 0.6577799916267395, val_loss: 0.7322167158126831

('sgd', 0.001, 64, 10)
train_loss: 0.7192003726959229, val_loss: 0.6841617822647095

('sgd', 0.001, 64, 50)
train_loss: 0.7591161727905273, val_loss: 0.6816667318344116

('sgd', 0.001, 64, 100)
train_loss: 0.7255424857139587, val_loss: 0.6821867227554321

('sgd', 0.01, 8, 10)
train_loss: 0.6552659869194031, val_loss: 0.7329241037368774

('sgd', 0.01, 8, 50)
train_loss: 0.6687593460083008, val_loss: 0.6912996172904968

('sgd', 0.01, 8, 100)
train_loss: 0.6532396078109741, val_loss: 0.7144356966018677

('sgd', 0.01, 16, 10)
train_loss: 0.6978086233139038, val_loss: 0.6859785318374634

('sgd', 0.01, 16, 50)
train_loss: 0.658063530921936, val_loss: 0.7086281776428223

('sgd', 0.01, 16, 100)
train_loss: 0.6479234099388123, val_loss: 0.7352502942085266

('sgd', 0.01, 32, 10)
train_loss: 0.6841397881507874, val_loss: 0.6965686082839966

('sgd', 0.01, 32, 50)
train_loss: 0.6782763600349426, val_loss: 0.6923846006393433

('sgd', 0.01, 32, 100)
train_loss: 0.6962940692901611, val_loss: 0.6783698201179504

('sgd', 0.01, 64, 10)
train_loss: 0.6749035716056824, val_loss: 0.7058531045913696

('sgd', 0.01, 64, 50)
train_loss: 0.6890324950218201, val_loss: 0.6898080706596375

('sgd', 0.01, 64, 100)
train_loss: 0.7215705513954163, val_loss: 0.6760740280151367

('sgd', 0.1, 8, 10)
train_loss: 0.6451870799064636, val_loss: 0.7490764856338501

('sgd', 0.1, 8, 50)
train_loss: 0.6432262063026428, val_loss: 0.7642632722854614

('sgd', 0.1, 8, 100)
train_loss: 0.6501105427742004, val_loss: 0.7295085787773132

('sgd', 0.1, 16, 10)
train_loss: 0.6455756425857544, val_loss: 0.7495748400688171

('sgd', 0.1, 16, 50)
train_loss: 0.6583427786827087, val_loss: 0.7232632040977478

('sgd', 0.1, 16, 100)
train_loss: 0.6426206827163696, val_loss: 0.7599745988845825

('sgd', 0.1, 32, 10)
train_loss: 0.6682279706001282, val_loss: 0.6956331133842468

('sgd', 0.1, 32, 50)
train_loss: 0.6558840870857239, val_loss: 0.7222899198532104

('sgd', 0.1, 32, 100)
train_loss: 0.6481412649154663, val_loss: 0.7411051988601685

('sgd', 0.1, 64, 10)
train_loss: 0.7243806719779968, val_loss: 0.6760368943214417

('sgd', 0.1, 64, 50)
train_loss: 0.6541524529457092, val_loss: 0.7095716595649719

('sgd', 0.1, 64, 100)
train_loss: 0.6439323425292969, val_loss: 0.7406591176986694


---------
BEST MODEL
('sgd', 0.1, 64, 10)
val_loss: 0.6760368943214417
---------

Run from 2023-04-17 16:07:24.360596
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6955611705780029, val_loss: 0.6887001991271973

('adam', 0.001, 8, 50)
train_loss: 0.6955539584159851, val_loss: 0.6772665977478027

('adam', 0.001, 8, 100)
train_loss: 0.6501752138137817, val_loss: 0.7313821911811829

('adam', 0.001, 16, 10)
train_loss: 0.7396161556243896, val_loss: 0.678905725479126

('adam', 0.001, 16, 50)
train_loss: 0.6585284471511841, val_loss: 0.7352416515350342

('adam', 0.001, 16, 100)
train_loss: 0.6539142727851868, val_loss: 0.7351693511009216

('adam', 0.001, 32, 10)
train_loss: 0.7261334657669067, val_loss: 0.6812280416488647

('adam', 0.001, 32, 50)
train_loss: 0.6621548533439636, val_loss: 0.7234088182449341

('adam', 0.001, 32, 100)
train_loss: 0.6624340415000916, val_loss: 0.7075916528701782

('adam', 0.001, 64, 10)
train_loss: 0.6750794649124146, val_loss: 0.7073096036911011

('adam', 0.001, 64, 50)
train_loss: 0.7406872510910034, val_loss: 0.6774318814277649

('adam', 0.001, 64, 100)
train_loss: 0.6590660810470581, val_loss: 0.7302185297012329

('adam', 0.01, 8, 10)
train_loss: 0.6509119272232056, val_loss: 0.7350901365280151

('adam', 0.01, 8, 50)
train_loss: 0.6422417759895325, val_loss: 0.7407960891723633

('adam', 0.01, 8, 100)
train_loss: 0.6419476270675659, val_loss: 0.7481651306152344

('adam', 0.01, 16, 10)
train_loss: 0.654658317565918, val_loss: 0.7438685297966003

('adam', 0.01, 16, 50)
train_loss: 0.6433916687965393, val_loss: 0.7518570423126221

('adam', 0.01, 16, 100)
train_loss: 0.641768753528595, val_loss: 0.7481943964958191

('adam', 0.01, 32, 10)
train_loss: 0.716654896736145, val_loss: 0.6735609769821167

('adam', 0.01, 32, 50)
train_loss: 0.642819344997406, val_loss: 0.7430440783500671

('adam', 0.01, 32, 100)
train_loss: 0.6446540951728821, val_loss: 0.756275475025177

('adam', 0.01, 64, 10)
train_loss: 0.659294068813324, val_loss: 0.7315914630889893

('adam', 0.01, 64, 50)
train_loss: 0.6458280682563782, val_loss: 0.7496436834335327

('adam', 0.01, 64, 100)
train_loss: 0.6439220905303955, val_loss: 0.7535653710365295

('adam', 0.1, 8, 10)
train_loss: 0.643079936504364, val_loss: 0.7430633902549744

('adam', 0.1, 8, 50)
train_loss: 0.6422705054283142, val_loss: 0.7429046630859375

('adam', 0.1, 8, 100)
train_loss: 0.6435349583625793, val_loss: 0.7550472617149353

('adam', 0.1, 16, 10)
train_loss: 0.6445690989494324, val_loss: 0.7457174062728882

('adam', 0.1, 16, 50)
train_loss: 0.6430755257606506, val_loss: 0.7389644384384155

('adam', 0.1, 16, 100)
train_loss: 0.6417754292488098, val_loss: 0.7428510189056396

('adam', 0.1, 32, 10)
train_loss: 0.6482566595077515, val_loss: 0.790045976638794

('adam', 0.1, 32, 50)
train_loss: 0.6419579982757568, val_loss: 0.7563211917877197

('adam', 0.1, 32, 100)
train_loss: 0.6416825652122498, val_loss: 0.7562305927276611

('adam', 0.1, 64, 10)
train_loss: 0.6435168385505676, val_loss: 0.7833751440048218

('adam', 0.1, 64, 50)
train_loss: 0.6413578987121582, val_loss: 0.740809977054596

('adam', 0.1, 64, 100)
train_loss: 0.6412490606307983, val_loss: 0.7451741099357605

('sgd', 0.001, 8, 10)
train_loss: 0.7246825098991394, val_loss: 0.6819276809692383

('sgd', 0.001, 8, 50)
train_loss: 0.7182614207267761, val_loss: 0.6799994707107544

('sgd', 0.001, 8, 100)
train_loss: 0.6652993559837341, val_loss: 0.7156767845153809

('sgd', 0.001, 16, 10)
train_loss: 0.7281787395477295, val_loss: 0.6817386150360107

('sgd', 0.001, 16, 50)
train_loss: 0.6887377500534058, val_loss: 0.693713903427124

('sgd', 0.001, 16, 100)
train_loss: 0.6636636257171631, val_loss: 0.7296122312545776

('sgd', 0.001, 32, 10)
train_loss: 0.6881832480430603, val_loss: 0.695923924446106

('sgd', 0.001, 32, 50)
train_loss: 0.682222306728363, val_loss: 0.6993481516838074

('sgd', 0.001, 32, 100)
train_loss: 0.6904775500297546, val_loss: 0.6918696165084839

('sgd', 0.001, 64, 10)
train_loss: 0.7058908939361572, val_loss: 0.6874116659164429

('sgd', 0.001, 64, 50)
train_loss: 0.7082573771476746, val_loss: 0.6860690116882324

('sgd', 0.001, 64, 100)
train_loss: 0.6806227564811707, val_loss: 0.7007259726524353

('sgd', 0.01, 8, 10)
train_loss: 0.6622748970985413, val_loss: 0.7296718955039978

('sgd', 0.01, 8, 50)
train_loss: 0.6602619290351868, val_loss: 0.6931310892105103

('sgd', 0.01, 8, 100)
train_loss: 0.6477450132369995, val_loss: 0.7473008036613464

('sgd', 0.01, 16, 10)
train_loss: 0.6799665689468384, val_loss: 0.6987684369087219

('sgd', 0.01, 16, 50)
train_loss: 0.6894692778587341, val_loss: 0.677626371383667

('sgd', 0.01, 16, 100)
train_loss: 0.6516908407211304, val_loss: 0.7348334193229675

('sgd', 0.01, 32, 10)
train_loss: 0.7348465323448181, val_loss: 0.6784200072288513

('sgd', 0.01, 32, 50)
train_loss: 0.6594676375389099, val_loss: 0.7296069860458374

('sgd', 0.01, 32, 100)
train_loss: 0.6758961081504822, val_loss: 0.682425856590271

('sgd', 0.01, 64, 10)
train_loss: 0.6872566938400269, val_loss: 0.6955323219299316

('sgd', 0.01, 64, 50)
train_loss: 0.6693992614746094, val_loss: 0.7088108062744141

('sgd', 0.01, 64, 100)
train_loss: 0.66368567943573, val_loss: 0.7116307616233826

('sgd', 0.1, 8, 10)
train_loss: 0.6462793350219727, val_loss: 0.71891188621521

('sgd', 0.1, 8, 50)
train_loss: 0.6447677612304688, val_loss: 0.7407354712486267

('sgd', 0.1, 8, 100)
train_loss: 0.6426496505737305, val_loss: 0.7415235638618469

('sgd', 0.1, 16, 10)
train_loss: 0.6521539688110352, val_loss: 0.7352660894393921

('sgd', 0.1, 16, 50)
train_loss: 0.647453248500824, val_loss: 0.7618637681007385

('sgd', 0.1, 16, 100)
train_loss: 0.6441127061843872, val_loss: 0.7383584976196289

('sgd', 0.1, 32, 10)
train_loss: 0.6695703864097595, val_loss: 0.6891520023345947

('sgd', 0.1, 32, 50)
train_loss: 0.6434745788574219, val_loss: 0.7286431193351746

('sgd', 0.1, 32, 100)
train_loss: 0.6426050066947937, val_loss: 0.7390060424804688

('sgd', 0.1, 64, 10)
train_loss: 0.6718951463699341, val_loss: 0.6988049149513245

('sgd', 0.1, 64, 50)
train_loss: 0.6475210189819336, val_loss: 0.7406517863273621

('sgd', 0.1, 64, 100)
train_loss: 0.6433578133583069, val_loss: 0.7275589108467102


---------
BEST MODEL
('adam', 0.01, 32, 10)
val_loss: 0.6735609769821167
---------
