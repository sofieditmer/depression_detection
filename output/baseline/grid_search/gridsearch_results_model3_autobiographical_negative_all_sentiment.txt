
Run from 2023-04-02 11:03:57.541682
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.5263822674751282, val_loss: 0.9565995335578918

('adam', 0.01, 50, 200)
train_loss: 0.4851454794406891, val_loss: 0.8969130516052246

('adam', 0.01, 50, 300)
train_loss: 0.46878328919410706, val_loss: 0.8613947033882141

('adam', 0.01, 100, 100)
train_loss: 0.5250990390777588, val_loss: 0.9435809254646301

('adam', 0.01, 100, 200)
train_loss: 0.4902556538581848, val_loss: 0.897648811340332

('adam', 0.01, 100, 300)
train_loss: 0.4683397710323334, val_loss: 0.8601804375648499

('adam', 0.01, 200, 100)
train_loss: 0.5166848301887512, val_loss: 0.7834146618843079

('adam', 0.01, 200, 200)
train_loss: 0.4833786189556122, val_loss: 0.863152801990509

('adam', 0.01, 200, 300)
train_loss: 0.4621447026729584, val_loss: 0.8382695317268372

('adam', 0.1, 50, 100)
train_loss: 0.3789805471897125, val_loss: 0.6512631773948669

('adam', 0.1, 50, 200)
train_loss: 0.35512974858283997, val_loss: 0.5782116055488586

('adam', 0.1, 50, 300)
train_loss: 0.34392255544662476, val_loss: 0.541371762752533

('adam', 0.1, 100, 100)
train_loss: 0.3957952558994293, val_loss: 0.6922863125801086

('adam', 0.1, 100, 200)
train_loss: 0.3458453118801117, val_loss: 0.5497294664382935

('adam', 0.1, 100, 300)
train_loss: 0.3391591012477875, val_loss: 0.5240333676338196

('adam', 0.1, 200, 100)
train_loss: 0.3975006639957428, val_loss: 0.6955113410949707

('adam', 0.1, 200, 200)
train_loss: 0.3480846881866455, val_loss: 0.556683361530304

('adam', 0.1, 200, 300)
train_loss: 0.3414483368396759, val_loss: 0.5325837731361389

('adam', 0.2, 50, 100)
train_loss: 0.34397923946380615, val_loss: 0.5431106090545654

('adam', 0.2, 50, 200)
train_loss: 0.3384905755519867, val_loss: 0.5211098790168762

('adam', 0.2, 50, 300)
train_loss: 0.3331933915615082, val_loss: 0.49297451972961426

('adam', 0.2, 100, 100)
train_loss: 0.35818690061569214, val_loss: 0.5897842049598694

('adam', 0.2, 100, 200)
train_loss: 0.33443278074264526, val_loss: 0.5023881793022156

('adam', 0.2, 100, 300)
train_loss: 0.33426016569137573, val_loss: 0.5009229779243469

('adam', 0.2, 200, 100)
train_loss: 0.3442864418029785, val_loss: 0.5441127419471741

('adam', 0.2, 200, 200)
train_loss: 0.33371081948280334, val_loss: 0.4979022443294525

('adam', 0.2, 200, 300)
train_loss: 0.33298808336257935, val_loss: 0.49035516381263733


---------
BEST MODEL
('adam', 0.2, 200, 300)
val_loss: 0.49035516381263733
---------

Run from 2023-04-05 15:39:21.139202
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6826082468032837, val_loss: 0.6910893321037292

('adam', 0.001, 8, 50)
train_loss: 0.5814634561538696, val_loss: 0.8620570302009583

('adam', 0.001, 8, 100)
train_loss: 0.5713546872138977, val_loss: 0.7538816928863525

('adam', 0.001, 16, 10)
train_loss: 0.7972133755683899, val_loss: 0.6554588079452515

('adam', 0.001, 16, 50)
train_loss: 0.7048300504684448, val_loss: 0.6583532691001892

('adam', 0.001, 16, 100)
train_loss: 0.700735330581665, val_loss: 0.6366714835166931

('adam', 0.001, 32, 10)
train_loss: 0.7022426128387451, val_loss: 0.6834335327148438

('adam', 0.001, 32, 50)
train_loss: 0.6009290814399719, val_loss: 0.8311904072761536

('adam', 0.001, 32, 100)
train_loss: 0.6162425875663757, val_loss: 0.7339535355567932

('adam', 0.001, 64, 10)
train_loss: 0.7271577715873718, val_loss: 0.6728509068489075

('adam', 0.001, 64, 50)
train_loss: 0.6007701754570007, val_loss: 0.8611030578613281

('adam', 0.001, 64, 100)
train_loss: 0.7711655497550964, val_loss: 0.6480832099914551

('adam', 0.01, 8, 10)
train_loss: 0.5649198889732361, val_loss: 0.8964594006538391

('adam', 0.01, 8, 50)
train_loss: 0.4746657907962799, val_loss: 0.8385424613952637

('adam', 0.01, 8, 100)
train_loss: 0.4664517343044281, val_loss: 0.8751060962677002

('adam', 0.01, 16, 10)
train_loss: 0.575063169002533, val_loss: 0.8417816758155823

('adam', 0.01, 16, 50)
train_loss: 0.4984451234340668, val_loss: 0.8235319256782532

('adam', 0.01, 16, 100)
train_loss: 0.4807266294956207, val_loss: 0.8827946186065674

('adam', 0.01, 32, 10)
train_loss: 0.587897777557373, val_loss: 0.8451361656188965

('adam', 0.01, 32, 50)
train_loss: 0.5236629843711853, val_loss: 0.7642688751220703

('adam', 0.01, 32, 100)
train_loss: 0.49644359946250916, val_loss: 0.8758732676506042

('adam', 0.01, 64, 10)
train_loss: 0.6007748246192932, val_loss: 0.8275909423828125

('adam', 0.01, 64, 50)
train_loss: 0.5636266469955444, val_loss: 0.7562735676765442

('adam', 0.01, 64, 100)
train_loss: 0.5178450345993042, val_loss: 0.7647258639335632

('adam', 0.1, 8, 10)
train_loss: 0.4759330153465271, val_loss: 0.9164726138114929

('adam', 0.1, 8, 50)
train_loss: 0.38039708137512207, val_loss: 0.6274349093437195

('adam', 0.1, 8, 100)
train_loss: 0.3504418432712555, val_loss: 0.5487808585166931

('adam', 0.1, 16, 10)
train_loss: 0.4860634505748749, val_loss: 1.0054386854171753

('adam', 0.1, 16, 50)
train_loss: 0.39926403760910034, val_loss: 0.7126337885856628

('adam', 0.1, 16, 100)
train_loss: 0.3613828420639038, val_loss: 0.5646912455558777

('adam', 0.1, 32, 10)
train_loss: 0.520599365234375, val_loss: 1.0001732110977173

('adam', 0.1, 32, 50)
train_loss: 0.42458051443099976, val_loss: 0.69480961561203

('adam', 0.1, 32, 100)
train_loss: 0.3874170780181885, val_loss: 0.6555166244506836

('adam', 0.1, 64, 10)
train_loss: 0.5265453457832336, val_loss: 1.034753680229187

('adam', 0.1, 64, 50)
train_loss: 0.44342097640037537, val_loss: 0.8088391423225403

('adam', 0.1, 64, 100)
train_loss: 0.379300981760025, val_loss: 0.6519505381584167


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5487808585166931
---------

Run from 2023-04-05 16:40:11.672258
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7087462544441223, val_loss: 0.6744974255561829

('adam', 0.001, 8, 50)
train_loss: 0.58331698179245, val_loss: 0.8445600867271423

('adam', 0.001, 8, 100)
train_loss: 0.5664433240890503, val_loss: 0.7826288342475891

('adam', 0.001, 16, 10)
train_loss: 0.8293496966362, val_loss: 0.6550782322883606

('adam', 0.001, 16, 50)
train_loss: 0.6213061213493347, val_loss: 0.7410890460014343

('adam', 0.001, 16, 100)
train_loss: 0.5947220325469971, val_loss: 0.7505046725273132

('adam', 0.001, 32, 10)
train_loss: 0.6457949876785278, val_loss: 0.73936527967453

('adam', 0.001, 32, 50)
train_loss: 0.6779964566230774, val_loss: 0.6839022636413574

('adam', 0.001, 32, 100)
train_loss: 0.6530864834785461, val_loss: 0.6826273798942566

('adam', 0.001, 64, 10)
train_loss: 0.7309530377388, val_loss: 0.671374499797821

('adam', 0.001, 64, 50)
train_loss: 0.6930778622627258, val_loss: 0.6825739741325378

('adam', 0.001, 64, 100)
train_loss: 0.7854819297790527, val_loss: 0.6468151211738586

('adam', 0.01, 8, 10)
train_loss: 0.5833449363708496, val_loss: 0.7198954224586487

('adam', 0.01, 8, 50)
train_loss: 0.46986445784568787, val_loss: 0.811409056186676

('adam', 0.01, 8, 100)
train_loss: 0.47447600960731506, val_loss: 0.9011979103088379

('adam', 0.01, 16, 10)
train_loss: 0.5917697548866272, val_loss: 0.7714244723320007

('adam', 0.01, 16, 50)
train_loss: 0.5170657634735107, val_loss: 0.9694703221321106

('adam', 0.01, 16, 100)
train_loss: 0.4988502562046051, val_loss: 0.968070924282074

('adam', 0.01, 32, 10)
train_loss: 0.766632080078125, val_loss: 0.6361579298973083

('adam', 0.01, 32, 50)
train_loss: 0.5234258770942688, val_loss: 0.8473114967346191

('adam', 0.01, 32, 100)
train_loss: 0.5020807385444641, val_loss: 0.9510867595672607

('adam', 0.01, 64, 10)
train_loss: 0.6358649134635925, val_loss: 0.7320882678031921

('adam', 0.01, 64, 50)
train_loss: 0.5522306561470032, val_loss: 0.9451993107795715

('adam', 0.01, 64, 100)
train_loss: 0.5200136303901672, val_loss: 0.7430152893066406

('adam', 0.1, 8, 10)
train_loss: 0.4528276026248932, val_loss: 0.8721952438354492

('adam', 0.1, 8, 50)
train_loss: 0.37644538283348083, val_loss: 0.6137266755104065

('adam', 0.1, 8, 100)
train_loss: 0.3498792052268982, val_loss: 0.5459370613098145

('adam', 0.1, 16, 10)
train_loss: 0.468799889087677, val_loss: 0.9389159679412842

('adam', 0.1, 16, 50)
train_loss: 0.4044590890407562, val_loss: 0.7421949505805969

('adam', 0.1, 16, 100)
train_loss: 0.3644668757915497, val_loss: 0.6018745303153992

('adam', 0.1, 32, 10)
train_loss: 0.5120711326599121, val_loss: 1.0623925924301147

('adam', 0.1, 32, 50)
train_loss: 0.4167223870754242, val_loss: 0.7224817276000977

('adam', 0.1, 32, 100)
train_loss: 0.38394713401794434, val_loss: 0.6442827582359314

('adam', 0.1, 64, 10)
train_loss: 0.5102096199989319, val_loss: 0.8636224865913391

('adam', 0.1, 64, 50)
train_loss: 0.44734102487564087, val_loss: 0.7893202304840088

('adam', 0.1, 64, 100)
train_loss: 0.3804386556148529, val_loss: 0.6545633673667908


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5459370613098145
---------

Run from 2023-04-10 10:17:43.788927
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6142286062240601, val_loss: 0.8031648993492126

('adam', 0.001, 8, 50)
train_loss: 0.7061562538146973, val_loss: 0.6426213979721069

('adam', 0.001, 8, 100)
train_loss: 0.6138362884521484, val_loss: 0.6635521054267883

('adam', 0.001, 16, 10)
train_loss: 0.6164830923080444, val_loss: 0.8037099242210388

('adam', 0.001, 16, 50)
train_loss: 0.6728772521018982, val_loss: 0.676899254322052

('adam', 0.001, 16, 100)
train_loss: 0.6418560147285461, val_loss: 0.6720755696296692

('adam', 0.001, 32, 10)
train_loss: 0.8479322195053101, val_loss: 0.6571192741394043

('adam', 0.001, 32, 50)
train_loss: 0.6225277185440063, val_loss: 0.7555662989616394

('adam', 0.001, 32, 100)
train_loss: 0.6440593600273132, val_loss: 0.6926040649414062

('adam', 0.001, 64, 10)
train_loss: 0.630339503288269, val_loss: 0.7719662189483643

('adam', 0.001, 64, 50)
train_loss: 0.6846516728401184, val_loss: 0.6883845925331116

('adam', 0.001, 64, 100)
train_loss: 0.8150070309638977, val_loss: 0.6462821960449219

('adam', 0.01, 8, 10)
train_loss: 0.5648573637008667, val_loss: 0.793822705745697

('adam', 0.01, 8, 50)
train_loss: 0.509670078754425, val_loss: 0.9677534103393555

('adam', 0.01, 8, 100)
train_loss: 0.4622701406478882, val_loss: 0.8677502274513245

('adam', 0.01, 16, 10)
train_loss: 0.6168040037155151, val_loss: 0.7095234394073486

('adam', 0.01, 16, 50)
train_loss: 0.5071597099304199, val_loss: 0.8811820149421692

('adam', 0.01, 16, 100)
train_loss: 0.4785862863063812, val_loss: 0.8857342600822449

('adam', 0.01, 32, 10)
train_loss: 0.6204261779785156, val_loss: 0.7313376069068909

('adam', 0.01, 32, 50)
train_loss: 0.5366044044494629, val_loss: 0.9656850695610046

('adam', 0.01, 32, 100)
train_loss: 0.4988657832145691, val_loss: 0.9114656448364258

('adam', 0.01, 64, 10)
train_loss: 0.6095805168151855, val_loss: 0.7921395897865295

('adam', 0.01, 64, 50)
train_loss: 0.5506325960159302, val_loss: 0.8633669018745422

('adam', 0.01, 64, 100)
train_loss: 0.5225813984870911, val_loss: 0.9232195019721985

('adam', 0.1, 8, 10)
train_loss: 0.45449286699295044, val_loss: 0.8578446507453918

('adam', 0.1, 8, 50)
train_loss: 0.3737116754055023, val_loss: 0.6394656300544739

('adam', 0.1, 8, 100)
train_loss: 0.35093551874160767, val_loss: 0.5599383115768433

('adam', 0.1, 16, 10)
train_loss: 0.47620275616645813, val_loss: 0.9934947490692139

('adam', 0.1, 16, 50)
train_loss: 0.39433231949806213, val_loss: 0.7362144589424133

('adam', 0.1, 16, 100)
train_loss: 0.3614535629749298, val_loss: 0.5917416214942932

('adam', 0.1, 32, 10)
train_loss: 0.5060108304023743, val_loss: 1.0852928161621094

('adam', 0.1, 32, 50)
train_loss: 0.4177266061306, val_loss: 0.7768141627311707

('adam', 0.1, 32, 100)
train_loss: 0.3888150155544281, val_loss: 0.7365981936454773

('adam', 0.1, 64, 10)
train_loss: 0.5356273055076599, val_loss: 1.0764273405075073

('adam', 0.1, 64, 50)
train_loss: 0.44416674971580505, val_loss: 0.8066487908363342

('adam', 0.1, 64, 100)
train_loss: 0.3984207212924957, val_loss: 0.6972907185554504


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5599383115768433
---------

Run from 2023-04-10 13:11:54.151496
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6908136606216431, val_loss: 0.6899016499519348

('adam', 0.001, 8, 50)
train_loss: 0.6925280690193176, val_loss: 0.6780586242675781

('adam', 0.001, 8, 100)
train_loss: 0.6362869739532471, val_loss: 0.6724987030029297

('adam', 0.001, 16, 10)
train_loss: 0.8003394603729248, val_loss: 0.7038505673408508

('adam', 0.001, 16, 50)
train_loss: 0.6913830637931824, val_loss: 0.682617723941803

('adam', 0.001, 16, 100)
train_loss: 0.6209276914596558, val_loss: 0.6994772553443909

('adam', 0.001, 32, 10)
train_loss: 0.7004958391189575, val_loss: 0.6898217797279358

('adam', 0.001, 32, 50)
train_loss: 0.6373047232627869, val_loss: 0.7141593098640442

('adam', 0.001, 32, 100)
train_loss: 0.6476449370384216, val_loss: 0.6924638748168945

('adam', 0.001, 64, 10)
train_loss: 0.6484103202819824, val_loss: 0.7181460857391357

('adam', 0.001, 64, 50)
train_loss: 0.6547513008117676, val_loss: 0.7062780857086182

('adam', 0.001, 64, 100)
train_loss: 0.5831947326660156, val_loss: 0.8159452080726624

('adam', 0.01, 8, 10)
train_loss: 0.5458812713623047, val_loss: 0.8776158690452576

('adam', 0.01, 8, 50)
train_loss: 0.506242573261261, val_loss: 1.0328835248947144

('adam', 0.01, 8, 100)
train_loss: 0.48418405652046204, val_loss: 0.9485978484153748

('adam', 0.01, 16, 10)
train_loss: 0.5598611235618591, val_loss: 0.8594573140144348

('adam', 0.01, 16, 50)
train_loss: 0.49616947770118713, val_loss: 0.8560184836387634

('adam', 0.01, 16, 100)
train_loss: 0.47361382842063904, val_loss: 0.8672204613685608

('adam', 0.01, 32, 10)
train_loss: 0.6091935038566589, val_loss: 0.7344496846199036

('adam', 0.01, 32, 50)
train_loss: 0.5310472249984741, val_loss: 0.7355360984802246

('adam', 0.01, 32, 100)
train_loss: 0.49060484766960144, val_loss: 0.9382894039154053

('adam', 0.01, 64, 10)
train_loss: 0.6766999363899231, val_loss: 0.6901154518127441

('adam', 0.01, 64, 50)
train_loss: 0.5406493544578552, val_loss: 0.8348901867866516

('adam', 0.01, 64, 100)
train_loss: 0.5108817219734192, val_loss: 0.9474179148674011

('adam', 0.1, 8, 10)
train_loss: 0.4905354976654053, val_loss: 0.9682775139808655

('adam', 0.1, 8, 50)
train_loss: 0.4332631230354309, val_loss: 0.7105981707572937

('adam', 0.1, 8, 100)
train_loss: 0.4156898558139801, val_loss: 0.6547148823738098

('adam', 0.1, 16, 10)
train_loss: 0.4856870174407959, val_loss: 1.0593382120132446

('adam', 0.1, 16, 50)
train_loss: 0.4500991404056549, val_loss: 0.8268067240715027

('adam', 0.1, 16, 100)
train_loss: 0.4228658676147461, val_loss: 0.6872556805610657

('adam', 0.1, 32, 10)
train_loss: 0.5180303454399109, val_loss: 1.2473443746566772

('adam', 0.1, 32, 50)
train_loss: 0.45492082834243774, val_loss: 0.7986274361610413

('adam', 0.1, 32, 100)
train_loss: 0.4303724467754364, val_loss: 0.7336089015007019

('adam', 0.1, 64, 10)
train_loss: 0.5058225393295288, val_loss: 0.8724536895751953

('adam', 0.1, 64, 50)
train_loss: 0.46895357966423035, val_loss: 0.8447079658508301

('adam', 0.1, 64, 100)
train_loss: 0.43494120240211487, val_loss: 0.7339528203010559


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6547148823738098
---------

Run from 2023-04-10 14:37:50.503723
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.8222964406013489, val_loss: 0.6550513505935669

('adam', 0.001, 8, 50)
train_loss: 0.5694947242736816, val_loss: 0.8141433596611023

('adam', 0.001, 8, 100)
train_loss: 0.6386933326721191, val_loss: 0.6529776453971863

('adam', 0.001, 16, 10)
train_loss: 0.6513937711715698, val_loss: 0.7192637324333191

('adam', 0.001, 16, 50)
train_loss: 0.6041741371154785, val_loss: 0.7526881694793701

('adam', 0.001, 16, 100)
train_loss: 0.7378348708152771, val_loss: 0.6355395317077637

('adam', 0.001, 32, 10)
train_loss: 0.646657407283783, val_loss: 0.7260448336601257

('adam', 0.001, 32, 50)
train_loss: 0.8180427551269531, val_loss: 0.64951092004776

('adam', 0.001, 32, 100)
train_loss: 0.5594193935394287, val_loss: 0.8782419562339783

('adam', 0.001, 64, 10)
train_loss: 0.7900058627128601, val_loss: 0.6616529226303101

('adam', 0.001, 64, 50)
train_loss: 0.6438839435577393, val_loss: 0.7218096256256104

('adam', 0.001, 64, 100)
train_loss: 0.5710206627845764, val_loss: 0.852915346622467

('adam', 0.01, 8, 10)
train_loss: 0.5520873665809631, val_loss: 0.806904137134552

('adam', 0.01, 8, 50)
train_loss: 0.47655224800109863, val_loss: 0.8367436528205872

('adam', 0.01, 8, 100)
train_loss: 0.488740473985672, val_loss: 0.9541541934013367

('adam', 0.01, 16, 10)
train_loss: 0.5946760773658752, val_loss: 0.7416010499000549

('adam', 0.01, 16, 50)
train_loss: 0.49663662910461426, val_loss: 0.8013206124305725

('adam', 0.01, 16, 100)
train_loss: 0.49945640563964844, val_loss: 0.9927414059638977

('adam', 0.01, 32, 10)
train_loss: 0.6960729360580444, val_loss: 0.6623362898826599

('adam', 0.01, 32, 50)
train_loss: 0.5279231667518616, val_loss: 0.7475443482398987

('adam', 0.01, 32, 100)
train_loss: 0.4967344105243683, val_loss: 0.9495833516120911

('adam', 0.01, 64, 10)
train_loss: 0.655810534954071, val_loss: 0.7029538750648499

('adam', 0.01, 64, 50)
train_loss: 0.5318297743797302, val_loss: 0.899287223815918

('adam', 0.01, 64, 100)
train_loss: 0.5095028877258301, val_loss: 0.9008796811103821

('adam', 0.1, 8, 10)
train_loss: 0.4742129445075989, val_loss: 0.8919036984443665

('adam', 0.1, 8, 50)
train_loss: 0.426198810338974, val_loss: 0.7768168449401855

('adam', 0.1, 8, 100)
train_loss: 0.40079811215400696, val_loss: 0.6804711222648621

('adam', 0.1, 16, 10)
train_loss: 0.4957731068134308, val_loss: 1.0196092128753662

('adam', 0.1, 16, 50)
train_loss: 0.43831774592399597, val_loss: 0.7823655009269714

('adam', 0.1, 16, 100)
train_loss: 0.41215434670448303, val_loss: 0.7183607220649719

('adam', 0.1, 32, 10)
train_loss: 0.5056836009025574, val_loss: 1.0921536684036255

('adam', 0.1, 32, 50)
train_loss: 0.45378363132476807, val_loss: 0.8874896168708801

('adam', 0.1, 32, 100)
train_loss: 0.437750905752182, val_loss: 0.7828795313835144

('adam', 0.1, 64, 10)
train_loss: 0.5099189877510071, val_loss: 1.034156322479248

('adam', 0.1, 64, 50)
train_loss: 0.4657762050628662, val_loss: 0.8546061515808105

('adam', 0.1, 64, 100)
train_loss: 0.42678093910217285, val_loss: 0.764803409576416


---------
BEST MODEL
('adam', 0.001, 16, 100)
val_loss: 0.6355395317077637
---------

Run from 2023-04-17 10:01:03.833242
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7477529048919678, val_loss: 0.665234386920929

('adam', 0.001, 8, 50)
train_loss: 0.6085411906242371, val_loss: 0.7528365254402161

('adam', 0.001, 8, 100)
train_loss: 0.6050186157226562, val_loss: 0.6849341988563538

('adam', 0.001, 16, 10)
train_loss: 0.7192289233207703, val_loss: 0.676192581653595

('adam', 0.001, 16, 50)
train_loss: 0.6138296723365784, val_loss: 0.7701155543327332

('adam', 0.001, 16, 100)
train_loss: 0.6035038828849792, val_loss: 0.7495418787002563

('adam', 0.001, 32, 10)
train_loss: 0.6192618608474731, val_loss: 0.8030500411987305

('adam', 0.001, 32, 50)
train_loss: 0.6541092395782471, val_loss: 0.7083380818367004

('adam', 0.001, 32, 100)
train_loss: 0.6411707401275635, val_loss: 0.702095091342926

('adam', 0.001, 64, 10)
train_loss: 0.7530394792556763, val_loss: 0.6697787642478943

('adam', 0.001, 64, 50)
train_loss: 0.6227703094482422, val_loss: 0.7761884927749634

('adam', 0.001, 64, 100)
train_loss: 0.6038237810134888, val_loss: 0.8350619077682495

('adam', 0.01, 8, 10)
train_loss: 0.590983510017395, val_loss: 0.7162749767303467

('adam', 0.01, 8, 50)
train_loss: 0.5514106750488281, val_loss: 0.9473013877868652

('adam', 0.01, 8, 100)
train_loss: 0.49260175228118896, val_loss: 0.8015722036361694

('adam', 0.01, 16, 10)
train_loss: 0.5892115235328674, val_loss: 0.824877142906189

('adam', 0.01, 16, 50)
train_loss: 0.5277152061462402, val_loss: 0.7841501235961914

('adam', 0.01, 16, 100)
train_loss: 0.5069536566734314, val_loss: 0.8156386613845825

('adam', 0.01, 32, 10)
train_loss: 0.6263540983200073, val_loss: 0.7314785718917847

('adam', 0.01, 32, 50)
train_loss: 0.5600771307945251, val_loss: 0.9111262559890747

('adam', 0.01, 32, 100)
train_loss: 0.5136925578117371, val_loss: 0.779884934425354

('adam', 0.01, 64, 10)
train_loss: 0.6165289878845215, val_loss: 0.7799087762832642

('adam', 0.01, 64, 50)
train_loss: 0.5784992575645447, val_loss: 0.7561419606208801

('adam', 0.01, 64, 100)
train_loss: 0.5533028244972229, val_loss: 0.90790194272995

('adam', 0.1, 8, 10)
train_loss: 0.5243701338768005, val_loss: 0.8308756947517395

('adam', 0.1, 8, 50)
train_loss: 0.4366018772125244, val_loss: 0.6648077964782715

('adam', 0.1, 8, 100)
train_loss: 0.41543683409690857, val_loss: 0.6256391406059265

('adam', 0.1, 16, 10)
train_loss: 0.5189179182052612, val_loss: 0.9323719143867493

('adam', 0.1, 16, 50)
train_loss: 0.4563637673854828, val_loss: 0.7137330174446106

('adam', 0.1, 16, 100)
train_loss: 0.42169076204299927, val_loss: 0.6430603861808777

('adam', 0.1, 32, 10)
train_loss: 0.5348281860351562, val_loss: 0.971163272857666

('adam', 0.1, 32, 50)
train_loss: 0.4736863076686859, val_loss: 0.7310178875923157

('adam', 0.1, 32, 100)
train_loss: 0.4348980486392975, val_loss: 0.6851667165756226

('adam', 0.1, 64, 10)
train_loss: 0.542682945728302, val_loss: 0.7499538064002991

('adam', 0.1, 64, 50)
train_loss: 0.49187713861465454, val_loss: 0.801797091960907

('adam', 0.1, 64, 100)
train_loss: 0.4355875551700592, val_loss: 0.6706584692001343

('sgd', 0.001, 8, 10)
train_loss: 0.6131730079650879, val_loss: 0.8400804400444031

('sgd', 0.001, 8, 50)
train_loss: 0.6732925176620483, val_loss: 0.6920865178108215

('sgd', 0.001, 8, 100)
train_loss: 0.6765203475952148, val_loss: 0.6717216372489929

('sgd', 0.001, 16, 10)
train_loss: 0.6670026779174805, val_loss: 0.7129331827163696

('sgd', 0.001, 16, 50)
train_loss: 0.7671490907669067, val_loss: 0.6626117825508118

('sgd', 0.001, 16, 100)
train_loss: 0.6710793972015381, val_loss: 0.6933944821357727

('sgd', 0.001, 32, 10)
train_loss: 0.7384790778160095, val_loss: 0.6731709241867065

('sgd', 0.001, 32, 50)
train_loss: 0.6422526240348816, val_loss: 0.7422121167182922

('sgd', 0.001, 32, 100)
train_loss: 0.6467911601066589, val_loss: 0.7281972169876099

('sgd', 0.001, 64, 10)
train_loss: 0.6586775779724121, val_loss: 0.7233633995056152

('sgd', 0.001, 64, 50)
train_loss: 0.7622377276420593, val_loss: 0.6674311757087708

('sgd', 0.001, 64, 100)
train_loss: 0.7310471534729004, val_loss: 0.6715151071548462

('sgd', 0.01, 8, 10)
train_loss: 0.7713587880134583, val_loss: 0.6451221704483032

('sgd', 0.01, 8, 50)
train_loss: 0.5735183954238892, val_loss: 0.6442536115646362

('sgd', 0.01, 8, 100)
train_loss: 0.521807849407196, val_loss: 0.7470012903213501

('sgd', 0.01, 16, 10)
train_loss: 0.8008298277854919, val_loss: 0.6553317308425903

('sgd', 0.01, 16, 50)
train_loss: 0.5920392274856567, val_loss: 0.8499466180801392

('sgd', 0.01, 16, 100)
train_loss: 0.56613689661026, val_loss: 0.6824251413345337

('sgd', 0.01, 32, 10)
train_loss: 0.628288984298706, val_loss: 0.7668823599815369

('sgd', 0.01, 32, 50)
train_loss: 0.6103234887123108, val_loss: 0.7545632123947144

('sgd', 0.01, 32, 100)
train_loss: 0.586039125919342, val_loss: 0.8666407465934753

('sgd', 0.01, 64, 10)
train_loss: 0.7132116556167603, val_loss: 0.6781591176986694

('sgd', 0.01, 64, 50)
train_loss: 0.6060853004455566, val_loss: 0.8412097096443176

('sgd', 0.01, 64, 100)
train_loss: 0.6713554859161377, val_loss: 0.6523512601852417

('sgd', 0.1, 8, 10)
train_loss: 0.5377022624015808, val_loss: 0.8107908368110657

('sgd', 0.1, 8, 50)
train_loss: 0.4870727062225342, val_loss: 0.7909123301506042

('sgd', 0.1, 8, 100)
train_loss: 0.4790387749671936, val_loss: 0.775183916091919

('sgd', 0.1, 16, 10)
train_loss: 0.5725057721138, val_loss: 0.6660453677177429

('sgd', 0.1, 16, 50)
train_loss: 0.5582383275032043, val_loss: 0.9501951336860657

('sgd', 0.1, 16, 100)
train_loss: 0.5338207483291626, val_loss: 0.9132789373397827

('sgd', 0.1, 32, 10)
train_loss: 0.5994742512702942, val_loss: 0.706383228302002

('sgd', 0.1, 32, 50)
train_loss: 0.5178024768829346, val_loss: 0.8217359781265259

('sgd', 0.1, 32, 100)
train_loss: 0.5536546111106873, val_loss: 0.9490560293197632

('sgd', 0.1, 64, 10)
train_loss: 0.6247822046279907, val_loss: 0.7177966237068176

('sgd', 0.1, 64, 50)
train_loss: 0.564285397529602, val_loss: 0.8916652798652649

('sgd', 0.1, 64, 100)
train_loss: 0.5510895848274231, val_loss: 0.9202394485473633


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6256391406059265
---------

Run from 2023-04-17 16:49:22.386707
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6533364057540894, val_loss: 0.7280449867248535

('adam', 0.001, 8, 50)
train_loss: 0.6769297122955322, val_loss: 0.6584593653678894

('adam', 0.001, 8, 100)
train_loss: 0.6023479104042053, val_loss: 0.8612598776817322

('adam', 0.001, 16, 10)
train_loss: 0.8013753294944763, val_loss: 0.6670899987220764

('adam', 0.001, 16, 50)
train_loss: 0.6225510239601135, val_loss: 0.8247912526130676

('adam', 0.001, 16, 100)
train_loss: 0.6166933178901672, val_loss: 0.7393786907196045

('adam', 0.001, 32, 10)
train_loss: 0.7523899674415588, val_loss: 0.6687378287315369

('adam', 0.001, 32, 50)
train_loss: 0.76859450340271, val_loss: 0.6593688726425171

('adam', 0.001, 32, 100)
train_loss: 0.6176764965057373, val_loss: 0.8256010413169861

('adam', 0.001, 64, 10)
train_loss: 0.744717001914978, val_loss: 0.6710085868835449

('adam', 0.001, 64, 50)
train_loss: 0.6419410705566406, val_loss: 0.7551147937774658

('adam', 0.001, 64, 100)
train_loss: 0.6305107474327087, val_loss: 0.7730000615119934

('adam', 0.01, 8, 10)
train_loss: 0.5931839346885681, val_loss: 0.7975404858589172

('adam', 0.01, 8, 50)
train_loss: 0.5597015023231506, val_loss: 0.875931441783905

('adam', 0.01, 8, 100)
train_loss: 0.5007343888282776, val_loss: 0.7867331504821777

('adam', 0.01, 16, 10)
train_loss: 0.6080887317657471, val_loss: 0.7967755198478699

('adam', 0.01, 16, 50)
train_loss: 0.5510959029197693, val_loss: 0.8320684432983398

('adam', 0.01, 16, 100)
train_loss: 0.5271622538566589, val_loss: 0.8160712718963623

('adam', 0.01, 32, 10)
train_loss: 0.6923466324806213, val_loss: 0.6640530824661255

('adam', 0.01, 32, 50)
train_loss: 0.5549620985984802, val_loss: 0.7474851012229919

('adam', 0.01, 32, 100)
train_loss: 0.5537071824073792, val_loss: 0.8610393404960632

('adam', 0.01, 64, 10)
train_loss: 0.6367197632789612, val_loss: 0.7532632946968079

('adam', 0.01, 64, 50)
train_loss: 0.5877655744552612, val_loss: 0.8149452805519104

('adam', 0.01, 64, 100)
train_loss: 0.5544632077217102, val_loss: 0.7638816237449646

('adam', 0.1, 8, 10)
train_loss: 0.5346477031707764, val_loss: 0.791450560092926

('adam', 0.1, 8, 50)
train_loss: 0.4372284710407257, val_loss: 0.6309468150138855

('adam', 0.1, 8, 100)
train_loss: 0.4146275520324707, val_loss: 0.5953908562660217

('adam', 0.1, 16, 10)
train_loss: 0.5242342352867126, val_loss: 0.8953647613525391

('adam', 0.1, 16, 50)
train_loss: 0.4427674114704132, val_loss: 0.6784941554069519

('adam', 0.1, 16, 100)
train_loss: 0.41987740993499756, val_loss: 0.6314162611961365

('adam', 0.1, 32, 10)
train_loss: 0.563859760761261, val_loss: 0.9245807528495789

('adam', 0.1, 32, 50)
train_loss: 0.4556942880153656, val_loss: 0.7104935646057129

('adam', 0.1, 32, 100)
train_loss: 0.42731258273124695, val_loss: 0.637450635433197

('adam', 0.1, 64, 10)
train_loss: 0.5487448573112488, val_loss: 0.800556480884552

('adam', 0.1, 64, 50)
train_loss: 0.48531264066696167, val_loss: 0.762224018573761

('adam', 0.1, 64, 100)
train_loss: 0.43640467524528503, val_loss: 0.669267475605011

('sgd', 0.001, 8, 10)
train_loss: 0.7633894085884094, val_loss: 0.6674492955207825

('sgd', 0.001, 8, 50)
train_loss: 0.6990012526512146, val_loss: 0.6738638877868652

('sgd', 0.001, 8, 100)
train_loss: 0.7018398642539978, val_loss: 0.6570303440093994

('sgd', 0.001, 16, 10)
train_loss: 0.7728555202484131, val_loss: 0.6681216359138489

('sgd', 0.001, 16, 50)
train_loss: 0.6347137689590454, val_loss: 0.8201875686645508

('sgd', 0.001, 16, 100)
train_loss: 0.7027583718299866, val_loss: 0.6718048453330994

('sgd', 0.001, 32, 10)
train_loss: 0.7625158429145813, val_loss: 0.6691460013389587

('sgd', 0.001, 32, 50)
train_loss: 0.6596065759658813, val_loss: 0.7246430516242981

('sgd', 0.001, 32, 100)
train_loss: 0.6339802145957947, val_loss: 0.8181846737861633

('sgd', 0.001, 64, 10)
train_loss: 0.643103837966919, val_loss: 0.7773480415344238

('sgd', 0.001, 64, 50)
train_loss: 0.6612473726272583, val_loss: 0.7255471348762512

('sgd', 0.001, 64, 100)
train_loss: 0.660743772983551, val_loss: 0.722820520401001

('sgd', 0.01, 8, 10)
train_loss: 0.6274939179420471, val_loss: 0.812283992767334

('sgd', 0.01, 8, 50)
train_loss: 0.5968577861785889, val_loss: 0.8335215449333191

('sgd', 0.01, 8, 100)
train_loss: 0.5289120078086853, val_loss: 0.7304331660270691

('sgd', 0.01, 16, 10)
train_loss: 0.7661571502685547, val_loss: 0.6579363942146301

('sgd', 0.01, 16, 50)
train_loss: 0.6130704283714294, val_loss: 0.7155148386955261

('sgd', 0.01, 16, 100)
train_loss: 0.5822658538818359, val_loss: 0.7712004780769348

('sgd', 0.01, 32, 10)
train_loss: 0.6654132604598999, val_loss: 0.7100147604942322

('sgd', 0.01, 32, 50)
train_loss: 0.6201545000076294, val_loss: 0.777492344379425

('sgd', 0.01, 32, 100)
train_loss: 0.6096115112304688, val_loss: 0.6577454209327698

('sgd', 0.01, 64, 10)
train_loss: 0.7214584350585938, val_loss: 0.6741213798522949

('sgd', 0.01, 64, 50)
train_loss: 0.7104421854019165, val_loss: 0.6589887738227844

('sgd', 0.01, 64, 100)
train_loss: 0.6213913559913635, val_loss: 0.7591838836669922

('sgd', 0.1, 8, 10)
train_loss: 0.5601208209991455, val_loss: 0.8314487338066101

('sgd', 0.1, 8, 50)
train_loss: 0.501639723777771, val_loss: 0.8021271824836731

('sgd', 0.1, 8, 100)
train_loss: 0.5046769976615906, val_loss: 0.7987077236175537

('sgd', 0.1, 16, 10)
train_loss: 0.5785970687866211, val_loss: 0.7305377125740051

('sgd', 0.1, 16, 50)
train_loss: 0.5381187796592712, val_loss: 0.8475358486175537

('sgd', 0.1, 16, 100)
train_loss: 0.5207645297050476, val_loss: 0.8265393376350403

('sgd', 0.1, 32, 10)
train_loss: 0.6050236225128174, val_loss: 0.6865684390068054

('sgd', 0.1, 32, 50)
train_loss: 0.5851262211799622, val_loss: 0.9039809703826904

('sgd', 0.1, 32, 100)
train_loss: 0.49762582778930664, val_loss: 0.7830731868743896

('sgd', 0.1, 64, 10)
train_loss: 0.6965894103050232, val_loss: 0.6422117352485657

('sgd', 0.1, 64, 50)
train_loss: 0.5842586159706116, val_loss: 0.8621693253517151

('sgd', 0.1, 64, 100)
train_loss: 0.539484977722168, val_loss: 0.8258040547370911


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5953908562660217
---------
