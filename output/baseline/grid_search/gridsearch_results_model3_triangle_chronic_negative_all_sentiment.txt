
Run from 2023-04-02 10:51:38.427140
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6209757924079895, val_loss: 0.7115154266357422

('adam', 0.01, 50, 200)
train_loss: 0.6183210015296936, val_loss: 0.7487139105796814

('adam', 0.01, 50, 300)
train_loss: 0.615737795829773, val_loss: 0.7462196350097656

('adam', 0.01, 100, 100)
train_loss: 0.6222614645957947, val_loss: 0.7063701748847961

('adam', 0.01, 100, 200)
train_loss: 0.6162216663360596, val_loss: 0.7414826154708862

('adam', 0.01, 100, 300)
train_loss: 0.615737795829773, val_loss: 0.7462069392204285

('adam', 0.01, 200, 100)
train_loss: 0.625748872756958, val_loss: 0.7512741088867188

('adam', 0.01, 200, 200)
train_loss: 0.6173959374427795, val_loss: 0.7467092275619507

('adam', 0.01, 200, 300)
train_loss: 0.6157382130622864, val_loss: 0.7461211085319519

('adam', 0.1, 50, 100)
train_loss: 0.6157386302947998, val_loss: 0.7460037469863892

('adam', 0.1, 50, 200)
train_loss: 0.6157374382019043, val_loss: 0.7463787794113159

('adam', 0.1, 50, 300)
train_loss: 0.6157374382019043, val_loss: 0.7463771104812622

('adam', 0.1, 100, 100)
train_loss: 0.6157383918762207, val_loss: 0.7464532256126404

('adam', 0.1, 100, 200)
train_loss: 0.6157374382019043, val_loss: 0.7463794350624084

('adam', 0.1, 100, 300)
train_loss: 0.6157375574111938, val_loss: 0.746377170085907

('adam', 0.1, 200, 100)
train_loss: 0.6157397031784058, val_loss: 0.7472394108772278

('adam', 0.1, 200, 200)
train_loss: 0.6157375574111938, val_loss: 0.7463781237602234

('adam', 0.1, 200, 300)
train_loss: 0.6157374978065491, val_loss: 0.7463771104812622

('adam', 0.2, 50, 100)
train_loss: 0.6157376766204834, val_loss: 0.7466418147087097

('adam', 0.2, 50, 200)
train_loss: 0.6157375574111938, val_loss: 0.7463753819465637

('adam', 0.2, 50, 300)
train_loss: 0.6157375574111938, val_loss: 0.7463771104812622

('adam', 0.2, 100, 100)
train_loss: 0.6157376170158386, val_loss: 0.7468143105506897

('adam', 0.2, 100, 200)
train_loss: 0.6157374978065491, val_loss: 0.7463751435279846

('adam', 0.2, 100, 300)
train_loss: 0.6157374978065491, val_loss: 0.7463771104812622

('adam', 0.2, 200, 100)
train_loss: 0.6157379150390625, val_loss: 0.7465676069259644

('adam', 0.2, 200, 200)
train_loss: 0.6157374978065491, val_loss: 0.7463788390159607

('adam', 0.2, 200, 300)
train_loss: 0.6157374978065491, val_loss: 0.7463771104812622


---------
BEST MODEL
('adam', 0.01, 100, 100)
val_loss: 0.7063701748847961
---------

Run from 2023-04-05 15:13:29.431298
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6836222410202026, val_loss: 0.7253630757331848

('adam', 0.001, 8, 50)
train_loss: 0.698157787322998, val_loss: 0.8244313597679138

('adam', 0.001, 8, 100)
train_loss: 0.6564117670059204, val_loss: 0.7385298013687134

('adam', 0.001, 16, 10)
train_loss: 0.6928192377090454, val_loss: 0.6858304142951965

('adam', 0.001, 16, 50)
train_loss: 0.675937831401825, val_loss: 0.7559418082237244

('adam', 0.001, 16, 100)
train_loss: 0.7087305784225464, val_loss: 0.6590105891227722

('adam', 0.001, 32, 10)
train_loss: 0.7144201397895813, val_loss: 0.6703673005104065

('adam', 0.001, 32, 50)
train_loss: 0.7194055914878845, val_loss: 0.6628569960594177

('adam', 0.001, 32, 100)
train_loss: 0.6960268616676331, val_loss: 0.6635659337043762

('adam', 0.001, 64, 10)
train_loss: 0.6912148594856262, val_loss: 0.7698345184326172

('adam', 0.001, 64, 50)
train_loss: 0.7240285873413086, val_loss: 0.6648927330970764

('adam', 0.001, 64, 100)
train_loss: 0.6732350587844849, val_loss: 0.7231383919715881

('adam', 0.01, 8, 10)
train_loss: 0.6795563101768494, val_loss: 0.6626093983650208

('adam', 0.01, 8, 50)
train_loss: 0.6197506785392761, val_loss: 0.7262367606163025

('adam', 0.01, 8, 100)
train_loss: 0.6277486681938171, val_loss: 0.774625837802887

('adam', 0.01, 16, 10)
train_loss: 0.6597706079483032, val_loss: 0.7099059820175171

('adam', 0.01, 16, 50)
train_loss: 0.6263582110404968, val_loss: 0.7514827847480774

('adam', 0.01, 16, 100)
train_loss: 0.6230520606040955, val_loss: 0.7532728314399719

('adam', 0.01, 32, 10)
train_loss: 0.6969406604766846, val_loss: 0.8323606848716736

('adam', 0.01, 32, 50)
train_loss: 0.6332807540893555, val_loss: 0.7518492937088013

('adam', 0.01, 32, 100)
train_loss: 0.6188692450523376, val_loss: 0.7521225810050964

('adam', 0.01, 64, 10)
train_loss: 0.693573534488678, val_loss: 0.6726396679878235

('adam', 0.01, 64, 50)
train_loss: 0.6375653147697449, val_loss: 0.7215592265129089

('adam', 0.01, 64, 100)
train_loss: 0.6222964525222778, val_loss: 0.7324044108390808

('adam', 0.1, 8, 10)
train_loss: 0.6238529086112976, val_loss: 0.7460174560546875

('adam', 0.1, 8, 50)
train_loss: 0.619249165058136, val_loss: 0.7698894143104553

('adam', 0.1, 8, 100)
train_loss: 0.6216537952423096, val_loss: 0.758595883846283

('adam', 0.1, 16, 10)
train_loss: 0.6208206415176392, val_loss: 0.7336961030960083

('adam', 0.1, 16, 50)
train_loss: 0.6194370985031128, val_loss: 0.728827953338623

('adam', 0.1, 16, 100)
train_loss: 0.6182863116264343, val_loss: 0.745895504951477

('adam', 0.1, 32, 10)
train_loss: 0.6346473097801208, val_loss: 0.7966750860214233

('adam', 0.1, 32, 50)
train_loss: 0.615734338760376, val_loss: 0.7622484564781189

('adam', 0.1, 32, 100)
train_loss: 0.6183097958564758, val_loss: 0.7182838320732117

('adam', 0.1, 64, 10)
train_loss: 0.6217257380485535, val_loss: 0.7767989039421082

('adam', 0.1, 64, 50)
train_loss: 0.6161389946937561, val_loss: 0.7566434741020203

('adam', 0.1, 64, 100)
train_loss: 0.615738034248352, val_loss: 0.7461785674095154


---------
BEST MODEL
('adam', 0.001, 16, 100)
val_loss: 0.6590105891227722
---------

Run from 2023-04-05 16:13:49.397856
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7626917362213135, val_loss: 0.6646819710731506

('adam', 0.001, 8, 50)
train_loss: 0.6862847805023193, val_loss: 0.7922118902206421

('adam', 0.001, 8, 100)
train_loss: 0.6557198166847229, val_loss: 0.6764046549797058

('adam', 0.001, 16, 10)
train_loss: 0.7766284346580505, val_loss: 0.6668533086776733

('adam', 0.001, 16, 50)
train_loss: 0.6773484349250793, val_loss: 0.683650016784668

('adam', 0.001, 16, 100)
train_loss: 0.6718206405639648, val_loss: 0.773832380771637

('adam', 0.001, 32, 10)
train_loss: 0.7012506723403931, val_loss: 0.6784582138061523

('adam', 0.001, 32, 50)
train_loss: 0.6796405911445618, val_loss: 0.7540740966796875

('adam', 0.001, 32, 100)
train_loss: 0.6906916499137878, val_loss: 0.6649684309959412

('adam', 0.001, 64, 10)
train_loss: 0.7313809990882874, val_loss: 0.6665999293327332

('adam', 0.001, 64, 50)
train_loss: 0.6919195652008057, val_loss: 0.7914147973060608

('adam', 0.001, 64, 100)
train_loss: 0.6858196258544922, val_loss: 0.6781440377235413

('adam', 0.01, 8, 10)
train_loss: 0.6579321622848511, val_loss: 0.7416181564331055

('adam', 0.01, 8, 50)
train_loss: 0.6252633333206177, val_loss: 0.752173900604248

('adam', 0.01, 8, 100)
train_loss: 0.616096556186676, val_loss: 0.7420802712440491

('adam', 0.01, 16, 10)
train_loss: 0.6786147952079773, val_loss: 0.8095885515213013

('adam', 0.01, 16, 50)
train_loss: 0.6219556927680969, val_loss: 0.7452031373977661

('adam', 0.01, 16, 100)
train_loss: 0.6194415092468262, val_loss: 0.7473800778388977

('adam', 0.01, 32, 10)
train_loss: 0.6669721603393555, val_loss: 0.7078297734260559

('adam', 0.01, 32, 50)
train_loss: 0.62913978099823, val_loss: 0.7532727122306824

('adam', 0.01, 32, 100)
train_loss: 0.6209546327590942, val_loss: 0.7454206347465515

('adam', 0.01, 64, 10)
train_loss: 0.6853430867195129, val_loss: 0.6805320978164673

('adam', 0.01, 64, 50)
train_loss: 0.6532877683639526, val_loss: 0.6715647578239441

('adam', 0.01, 64, 100)
train_loss: 0.6208270788192749, val_loss: 0.7305429577827454

('adam', 0.1, 8, 10)
train_loss: 0.6322303414344788, val_loss: 0.800442099571228

('adam', 0.1, 8, 50)
train_loss: 0.6249319314956665, val_loss: 0.7982197999954224

('adam', 0.1, 8, 100)
train_loss: 0.6184710264205933, val_loss: 0.7923051118850708

('adam', 0.1, 16, 10)
train_loss: 0.618318498134613, val_loss: 0.7535837292671204

('adam', 0.1, 16, 50)
train_loss: 0.6165300011634827, val_loss: 0.7368006110191345

('adam', 0.1, 16, 100)
train_loss: 0.6180153489112854, val_loss: 0.7457658052444458

('adam', 0.1, 32, 10)
train_loss: 0.6292644143104553, val_loss: 0.813568651676178

('adam', 0.1, 32, 50)
train_loss: 0.6169406175613403, val_loss: 0.7309021949768066

('adam', 0.1, 32, 100)
train_loss: 0.6162077784538269, val_loss: 0.7379304766654968

('adam', 0.1, 64, 10)
train_loss: 0.6387732625007629, val_loss: 0.8043724894523621

('adam', 0.1, 64, 50)
train_loss: 0.6161340475082397, val_loss: 0.7479267716407776

('adam', 0.1, 64, 100)
train_loss: 0.6157407760620117, val_loss: 0.7454752326011658


---------
BEST MODEL
('adam', 0.001, 8, 10)
val_loss: 0.6646819710731506
---------

Run from 2023-04-10 09:34:54.357456
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6825225949287415, val_loss: 0.7380899786949158

('adam', 0.001, 8, 50)
train_loss: 0.6669929027557373, val_loss: 0.7053715586662292

('adam', 0.001, 8, 100)
train_loss: 0.6888840198516846, val_loss: 0.8253495097160339

('adam', 0.001, 16, 10)
train_loss: 0.7383682727813721, val_loss: 0.6647142171859741

('adam', 0.001, 16, 50)
train_loss: 0.6719995141029358, val_loss: 0.6988146901130676

('adam', 0.001, 16, 100)
train_loss: 0.6579329967498779, val_loss: 0.7139951586723328

('adam', 0.001, 32, 10)
train_loss: 0.730452835559845, val_loss: 0.6661669015884399

('adam', 0.001, 32, 50)
train_loss: 0.6805245876312256, val_loss: 0.7554221749305725

('adam', 0.001, 32, 100)
train_loss: 0.667670488357544, val_loss: 0.7404924035072327

('adam', 0.001, 64, 10)
train_loss: 0.7090339660644531, val_loss: 0.8296818137168884

('adam', 0.001, 64, 50)
train_loss: 0.7012196779251099, val_loss: 0.8202769160270691

('adam', 0.001, 64, 100)
train_loss: 0.7135125398635864, val_loss: 0.6636096239089966

('adam', 0.01, 8, 10)
train_loss: 0.691903829574585, val_loss: 0.8328873515129089

('adam', 0.01, 8, 50)
train_loss: 0.6501465439796448, val_loss: 0.7719231247901917

('adam', 0.01, 8, 100)
train_loss: 0.6157457828521729, val_loss: 0.7412053346633911

('adam', 0.01, 16, 10)
train_loss: 0.6596091985702515, val_loss: 0.6981143951416016

('adam', 0.01, 16, 50)
train_loss: 0.6305508613586426, val_loss: 0.7607840299606323

('adam', 0.01, 16, 100)
train_loss: 0.6185597777366638, val_loss: 0.7479801177978516

('adam', 0.01, 32, 10)
train_loss: 0.6633250713348389, val_loss: 0.7187055349349976

('adam', 0.01, 32, 50)
train_loss: 0.628194272518158, val_loss: 0.743321418762207

('adam', 0.01, 32, 100)
train_loss: 0.6191018223762512, val_loss: 0.7595828771591187

('adam', 0.01, 64, 10)
train_loss: 0.7131875157356262, val_loss: 0.6641015410423279

('adam', 0.01, 64, 50)
train_loss: 0.6373851299285889, val_loss: 0.7287248373031616

('adam', 0.01, 64, 100)
train_loss: 0.6213440895080566, val_loss: 0.7364931702613831

('adam', 0.1, 8, 10)
train_loss: 0.6278721690177917, val_loss: 0.7704657912254333

('adam', 0.1, 8, 50)
train_loss: 0.618486762046814, val_loss: 0.784939169883728

('adam', 0.1, 8, 100)
train_loss: 0.6204038262367249, val_loss: 0.7249522805213928

('adam', 0.1, 16, 10)
train_loss: 0.6157312393188477, val_loss: 0.7404772639274597

('adam', 0.1, 16, 50)
train_loss: 0.615705668926239, val_loss: 0.7365140318870544

('adam', 0.1, 16, 100)
train_loss: 0.6181212663650513, val_loss: 0.7629304528236389

('adam', 0.1, 32, 10)
train_loss: 0.6335204243659973, val_loss: 0.7633546590805054

('adam', 0.1, 32, 50)
train_loss: 0.6158648133277893, val_loss: 0.7419704794883728

('adam', 0.1, 32, 100)
train_loss: 0.6166317462921143, val_loss: 0.7564330697059631

('adam', 0.1, 64, 10)
train_loss: 0.6201229095458984, val_loss: 0.759019672870636

('adam', 0.1, 64, 50)
train_loss: 0.6161028742790222, val_loss: 0.7557793259620667

('adam', 0.1, 64, 100)
train_loss: 0.6157388687133789, val_loss: 0.7459412813186646


---------
BEST MODEL
('adam', 0.001, 64, 100)
val_loss: 0.6636096239089966
---------

Run from 2023-04-10 12:39:55.544725
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6847768425941467, val_loss: 0.6983853578567505

('adam', 0.001, 8, 50)
train_loss: 0.6742588877677917, val_loss: 0.687129020690918

('adam', 0.001, 8, 100)
train_loss: 0.6682044267654419, val_loss: 0.7303062081336975

('adam', 0.001, 16, 10)
train_loss: 0.6869704723358154, val_loss: 0.7015774846076965

('adam', 0.001, 16, 50)
train_loss: 0.6891381740570068, val_loss: 0.7289206385612488

('adam', 0.001, 16, 100)
train_loss: 0.6697157621383667, val_loss: 0.7219284176826477

('adam', 0.001, 32, 10)
train_loss: 0.7002678513526917, val_loss: 0.6874487996101379

('adam', 0.001, 32, 50)
train_loss: 0.681445837020874, val_loss: 0.7107636332511902

('adam', 0.001, 32, 100)
train_loss: 0.6820458769798279, val_loss: 0.727047860622406

('adam', 0.001, 64, 10)
train_loss: 0.7115688920021057, val_loss: 0.6860804557800293

('adam', 0.001, 64, 50)
train_loss: 0.728665292263031, val_loss: 0.685703456401825

('adam', 0.001, 64, 100)
train_loss: 0.7216635942459106, val_loss: 0.68498694896698

('adam', 0.01, 8, 10)
train_loss: 0.670880138874054, val_loss: 0.6893938779830933

('adam', 0.01, 8, 50)
train_loss: 0.6190734505653381, val_loss: 0.7376591563224792

('adam', 0.01, 8, 100)
train_loss: 0.6203370690345764, val_loss: 0.7447608709335327

('adam', 0.01, 16, 10)
train_loss: 0.6761845350265503, val_loss: 0.6866073608398438

('adam', 0.01, 16, 50)
train_loss: 0.6144065260887146, val_loss: 0.7482392191886902

('adam', 0.01, 16, 100)
train_loss: 0.6122193932533264, val_loss: 0.7626928687095642

('adam', 0.01, 32, 10)
train_loss: 0.6678293347358704, val_loss: 0.6955447793006897

('adam', 0.01, 32, 50)
train_loss: 0.6309469938278198, val_loss: 0.7314539551734924

('adam', 0.01, 32, 100)
train_loss: 0.6196479797363281, val_loss: 0.7483171820640564

('adam', 0.01, 64, 10)
train_loss: 0.6767604947090149, val_loss: 0.7019158005714417

('adam', 0.01, 64, 50)
train_loss: 0.6356793642044067, val_loss: 0.708116888999939

('adam', 0.01, 64, 100)
train_loss: 0.6191752552986145, val_loss: 0.7394161820411682

('adam', 0.1, 8, 10)
train_loss: 0.6224667429924011, val_loss: 0.8060051798820496

('adam', 0.1, 8, 50)
train_loss: 0.6139707565307617, val_loss: 0.796477735042572

('adam', 0.1, 8, 100)
train_loss: 0.6151790618896484, val_loss: 0.7732533812522888

('adam', 0.1, 16, 10)
train_loss: 0.6132808923721313, val_loss: 0.7863655090332031

('adam', 0.1, 16, 50)
train_loss: 0.6107131838798523, val_loss: 0.7799572944641113

('adam', 0.1, 16, 100)
train_loss: 0.6115018725395203, val_loss: 0.7731267809867859

('adam', 0.1, 32, 10)
train_loss: 0.6244056820869446, val_loss: 0.8156642317771912

('adam', 0.1, 32, 50)
train_loss: 0.6113812327384949, val_loss: 0.7615554928779602

('adam', 0.1, 32, 100)
train_loss: 0.6114231944084167, val_loss: 0.7597383856773376

('adam', 0.1, 64, 10)
train_loss: 0.6247881650924683, val_loss: 0.7785395383834839

('adam', 0.1, 64, 50)
train_loss: 0.6109984517097473, val_loss: 0.7916877865791321

('adam', 0.1, 64, 100)
train_loss: 0.6105555295944214, val_loss: 0.7784117460250854


---------
BEST MODEL
('adam', 0.001, 64, 100)
val_loss: 0.68498694896698
---------

Run from 2023-04-10 13:55:10.550024
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6880149841308594, val_loss: 0.6926813125610352

('adam', 0.001, 8, 50)
train_loss: 0.6883090138435364, val_loss: 0.7029374837875366

('adam', 0.001, 8, 100)
train_loss: 0.6464163064956665, val_loss: 0.7051429748535156

('adam', 0.001, 16, 10)
train_loss: 0.6943045258522034, val_loss: 0.703582763671875

('adam', 0.001, 16, 50)
train_loss: 0.7036653161048889, val_loss: 0.702248215675354

('adam', 0.001, 16, 100)
train_loss: 0.6715883612632751, val_loss: 0.6996564269065857

('adam', 0.001, 32, 10)
train_loss: 0.720184326171875, val_loss: 0.7011041045188904

('adam', 0.001, 32, 50)
train_loss: 0.6859126091003418, val_loss: 0.6934685707092285

('adam', 0.001, 32, 100)
train_loss: 0.6672828793525696, val_loss: 0.6962507367134094

('adam', 0.001, 64, 10)
train_loss: 0.7233572006225586, val_loss: 0.7298035025596619

('adam', 0.001, 64, 50)
train_loss: 0.6953023076057434, val_loss: 0.7099857330322266

('adam', 0.001, 64, 100)
train_loss: 0.7192761898040771, val_loss: 0.7387665510177612

('adam', 0.01, 8, 10)
train_loss: 0.6641414165496826, val_loss: 0.7222712635993958

('adam', 0.01, 8, 50)
train_loss: 0.6048254370689392, val_loss: 0.774768054485321

('adam', 0.01, 8, 100)
train_loss: 0.6062552332878113, val_loss: 0.760807991027832

('adam', 0.01, 16, 10)
train_loss: 0.6556395292282104, val_loss: 0.699865996837616

('adam', 0.01, 16, 50)
train_loss: 0.6168277263641357, val_loss: 0.7475227117538452

('adam', 0.01, 16, 100)
train_loss: 0.6028456091880798, val_loss: 0.781371533870697

('adam', 0.01, 32, 10)
train_loss: 0.7229740619659424, val_loss: 0.7404722571372986

('adam', 0.01, 32, 50)
train_loss: 0.6298151016235352, val_loss: 0.7332488894462585

('adam', 0.01, 32, 100)
train_loss: 0.6029810905456543, val_loss: 0.770588755607605

('adam', 0.01, 64, 10)
train_loss: 0.7087468504905701, val_loss: 0.7009514570236206

('adam', 0.01, 64, 50)
train_loss: 0.6330111026763916, val_loss: 0.7110896706581116

('adam', 0.01, 64, 100)
train_loss: 0.6185129880905151, val_loss: 0.7488030195236206

('adam', 0.1, 8, 10)
train_loss: 0.611689031124115, val_loss: 0.7461429238319397

('adam', 0.1, 8, 50)
train_loss: 0.6002362966537476, val_loss: 0.7635214924812317

('adam', 0.1, 8, 100)
train_loss: 0.6056733727455139, val_loss: 0.7763211131095886

('adam', 0.1, 16, 10)
train_loss: 0.6061574220657349, val_loss: 0.7594489455223083

('adam', 0.1, 16, 50)
train_loss: 0.6005129814147949, val_loss: 0.813158392906189

('adam', 0.1, 16, 100)
train_loss: 0.5999534130096436, val_loss: 0.8064029812812805

('adam', 0.1, 32, 10)
train_loss: 0.6168816685676575, val_loss: 0.7955420613288879

('adam', 0.1, 32, 50)
train_loss: 0.6051532626152039, val_loss: 0.8269092440605164

('adam', 0.1, 32, 100)
train_loss: 0.6001110076904297, val_loss: 0.8240020871162415

('adam', 0.1, 64, 10)
train_loss: 0.6168965697288513, val_loss: 0.7777430415153503

('adam', 0.1, 64, 50)
train_loss: 0.5998740792274475, val_loss: 0.7983465790748596

('adam', 0.1, 64, 100)
train_loss: 0.5998495817184448, val_loss: 0.8028021454811096


---------
BEST MODEL
('adam', 0.001, 8, 10)
val_loss: 0.6926813125610352
---------

Run from 2023-04-17 09:44:27.173401
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7011966109275818, val_loss: 0.7560353875160217

('adam', 0.001, 8, 50)
train_loss: 0.67191082239151, val_loss: 0.7300723791122437

('adam', 0.001, 8, 100)
train_loss: 0.666255533695221, val_loss: 0.8142296075820923

('adam', 0.001, 16, 10)
train_loss: 0.6904749274253845, val_loss: 0.6916146278381348

('adam', 0.001, 16, 50)
train_loss: 0.6845504641532898, val_loss: 0.7366452217102051

('adam', 0.001, 16, 100)
train_loss: 0.6659930348396301, val_loss: 0.7282007336616516

('adam', 0.001, 32, 10)
train_loss: 0.7275437116622925, val_loss: 0.573890209197998

('adam', 0.001, 32, 50)
train_loss: 0.6810722947120667, val_loss: 0.6873272657394409

('adam', 0.001, 32, 100)
train_loss: 0.6985772848129272, val_loss: 0.6111237406730652

('adam', 0.001, 64, 10)
train_loss: 0.7042341232299805, val_loss: 0.610777735710144

('adam', 0.001, 64, 50)
train_loss: 0.7156710624694824, val_loss: 0.8023087382316589

('adam', 0.001, 64, 100)
train_loss: 0.6794952154159546, val_loss: 0.6871265172958374

('adam', 0.01, 8, 10)
train_loss: 0.6810927391052246, val_loss: 0.6433656215667725

('adam', 0.01, 8, 50)
train_loss: 0.6534267067909241, val_loss: 0.7059717178344727

('adam', 0.01, 8, 100)
train_loss: 0.6346086263656616, val_loss: 0.8295171856880188

('adam', 0.01, 16, 10)
train_loss: 0.6693165302276611, val_loss: 0.6871225237846375

('adam', 0.01, 16, 50)
train_loss: 0.6355586647987366, val_loss: 0.8020212054252625

('adam', 0.01, 16, 100)
train_loss: 0.6369972825050354, val_loss: 0.7847918272018433

('adam', 0.01, 32, 10)
train_loss: 0.683258593082428, val_loss: 0.7734984159469604

('adam', 0.01, 32, 50)
train_loss: 0.6512901782989502, val_loss: 0.7142415046691895

('adam', 0.01, 32, 100)
train_loss: 0.636195182800293, val_loss: 0.7932480573654175

('adam', 0.01, 64, 10)
train_loss: 0.6878852844238281, val_loss: 0.638205349445343

('adam', 0.01, 64, 50)
train_loss: 0.6563546061515808, val_loss: 0.8057487607002258

('adam', 0.01, 64, 100)
train_loss: 0.6367084980010986, val_loss: 0.7898419499397278

('adam', 0.1, 8, 10)
train_loss: 0.6400888562202454, val_loss: 0.8530422449111938

('adam', 0.1, 8, 50)
train_loss: 0.6372724175453186, val_loss: 0.819351077079773

('adam', 0.1, 8, 100)
train_loss: 0.6577855348587036, val_loss: 0.7933579087257385

('adam', 0.1, 16, 10)
train_loss: 0.6655141115188599, val_loss: 0.6902715563774109

('adam', 0.1, 16, 50)
train_loss: 0.6361051201820374, val_loss: 0.8830996751785278

('adam', 0.1, 16, 100)
train_loss: 0.6493712663650513, val_loss: 0.8526936769485474

('adam', 0.1, 32, 10)
train_loss: 0.6379458904266357, val_loss: 0.8164094686508179

('adam', 0.1, 32, 50)
train_loss: 0.6352204084396362, val_loss: 0.8269786834716797

('adam', 0.1, 32, 100)
train_loss: 0.6348128318786621, val_loss: 0.8409267663955688

('adam', 0.1, 64, 10)
train_loss: 0.6442080140113831, val_loss: 0.7813767194747925

('adam', 0.1, 64, 50)
train_loss: 0.6345400214195251, val_loss: 0.827289879322052

('adam', 0.1, 64, 100)
train_loss: 0.6344313025474548, val_loss: 0.8358737230300903

('sgd', 0.001, 8, 10)
train_loss: 0.7076688408851624, val_loss: 0.7651461362838745

('sgd', 0.001, 8, 50)
train_loss: 0.7375879287719727, val_loss: 0.8602781295776367

('sgd', 0.001, 8, 100)
train_loss: 0.6817875504493713, val_loss: 0.7342684864997864

('sgd', 0.001, 16, 10)
train_loss: 0.7264504432678223, val_loss: 0.8094967603683472

('sgd', 0.001, 16, 50)
train_loss: 0.6884788870811462, val_loss: 0.6876497268676758

('sgd', 0.001, 16, 100)
train_loss: 0.6985986828804016, val_loss: 0.7671774625778198

('sgd', 0.001, 32, 10)
train_loss: 0.7170045971870422, val_loss: 0.7855441570281982

('sgd', 0.001, 32, 50)
train_loss: 0.6904367208480835, val_loss: 0.6684935688972473

('sgd', 0.001, 32, 100)
train_loss: 0.740210235118866, val_loss: 0.8534356951713562

('sgd', 0.001, 64, 10)
train_loss: 0.6949836015701294, val_loss: 0.7089487910270691

('sgd', 0.001, 64, 50)
train_loss: 0.7124956846237183, val_loss: 0.7767305970191956

('sgd', 0.001, 64, 100)
train_loss: 0.6961050033569336, val_loss: 0.6327919363975525

('sgd', 0.01, 8, 10)
train_loss: 0.6840983033180237, val_loss: 0.6537755131721497

('sgd', 0.01, 8, 50)
train_loss: 0.649135172367096, val_loss: 0.7652703523635864

('sgd', 0.01, 8, 100)
train_loss: 0.6762474775314331, val_loss: 0.6520997285842896

('sgd', 0.01, 16, 10)
train_loss: 0.6968252062797546, val_loss: 0.6231433153152466

('sgd', 0.01, 16, 50)
train_loss: 0.6959810853004456, val_loss: 0.6155737042427063

('sgd', 0.01, 16, 100)
train_loss: 0.6441617012023926, val_loss: 0.7879799604415894

('sgd', 0.01, 32, 10)
train_loss: 0.7079597115516663, val_loss: 0.6022206544876099

('sgd', 0.01, 32, 50)
train_loss: 0.714720606803894, val_loss: 0.5892091989517212

('sgd', 0.01, 32, 100)
train_loss: 0.7008376717567444, val_loss: 0.6079822778701782

('sgd', 0.01, 64, 10)
train_loss: 0.6905226707458496, val_loss: 0.6708241701126099

('sgd', 0.01, 64, 50)
train_loss: 0.6999219655990601, val_loss: 0.7823413610458374

('sgd', 0.01, 64, 100)
train_loss: 0.6848317384719849, val_loss: 0.7781552076339722

('sgd', 0.1, 8, 10)
train_loss: 0.6972389817237854, val_loss: 0.6153836250305176

('sgd', 0.1, 8, 50)
train_loss: 0.649331271648407, val_loss: 0.731648325920105

('sgd', 0.1, 8, 100)
train_loss: 0.6368716955184937, val_loss: 0.8514069318771362

('sgd', 0.1, 16, 10)
train_loss: 0.6576612591743469, val_loss: 0.7165874242782593

('sgd', 0.1, 16, 50)
train_loss: 0.6728555560112, val_loss: 0.7028711438179016

('sgd', 0.1, 16, 100)
train_loss: 0.6353458762168884, val_loss: 0.8036702275276184

('sgd', 0.1, 32, 10)
train_loss: 0.6707786917686462, val_loss: 0.6721919178962708

('sgd', 0.1, 32, 50)
train_loss: 0.6354424357414246, val_loss: 0.8229576349258423

('sgd', 0.1, 32, 100)
train_loss: 0.6487164497375488, val_loss: 0.7264655828475952

('sgd', 0.1, 64, 10)
train_loss: 0.6785555481910706, val_loss: 0.7394034266471863

('sgd', 0.1, 64, 50)
train_loss: 0.6434647440910339, val_loss: 0.8236757516860962

('sgd', 0.1, 64, 100)
train_loss: 0.6882623434066772, val_loss: 0.637396514415741


---------
BEST MODEL
('adam', 0.001, 32, 10)
val_loss: 0.573890209197998
---------

Run from 2023-04-17 16:11:26.605888
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6936774849891663, val_loss: 0.6701779365539551

('adam', 0.001, 8, 50)
train_loss: 0.6697989702224731, val_loss: 0.7700847387313843

('adam', 0.001, 8, 100)
train_loss: 0.696553647518158, val_loss: 0.700296938419342

('adam', 0.001, 16, 10)
train_loss: 0.7170782089233398, val_loss: 0.676908552646637

('adam', 0.001, 16, 50)
train_loss: 0.6825868487358093, val_loss: 0.6774730682373047

('adam', 0.001, 16, 100)
train_loss: 0.670874834060669, val_loss: 0.6868310570716858

('adam', 0.001, 32, 10)
train_loss: 0.7286280393600464, val_loss: 0.6854130029678345

('adam', 0.001, 32, 50)
train_loss: 0.7073820233345032, val_loss: 0.6779501438140869

('adam', 0.001, 32, 100)
train_loss: 0.7011741399765015, val_loss: 0.6873819828033447

('adam', 0.001, 64, 10)
train_loss: 0.6919337511062622, val_loss: 0.6983253955841064

('adam', 0.001, 64, 50)
train_loss: 0.7259035110473633, val_loss: 0.6912500262260437

('adam', 0.001, 64, 100)
train_loss: 0.6995432376861572, val_loss: 0.8226107358932495

('adam', 0.01, 8, 10)
train_loss: 0.6741805672645569, val_loss: 0.6937706470489502

('adam', 0.01, 8, 50)
train_loss: 0.6263977885246277, val_loss: 0.7843953371047974

('adam', 0.01, 8, 100)
train_loss: 0.6238000988960266, val_loss: 0.8112073540687561

('adam', 0.01, 16, 10)
train_loss: 0.6817003488540649, val_loss: 0.688962459564209

('adam', 0.01, 16, 50)
train_loss: 0.6275613307952881, val_loss: 0.7751648426055908

('adam', 0.01, 16, 100)
train_loss: 0.6258333921432495, val_loss: 0.7879505157470703

('adam', 0.01, 32, 10)
train_loss: 0.6798964142799377, val_loss: 0.7781689167022705

('adam', 0.01, 32, 50)
train_loss: 0.6321985125541687, val_loss: 0.754688024520874

('adam', 0.01, 32, 100)
train_loss: 0.6318865418434143, val_loss: 0.7597543597221375

('adam', 0.01, 64, 10)
train_loss: 0.6921494603157043, val_loss: 0.7825556397438049

('adam', 0.01, 64, 50)
train_loss: 0.6490598917007446, val_loss: 0.8173493146896362

('adam', 0.01, 64, 100)
train_loss: 0.6316966414451599, val_loss: 0.7580572962760925

('adam', 0.1, 8, 10)
train_loss: 0.6253413558006287, val_loss: 0.8387353420257568

('adam', 0.1, 8, 50)
train_loss: 0.6238580346107483, val_loss: 0.84293133020401

('adam', 0.1, 8, 100)
train_loss: 0.6238616108894348, val_loss: 0.8429084420204163

('adam', 0.1, 16, 10)
train_loss: 0.6415496468544006, val_loss: 0.7518221139907837

('adam', 0.1, 16, 50)
train_loss: 0.6229733824729919, val_loss: 0.8360852599143982

('adam', 0.1, 16, 100)
train_loss: 0.6243074536323547, val_loss: 0.8385341167449951

('adam', 0.1, 32, 10)
train_loss: 0.6268423199653625, val_loss: 0.7924726605415344

('adam', 0.1, 32, 50)
train_loss: 0.6230989098548889, val_loss: 0.8562417030334473

('adam', 0.1, 32, 100)
train_loss: 0.6241468191146851, val_loss: 0.8463520407676697

('adam', 0.1, 64, 10)
train_loss: 0.6299193501472473, val_loss: 0.7874202728271484

('adam', 0.1, 64, 50)
train_loss: 0.6229442358016968, val_loss: 0.826553463935852

('adam', 0.1, 64, 100)
train_loss: 0.6227211952209473, val_loss: 0.8398261070251465

('sgd', 0.001, 8, 10)
train_loss: 0.7105865478515625, val_loss: 0.6693323850631714

('sgd', 0.001, 8, 50)
train_loss: 0.73092120885849, val_loss: 0.6913285255432129

('sgd', 0.001, 8, 100)
train_loss: 0.682286262512207, val_loss: 0.7454271912574768

('sgd', 0.001, 16, 10)
train_loss: 0.7315464615821838, val_loss: 0.8839590549468994

('sgd', 0.001, 16, 50)
train_loss: 0.7295680642127991, val_loss: 0.8957713842391968

('sgd', 0.001, 16, 100)
train_loss: 0.6904678344726562, val_loss: 0.6733916997909546

('sgd', 0.001, 32, 10)
train_loss: 0.7122010588645935, val_loss: 0.6696439981460571

('sgd', 0.001, 32, 50)
train_loss: 0.6914241909980774, val_loss: 0.6803135275840759

('sgd', 0.001, 32, 100)
train_loss: 0.7073328495025635, val_loss: 0.6699097156524658

('sgd', 0.001, 64, 10)
train_loss: 0.7118355631828308, val_loss: 0.6692355871200562

('sgd', 0.001, 64, 50)
train_loss: 0.6982362866401672, val_loss: 0.7454304099082947

('sgd', 0.001, 64, 100)
train_loss: 0.7228148579597473, val_loss: 0.6795715093612671

('sgd', 0.01, 8, 10)
train_loss: 0.6844343543052673, val_loss: 0.6789004802703857

('sgd', 0.01, 8, 50)
train_loss: 0.6445958018302917, val_loss: 0.7795290946960449

('sgd', 0.01, 8, 100)
train_loss: 0.6958819627761841, val_loss: 0.7170190811157227

('sgd', 0.01, 16, 10)
train_loss: 0.7274686694145203, val_loss: 0.9078959226608276

('sgd', 0.01, 16, 50)
train_loss: 0.6835957765579224, val_loss: 0.6797807216644287

('sgd', 0.01, 16, 100)
train_loss: 0.6914370656013489, val_loss: 0.6961284875869751

('sgd', 0.01, 32, 10)
train_loss: 0.7288627028465271, val_loss: 0.6864414215087891

('sgd', 0.01, 32, 50)
train_loss: 0.6746893525123596, val_loss: 0.7594465613365173

('sgd', 0.01, 32, 100)
train_loss: 0.6566133499145508, val_loss: 0.7599321603775024

('sgd', 0.01, 64, 10)
train_loss: 0.7045983076095581, val_loss: 0.6673469543457031

('sgd', 0.01, 64, 50)
train_loss: 0.7087684869766235, val_loss: 0.86556077003479

('sgd', 0.01, 64, 100)
train_loss: 0.7142040133476257, val_loss: 0.6874780654907227

('sgd', 0.1, 8, 10)
train_loss: 0.6583686470985413, val_loss: 0.7123619318008423

('sgd', 0.1, 8, 50)
train_loss: 0.6261270046234131, val_loss: 0.8015259504318237

('sgd', 0.1, 8, 100)
train_loss: 0.626215398311615, val_loss: 0.7925612926483154

('sgd', 0.1, 16, 10)
train_loss: 0.6461908221244812, val_loss: 0.7509044408798218

('sgd', 0.1, 16, 50)
train_loss: 0.6617770791053772, val_loss: 0.7276615500450134

('sgd', 0.1, 16, 100)
train_loss: 0.6256687045097351, val_loss: 0.7955012917518616

('sgd', 0.1, 32, 10)
train_loss: 0.6654683947563171, val_loss: 0.6957305669784546

('sgd', 0.1, 32, 50)
train_loss: 0.6309427618980408, val_loss: 0.7593865990638733

('sgd', 0.1, 32, 100)
train_loss: 0.6559085249900818, val_loss: 0.7302454710006714

('sgd', 0.1, 64, 10)
train_loss: 0.7003429532051086, val_loss: 0.6780291795730591

('sgd', 0.1, 64, 50)
train_loss: 0.6320095658302307, val_loss: 0.8147180676460266

('sgd', 0.1, 64, 100)
train_loss: 0.6684152483940125, val_loss: 0.7211586236953735


---------
BEST MODEL
('sgd', 0.01, 64, 10)
val_loss: 0.6673469543457031
---------
