
Run from 2023-04-10 10:23:09.117614
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 1.1152677536010742, val_loss: 0.8335940837860107

('adam', 0.001, 8, 50)
train_loss: 1.0469890832901, val_loss: 0.8116266131401062

('adam', 0.001, 8, 100)
train_loss: 0.6454095244407654, val_loss: 0.7123776078224182

('adam', 0.001, 16, 10)
train_loss: 0.48511022329330444, val_loss: 0.7215485572814941

('adam', 0.001, 16, 50)
train_loss: 0.46135857701301575, val_loss: 0.7549745440483093

('adam', 0.001, 16, 100)
train_loss: 0.5369074940681458, val_loss: 0.7129850387573242

('adam', 0.001, 32, 10)
train_loss: 0.5668530464172363, val_loss: 0.6889482140541077

('adam', 0.001, 32, 50)
train_loss: 0.9278212189674377, val_loss: 0.7621641159057617

('adam', 0.001, 32, 100)
train_loss: 0.45057412981987, val_loss: 0.7737317085266113

('adam', 0.001, 64, 10)
train_loss: 0.8616385459899902, val_loss: 0.7365328669548035

('adam', 0.001, 64, 50)
train_loss: 1.2193528413772583, val_loss: 0.8803120255470276

('adam', 0.001, 64, 100)
train_loss: 0.4598732888698578, val_loss: 0.7557640671730042

('adam', 0.01, 8, 10)
train_loss: 0.745724618434906, val_loss: 0.7245360016822815

('adam', 0.01, 8, 50)
train_loss: 0.43887460231781006, val_loss: 0.8872153162956238

('adam', 0.01, 8, 100)
train_loss: 0.42456215620040894, val_loss: 0.8463216423988342

('adam', 0.01, 16, 10)
train_loss: 0.7845191359519958, val_loss: 0.7238362431526184

('adam', 0.01, 16, 50)
train_loss: 0.4705837070941925, val_loss: 0.8470589518547058

('adam', 0.01, 16, 100)
train_loss: 0.4811169505119324, val_loss: 0.9126400947570801

('adam', 0.01, 32, 10)
train_loss: 0.43920987844467163, val_loss: 0.8029497265815735

('adam', 0.01, 32, 50)
train_loss: 0.4536832869052887, val_loss: 0.8518776297569275

('adam', 0.01, 32, 100)
train_loss: 0.45251235365867615, val_loss: 0.8650064468383789

('adam', 0.01, 64, 10)
train_loss: 0.7520892024040222, val_loss: 0.7086989879608154

('adam', 0.01, 64, 50)
train_loss: 0.4347160756587982, val_loss: 0.8731396198272705

('adam', 0.01, 64, 100)
train_loss: 0.4356788396835327, val_loss: 0.8747873902320862

('adam', 0.1, 8, 10)
train_loss: 0.4597674012184143, val_loss: 0.9255256056785583

('adam', 0.1, 8, 50)
train_loss: 0.41597092151641846, val_loss: 0.8242735862731934

('adam', 0.1, 8, 100)
train_loss: 0.38612279295921326, val_loss: 0.7304258346557617

('adam', 0.1, 16, 10)
train_loss: 0.4809001088142395, val_loss: 1.0931752920150757

('adam', 0.1, 16, 50)
train_loss: 0.40336933732032776, val_loss: 0.8128932118415833

('adam', 0.1, 16, 100)
train_loss: 0.39340901374816895, val_loss: 0.7952580451965332

('adam', 0.1, 32, 10)
train_loss: 0.43228787183761597, val_loss: 0.8242082595825195

('adam', 0.1, 32, 50)
train_loss: 0.4570463001728058, val_loss: 0.9057027697563171

('adam', 0.1, 32, 100)
train_loss: 0.43424656987190247, val_loss: 0.9272310137748718

('adam', 0.1, 64, 10)
train_loss: 0.4474971294403076, val_loss: 0.9737785458564758

('adam', 0.1, 64, 50)
train_loss: 0.4688624441623688, val_loss: 0.9765665531158447

('adam', 0.1, 64, 100)
train_loss: 0.44954535365104675, val_loss: 0.9114993214607239


---------
BEST MODEL
('adam', 0.001, 32, 10)
val_loss: 0.6889482140541077
---------

Run from 2023-04-10 13:15:42.726105
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 1.0294462442398071, val_loss: 0.7635869979858398

('adam', 0.001, 8, 50)
train_loss: 0.8612217903137207, val_loss: 0.7250843644142151

('adam', 0.001, 8, 100)
train_loss: 0.8085570931434631, val_loss: 0.7214637398719788

('adam', 0.001, 16, 10)
train_loss: 0.7075244784355164, val_loss: 0.6954710483551025

('adam', 0.001, 16, 50)
train_loss: 1.0722274780273438, val_loss: 0.7756327986717224

('adam', 0.001, 16, 100)
train_loss: 0.452714204788208, val_loss: 0.7886376976966858

('adam', 0.001, 32, 10)
train_loss: 0.7968512177467346, val_loss: 0.7087289690971375

('adam', 0.001, 32, 50)
train_loss: 0.7506254315376282, val_loss: 0.7029969096183777

('adam', 0.001, 32, 100)
train_loss: 0.5660119652748108, val_loss: 0.7028040289878845

('adam', 0.001, 64, 10)
train_loss: 0.49110403656959534, val_loss: 0.7205772399902344

('adam', 0.001, 64, 50)
train_loss: 1.0033636093139648, val_loss: 0.7569916844367981

('adam', 0.001, 64, 100)
train_loss: 0.6245420575141907, val_loss: 0.6935069561004639

('adam', 0.01, 8, 10)
train_loss: 0.49691009521484375, val_loss: 0.7498586773872375

('adam', 0.01, 8, 50)
train_loss: 0.4769924581050873, val_loss: 0.8927016258239746

('adam', 0.01, 8, 100)
train_loss: 0.4664708077907562, val_loss: 0.9024335741996765

('adam', 0.01, 16, 10)
train_loss: 0.5381187200546265, val_loss: 0.7175626754760742

('adam', 0.01, 16, 50)
train_loss: 0.5374865531921387, val_loss: 0.7905977368354797

('adam', 0.01, 16, 100)
train_loss: 0.47343724966049194, val_loss: 0.9256789684295654

('adam', 0.01, 32, 10)
train_loss: 0.4667789936065674, val_loss: 0.7552652359008789

('adam', 0.01, 32, 50)
train_loss: 0.46444225311279297, val_loss: 0.824713408946991

('adam', 0.01, 32, 100)
train_loss: 0.4731365442276001, val_loss: 0.8913581371307373

('adam', 0.01, 64, 10)
train_loss: 0.5419036746025085, val_loss: 0.7045753598213196

('adam', 0.01, 64, 50)
train_loss: 0.4987408518791199, val_loss: 0.7533510327339172

('adam', 0.01, 64, 100)
train_loss: 0.6000441908836365, val_loss: 0.7418889999389648

('adam', 0.1, 8, 10)
train_loss: 0.47543612122535706, val_loss: 0.9939765930175781

('adam', 0.1, 8, 50)
train_loss: 0.43736326694488525, val_loss: 0.7581937909126282

('adam', 0.1, 8, 100)
train_loss: 0.41767701506614685, val_loss: 0.6681079268455505

('adam', 0.1, 16, 10)
train_loss: 0.4847928583621979, val_loss: 1.0703550577163696

('adam', 0.1, 16, 50)
train_loss: 0.46364423632621765, val_loss: 0.9208199381828308

('adam', 0.1, 16, 100)
train_loss: 0.43131160736083984, val_loss: 0.7342104911804199

('adam', 0.1, 32, 10)
train_loss: 0.47977471351623535, val_loss: 0.9677441120147705

('adam', 0.1, 32, 50)
train_loss: 0.4701601266860962, val_loss: 0.9010952115058899

('adam', 0.1, 32, 100)
train_loss: 0.44975924491882324, val_loss: 0.857597827911377

('adam', 0.1, 64, 10)
train_loss: 0.5308171510696411, val_loss: 0.7918059229850769

('adam', 0.1, 64, 50)
train_loss: 0.4400806427001953, val_loss: 0.7728808522224426

('adam', 0.1, 64, 100)
train_loss: 0.4307381510734558, val_loss: 0.7329737544059753


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6681079268455505
---------

Run from 2023-04-10 14:42:53.323077
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 1.0000905990600586, val_loss: 0.8143131136894226

('adam', 0.001, 8, 50)
train_loss: 0.7354311347007751, val_loss: 0.7214035987854004

('adam', 0.001, 8, 100)
train_loss: 0.7547006011009216, val_loss: 0.7440786361694336

('adam', 0.001, 16, 10)
train_loss: 0.9177878499031067, val_loss: 0.7752721905708313

('adam', 0.001, 16, 50)
train_loss: 1.0477397441864014, val_loss: 0.8449983596801758

('adam', 0.001, 16, 100)
train_loss: 0.4875795543193817, val_loss: 0.7359922528266907

('adam', 0.001, 32, 10)
train_loss: 0.549313485622406, val_loss: 0.6855828166007996

('adam', 0.001, 32, 50)
train_loss: 0.6836602091789246, val_loss: 0.6978263258934021

('adam', 0.001, 32, 100)
train_loss: 0.5268725752830505, val_loss: 0.7048659920692444

('adam', 0.001, 64, 10)
train_loss: 0.6042268872261047, val_loss: 0.6807228922843933

('adam', 0.001, 64, 50)
train_loss: 0.9999628663063049, val_loss: 0.8151814341545105

('adam', 0.001, 64, 100)
train_loss: 0.7209134101867676, val_loss: 0.7075398564338684

('adam', 0.01, 8, 10)
train_loss: 0.7268447875976562, val_loss: 0.7337429523468018

('adam', 0.01, 8, 50)
train_loss: 0.4580557346343994, val_loss: 0.8898375630378723

('adam', 0.01, 8, 100)
train_loss: 0.44992485642433167, val_loss: 0.8776705861091614

('adam', 0.01, 16, 10)
train_loss: 0.49365851283073425, val_loss: 0.7332806587219238

('adam', 0.01, 16, 50)
train_loss: 0.46686092019081116, val_loss: 0.8611845374107361

('adam', 0.01, 16, 100)
train_loss: 0.4537877142429352, val_loss: 0.89225834608078

('adam', 0.01, 32, 10)
train_loss: 0.5950143933296204, val_loss: 0.6949977874755859

('adam', 0.01, 32, 50)
train_loss: 0.613422155380249, val_loss: 0.7491992115974426

('adam', 0.01, 32, 100)
train_loss: 0.4641747772693634, val_loss: 0.8829477429389954

('adam', 0.01, 64, 10)
train_loss: 0.4922374486923218, val_loss: 0.7207991480827332

('adam', 0.01, 64, 50)
train_loss: 0.4471317231655121, val_loss: 0.8577864766120911

('adam', 0.01, 64, 100)
train_loss: 0.45505648851394653, val_loss: 0.8625194430351257

('adam', 0.1, 8, 10)
train_loss: 0.4751339852809906, val_loss: 0.974656879901886

('adam', 0.1, 8, 50)
train_loss: 0.42737123370170593, val_loss: 0.7982293963432312

('adam', 0.1, 8, 100)
train_loss: 0.4141441285610199, val_loss: 0.7623751163482666

('adam', 0.1, 16, 10)
train_loss: 0.4837326109409332, val_loss: 1.0824894905090332

('adam', 0.1, 16, 50)
train_loss: 0.45665520429611206, val_loss: 0.9008012413978577

('adam', 0.1, 16, 100)
train_loss: 0.4296846389770508, val_loss: 0.7605312466621399

('adam', 0.1, 32, 10)
train_loss: 0.466682493686676, val_loss: 1.0248833894729614

('adam', 0.1, 32, 50)
train_loss: 0.4489355683326721, val_loss: 0.8327276110649109

('adam', 0.1, 32, 100)
train_loss: 0.4395885467529297, val_loss: 0.7971828579902649

('adam', 0.1, 64, 10)
train_loss: 0.5940399169921875, val_loss: 0.7686828970909119

('adam', 0.1, 64, 50)
train_loss: 0.44174179434776306, val_loss: 0.8637371063232422

('adam', 0.1, 64, 100)
train_loss: 0.44844603538513184, val_loss: 0.8674734234809875


---------
BEST MODEL
('adam', 0.001, 64, 10)
val_loss: 0.6807228922843933
---------

Run from 2023-04-17 10:03:07.217088
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 1.0702176094055176, val_loss: 0.8424792289733887

('adam', 0.001, 8, 50)
train_loss: 0.5313635468482971, val_loss: 0.7155135869979858

('adam', 0.001, 8, 100)
train_loss: 0.5498746037483215, val_loss: 0.7247918844223022

('adam', 0.001, 16, 10)
train_loss: 0.8681414723396301, val_loss: 0.7514477372169495

('adam', 0.001, 16, 50)
train_loss: 0.4822123050689697, val_loss: 0.7648887634277344

('adam', 0.001, 16, 100)
train_loss: 0.4956812262535095, val_loss: 0.7502734065055847

('adam', 0.001, 32, 10)
train_loss: 0.7330452799797058, val_loss: 0.7042308449745178

('adam', 0.001, 32, 50)
train_loss: 0.5301812291145325, val_loss: 0.705284595489502

('adam', 0.001, 32, 100)
train_loss: 0.7778623700141907, val_loss: 0.7279599905014038

('adam', 0.001, 64, 10)
train_loss: 0.49776405096054077, val_loss: 0.7270730137825012

('adam', 0.001, 64, 50)
train_loss: 0.7090893983840942, val_loss: 0.6999659538269043

('adam', 0.001, 64, 100)
train_loss: 0.5128585696220398, val_loss: 0.718868613243103

('adam', 0.01, 8, 10)
train_loss: 0.4811961352825165, val_loss: 0.7887171506881714

('adam', 0.01, 8, 50)
train_loss: 0.49231061339378357, val_loss: 0.8408623933792114

('adam', 0.01, 8, 100)
train_loss: 0.4955889880657196, val_loss: 0.8508235812187195

('adam', 0.01, 16, 10)
train_loss: 0.6558141112327576, val_loss: 0.70404052734375

('adam', 0.01, 16, 50)
train_loss: 0.49527955055236816, val_loss: 0.8388178944587708

('adam', 0.01, 16, 100)
train_loss: 0.5184319019317627, val_loss: 0.8852376937866211

('adam', 0.01, 32, 10)
train_loss: 0.6289282441139221, val_loss: 0.6958805322647095

('adam', 0.01, 32, 50)
train_loss: 0.5040284991264343, val_loss: 0.8036792874336243

('adam', 0.01, 32, 100)
train_loss: 0.48831504583358765, val_loss: 0.8284808397293091

('adam', 0.01, 64, 10)
train_loss: 0.5494509935379028, val_loss: 0.6983743906021118

('adam', 0.01, 64, 50)
train_loss: 0.6872245669364929, val_loss: 0.7233163714408875

('adam', 0.01, 64, 100)
train_loss: 0.500529408454895, val_loss: 0.816087543964386

('adam', 0.1, 8, 10)
train_loss: 0.4741719961166382, val_loss: 0.7909486293792725

('adam', 0.1, 8, 50)
train_loss: 0.4432395398616791, val_loss: 0.7182880640029907

('adam', 0.1, 8, 100)
train_loss: 0.42803892493247986, val_loss: 0.6267449259757996

('adam', 0.1, 16, 10)
train_loss: 0.4837943911552429, val_loss: 0.7986777424812317

('adam', 0.1, 16, 50)
train_loss: 0.4635840058326721, val_loss: 0.7401140928268433

('adam', 0.1, 16, 100)
train_loss: 0.4434594511985779, val_loss: 0.7109212279319763

('adam', 0.1, 32, 10)
train_loss: 0.525489330291748, val_loss: 0.9488360285758972

('adam', 0.1, 32, 50)
train_loss: 0.4671145975589752, val_loss: 0.7509750127792358

('adam', 0.1, 32, 100)
train_loss: 0.44175606966018677, val_loss: 0.6909060478210449

('adam', 0.1, 64, 10)
train_loss: 0.4751218259334564, val_loss: 0.7939189076423645

('adam', 0.1, 64, 50)
train_loss: 0.5072745680809021, val_loss: 0.8517548441886902

('adam', 0.1, 64, 100)
train_loss: 0.4743069112300873, val_loss: 0.7950981855392456

('sgd', 0.001, 8, 10)
train_loss: 0.520818829536438, val_loss: 0.7034096717834473

('sgd', 0.001, 8, 50)
train_loss: 0.7444332838058472, val_loss: 0.7126517295837402

('sgd', 0.001, 8, 100)
train_loss: 0.642737090587616, val_loss: 0.6953164339065552

('sgd', 0.001, 16, 10)
train_loss: 0.4988887310028076, val_loss: 0.724433958530426

('sgd', 0.001, 16, 50)
train_loss: 0.5632336139678955, val_loss: 0.6874356269836426

('sgd', 0.001, 16, 100)
train_loss: 0.5355360507965088, val_loss: 0.6979846954345703

('sgd', 0.001, 32, 10)
train_loss: 0.6102100014686584, val_loss: 0.6819782257080078

('sgd', 0.001, 32, 50)
train_loss: 0.5765868425369263, val_loss: 0.6846945881843567

('sgd', 0.001, 32, 100)
train_loss: 0.5780994892120361, val_loss: 0.6859745979309082

('sgd', 0.001, 64, 10)
train_loss: 1.0732841491699219, val_loss: 0.8427106738090515

('sgd', 0.001, 64, 50)
train_loss: 0.5380892157554626, val_loss: 0.6938174962997437

('sgd', 0.001, 64, 100)
train_loss: 0.8452416658401489, val_loss: 0.7434847950935364

('sgd', 0.01, 8, 10)
train_loss: 0.8097012042999268, val_loss: 0.7399297952651978

('sgd', 0.01, 8, 50)
train_loss: 0.5532358884811401, val_loss: 0.7433561086654663

('sgd', 0.01, 8, 100)
train_loss: 0.49163004755973816, val_loss: 0.7896981239318848

('sgd', 0.01, 16, 10)
train_loss: 0.6764426231384277, val_loss: 0.6948075890541077

('sgd', 0.01, 16, 50)
train_loss: 0.7030185461044312, val_loss: 0.7316147685050964

('sgd', 0.01, 16, 100)
train_loss: 0.5202720761299133, val_loss: 0.7482421398162842

('sgd', 0.01, 32, 10)
train_loss: 0.8830720782279968, val_loss: 0.7592884302139282

('sgd', 0.01, 32, 50)
train_loss: 0.5009808540344238, val_loss: 0.7303363680839539

('sgd', 0.01, 32, 100)
train_loss: 0.5640836954116821, val_loss: 0.7213815450668335

('sgd', 0.01, 64, 10)
train_loss: 1.0347139835357666, val_loss: 0.824795126914978

('sgd', 0.01, 64, 50)
train_loss: 0.8617464900016785, val_loss: 0.7588697671890259

('sgd', 0.01, 64, 100)
train_loss: 0.5226252675056458, val_loss: 0.7134760618209839

('sgd', 0.1, 8, 10)
train_loss: 0.5128914713859558, val_loss: 0.8202045559883118

('sgd', 0.1, 8, 50)
train_loss: 0.504898190498352, val_loss: 0.8629899024963379

('sgd', 0.1, 8, 100)
train_loss: 0.5073979496955872, val_loss: 0.8851805925369263

('sgd', 0.1, 16, 10)
train_loss: 0.5433645844459534, val_loss: 0.7430893182754517

('sgd', 0.1, 16, 50)
train_loss: 0.526149332523346, val_loss: 0.9077423810958862

('sgd', 0.1, 16, 100)
train_loss: 0.47888487577438354, val_loss: 0.798423171043396

('sgd', 0.1, 32, 10)
train_loss: 0.5451698899269104, val_loss: 0.7276352643966675

('sgd', 0.1, 32, 50)
train_loss: 0.4903137981891632, val_loss: 0.8267954587936401

('sgd', 0.1, 32, 100)
train_loss: 0.49128296971321106, val_loss: 0.8223246335983276

('sgd', 0.1, 64, 10)
train_loss: 0.5168429017066956, val_loss: 0.718030571937561

('sgd', 0.1, 64, 50)
train_loss: 0.5543718338012695, val_loss: 0.8145581483840942

('sgd', 0.1, 64, 100)
train_loss: 0.4815559983253479, val_loss: 0.8001528978347778


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6267449259757996
---------

Run from 2023-04-17 16:54:02.446593
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.5834463238716125, val_loss: 0.6896122097969055

('adam', 0.001, 8, 50)
train_loss: 0.9565364718437195, val_loss: 0.7946689128875732

('adam', 0.001, 8, 100)
train_loss: 0.5377423763275146, val_loss: 0.7383017539978027

('adam', 0.001, 16, 10)
train_loss: 0.987000048160553, val_loss: 0.7985058426856995

('adam', 0.001, 16, 50)
train_loss: 0.8433746099472046, val_loss: 0.7444465160369873

('adam', 0.001, 16, 100)
train_loss: 0.5104385018348694, val_loss: 0.7489943504333496

('adam', 0.001, 32, 10)
train_loss: 0.6192373633384705, val_loss: 0.6854214668273926

('adam', 0.001, 32, 50)
train_loss: 0.5351408123970032, val_loss: 0.7102777361869812

('adam', 0.001, 32, 100)
train_loss: 0.5479929447174072, val_loss: 0.711291491985321

('adam', 0.001, 64, 10)
train_loss: 0.5414149761199951, val_loss: 0.7002079486846924

('adam', 0.001, 64, 50)
train_loss: 0.5299592614173889, val_loss: 0.7109901309013367

('adam', 0.001, 64, 100)
train_loss: 0.7791281342506409, val_loss: 0.7207204699516296

('adam', 0.01, 8, 10)
train_loss: 0.7360444664955139, val_loss: 0.7303757667541504

('adam', 0.01, 8, 50)
train_loss: 0.5266342759132385, val_loss: 0.8510507941246033

('adam', 0.01, 8, 100)
train_loss: 0.4765271246433258, val_loss: 0.7770760655403137

('adam', 0.01, 16, 10)
train_loss: 0.9081003069877625, val_loss: 0.7710536122322083

('adam', 0.01, 16, 50)
train_loss: 0.4965934455394745, val_loss: 0.8158063888549805

('adam', 0.01, 16, 100)
train_loss: 0.5366911888122559, val_loss: 0.8761373162269592

('adam', 0.01, 32, 10)
train_loss: 0.7636774182319641, val_loss: 0.7185317873954773

('adam', 0.01, 32, 50)
train_loss: 0.4878014624118805, val_loss: 0.8020731806755066

('adam', 0.01, 32, 100)
train_loss: 0.5018316507339478, val_loss: 0.8333401679992676

('adam', 0.01, 64, 10)
train_loss: 0.7751634120941162, val_loss: 0.7180018424987793

('adam', 0.01, 64, 50)
train_loss: 0.5227600932121277, val_loss: 0.7604115605354309

('adam', 0.01, 64, 100)
train_loss: 0.548775315284729, val_loss: 0.7707669138908386

('adam', 0.1, 8, 10)
train_loss: 0.4782768487930298, val_loss: 0.7756713032722473

('adam', 0.1, 8, 50)
train_loss: 0.46487024426460266, val_loss: 0.725261390209198

('adam', 0.1, 8, 100)
train_loss: 0.42851781845092773, val_loss: 0.6464865207672119

('adam', 0.1, 16, 10)
train_loss: 0.5557393431663513, val_loss: 1.0411752462387085

('adam', 0.1, 16, 50)
train_loss: 0.4648604393005371, val_loss: 0.7480759620666504

('adam', 0.1, 16, 100)
train_loss: 0.4420027434825897, val_loss: 0.7185487151145935

('adam', 0.1, 32, 10)
train_loss: 0.5308942198753357, val_loss: 0.9874176979064941

('adam', 0.1, 32, 50)
train_loss: 0.45405709743499756, val_loss: 0.7483286261558533

('adam', 0.1, 32, 100)
train_loss: 0.4489913284778595, val_loss: 0.7334540486335754

('adam', 0.1, 64, 10)
train_loss: 0.4954991042613983, val_loss: 0.8571966290473938

('adam', 0.1, 64, 50)
train_loss: 0.47120562195777893, val_loss: 0.7610707879066467

('adam', 0.1, 64, 100)
train_loss: 0.42304059863090515, val_loss: 0.6587992310523987

('sgd', 0.001, 8, 10)
train_loss: 0.8454534411430359, val_loss: 0.7398435473442078

('sgd', 0.001, 8, 50)
train_loss: 0.5222710967063904, val_loss: 0.7150413393974304

('sgd', 0.001, 8, 100)
train_loss: 0.9901846051216125, val_loss: 0.8104926943778992

('sgd', 0.001, 16, 10)
train_loss: 0.7074323296546936, val_loss: 0.6966362595558167

('sgd', 0.001, 16, 50)
train_loss: 0.7645635008811951, val_loss: 0.713892936706543

('sgd', 0.001, 16, 100)
train_loss: 0.5740354061126709, val_loss: 0.6916524767875671

('sgd', 0.001, 32, 10)
train_loss: 0.629019558429718, val_loss: 0.6847708821296692

('sgd', 0.001, 32, 50)
train_loss: 1.1419824361801147, val_loss: 0.8734588027000427

('sgd', 0.001, 32, 100)
train_loss: 0.5722923278808594, val_loss: 0.6907253265380859

('sgd', 0.001, 64, 10)
train_loss: 0.8055322766304016, val_loss: 0.7248930335044861

('sgd', 0.001, 64, 50)
train_loss: 1.039512276649475, val_loss: 0.8229668736457825

('sgd', 0.001, 64, 100)
train_loss: 0.4914194941520691, val_loss: 0.7640518546104431

('sgd', 0.01, 8, 10)
train_loss: 0.7179575562477112, val_loss: 0.7077311873435974

('sgd', 0.01, 8, 50)
train_loss: 0.4964091181755066, val_loss: 0.7689535617828369

('sgd', 0.01, 8, 100)
train_loss: 0.5060246586799622, val_loss: 0.7859358787536621

('sgd', 0.01, 16, 10)
train_loss: 0.5032198429107666, val_loss: 0.7377219200134277

('sgd', 0.01, 16, 50)
train_loss: 0.6230606436729431, val_loss: 0.7110219597816467

('sgd', 0.01, 16, 100)
train_loss: 0.5589494705200195, val_loss: 0.7445165514945984

('sgd', 0.01, 32, 10)
train_loss: 0.9514267444610596, val_loss: 0.7838221192359924

('sgd', 0.01, 32, 50)
train_loss: 0.5523319244384766, val_loss: 0.7066659927368164

('sgd', 0.01, 32, 100)
train_loss: 0.4981967806816101, val_loss: 0.7582058906555176

('sgd', 0.01, 64, 10)
train_loss: 0.7214031219482422, val_loss: 0.7008122801780701

('sgd', 0.01, 64, 50)
train_loss: 0.6492884755134583, val_loss: 0.6938798427581787

('sgd', 0.01, 64, 100)
train_loss: 0.5144556164741516, val_loss: 0.7290886044502258

('sgd', 0.1, 8, 10)
train_loss: 0.5328071117401123, val_loss: 0.795332133769989

('sgd', 0.1, 8, 50)
train_loss: 0.5221042633056641, val_loss: 0.8551405072212219

('sgd', 0.1, 8, 100)
train_loss: 0.4879087507724762, val_loss: 0.7972240447998047

('sgd', 0.1, 16, 10)
train_loss: 0.59566730260849, val_loss: 0.747349739074707

('sgd', 0.1, 16, 50)
train_loss: 0.5265210866928101, val_loss: 0.8666104674339294

('sgd', 0.1, 16, 100)
train_loss: 0.5235490202903748, val_loss: 0.861458957195282

('sgd', 0.1, 32, 10)
train_loss: 0.49378716945648193, val_loss: 0.7617830634117126

('sgd', 0.1, 32, 50)
train_loss: 0.5289507508277893, val_loss: 0.8578000068664551

('sgd', 0.1, 32, 100)
train_loss: 0.5090193152427673, val_loss: 0.8372316956520081

('sgd', 0.1, 64, 10)
train_loss: 0.5132400989532471, val_loss: 0.7312831878662109

('sgd', 0.1, 64, 50)
train_loss: 0.5681852698326111, val_loss: 0.8033187985420227

('sgd', 0.1, 64, 100)
train_loss: 0.5385515093803406, val_loss: 0.8636183738708496


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6464865207672119
---------
