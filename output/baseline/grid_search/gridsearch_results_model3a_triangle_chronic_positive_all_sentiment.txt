
Run from 2023-04-10 09:40:22.808654
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 1.0348905324935913, val_loss: 0.9889259934425354

('adam', 0.001, 8, 50)
train_loss: 0.5973079204559326, val_loss: 0.6382654905319214

('adam', 0.001, 8, 100)
train_loss: 0.7319808006286621, val_loss: 0.7801691889762878

('adam', 0.001, 16, 10)
train_loss: 0.6900447010993958, val_loss: 0.6941100358963013

('adam', 0.001, 16, 50)
train_loss: 0.5857969522476196, val_loss: 0.6277586817741394

('adam', 0.001, 16, 100)
train_loss: 0.5962567925453186, val_loss: 0.6473983526229858

('adam', 0.001, 32, 10)
train_loss: 0.6415502429008484, val_loss: 0.6555951237678528

('adam', 0.001, 32, 50)
train_loss: 0.5884104371070862, val_loss: 0.6280863881111145

('adam', 0.001, 32, 100)
train_loss: 0.8180891871452332, val_loss: 0.8224982619285583

('adam', 0.001, 64, 10)
train_loss: 0.739335298538208, val_loss: 0.7317587733268738

('adam', 0.001, 64, 50)
train_loss: 0.9684838056564331, val_loss: 0.9322490096092224

('adam', 0.001, 64, 100)
train_loss: 0.9486252665519714, val_loss: 0.9221468567848206

('adam', 0.01, 8, 10)
train_loss: 0.7699083685874939, val_loss: 0.796894907951355

('adam', 0.01, 8, 50)
train_loss: 0.5860695838928223, val_loss: 0.6456472277641296

('adam', 0.01, 8, 100)
train_loss: 0.590369701385498, val_loss: 0.6815425157546997

('adam', 0.01, 16, 10)
train_loss: 0.5852439403533936, val_loss: 0.6329962015151978

('adam', 0.01, 16, 50)
train_loss: 0.5850929021835327, val_loss: 0.637558102607727

('adam', 0.01, 16, 100)
train_loss: 0.5898820161819458, val_loss: 0.6822472810745239

('adam', 0.01, 32, 10)
train_loss: 0.9668902158737183, val_loss: 0.9431846737861633

('adam', 0.01, 32, 50)
train_loss: 0.6627424955368042, val_loss: 0.7792698740959167

('adam', 0.01, 32, 100)
train_loss: 0.5944952964782715, val_loss: 0.6940456032752991

('adam', 0.01, 64, 10)
train_loss: 0.8849910497665405, val_loss: 0.8634867668151855

('adam', 0.01, 64, 50)
train_loss: 0.8144899606704712, val_loss: 0.8603935241699219

('adam', 0.01, 64, 100)
train_loss: 0.5856078267097473, val_loss: 0.6541777849197388

('adam', 0.1, 8, 10)
train_loss: 0.589309573173523, val_loss: 0.6575517058372498

('adam', 0.1, 8, 50)
train_loss: 0.586761474609375, val_loss: 0.6520164608955383

('adam', 0.1, 8, 100)
train_loss: 0.58861243724823, val_loss: 0.6545498967170715

('adam', 0.1, 16, 10)
train_loss: 0.6107208728790283, val_loss: 0.7627624869346619

('adam', 0.1, 16, 50)
train_loss: 0.5857601761817932, val_loss: 0.638628363609314

('adam', 0.1, 16, 100)
train_loss: 0.5877882242202759, val_loss: 0.6390485763549805

('adam', 0.1, 32, 10)
train_loss: 0.5862200856208801, val_loss: 0.6471339464187622

('adam', 0.1, 32, 50)
train_loss: 0.5851216912269592, val_loss: 0.6325365304946899

('adam', 0.1, 32, 100)
train_loss: 0.5856650471687317, val_loss: 0.6540195345878601

('adam', 0.1, 64, 10)
train_loss: 0.5854172706604004, val_loss: 0.6349908113479614

('adam', 0.1, 64, 50)
train_loss: 0.5849090814590454, val_loss: 0.6417578458786011

('adam', 0.1, 64, 100)
train_loss: 0.58490389585495, val_loss: 0.6411956548690796


---------
BEST MODEL
('adam', 0.001, 16, 50)
val_loss: 0.6277586817741394
---------

Run from 2023-04-10 12:45:20.310445
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.8836779594421387, val_loss: 0.8284174799919128

('adam', 0.001, 8, 50)
train_loss: 0.5393629670143127, val_loss: 0.6753056645393372

('adam', 0.001, 8, 100)
train_loss: 0.6185700297355652, val_loss: 0.6892310380935669

('adam', 0.001, 16, 10)
train_loss: 0.6102822422981262, val_loss: 0.6573160290718079

('adam', 0.001, 16, 50)
train_loss: 0.6843610405921936, val_loss: 0.7008756995201111

('adam', 0.001, 16, 100)
train_loss: 0.8657953143119812, val_loss: 0.8383680582046509

('adam', 0.001, 32, 10)
train_loss: 0.8160606026649475, val_loss: 0.7754658460617065

('adam', 0.001, 32, 50)
train_loss: 0.5421960353851318, val_loss: 0.6700811386108398

('adam', 0.001, 32, 100)
train_loss: 0.6587475538253784, val_loss: 0.6916713714599609

('adam', 0.001, 64, 10)
train_loss: 1.0124679803848267, val_loss: 0.925048828125

('adam', 0.001, 64, 50)
train_loss: 0.9591917395591736, val_loss: 0.8872174620628357

('adam', 0.001, 64, 100)
train_loss: 0.6274616718292236, val_loss: 0.6709527373313904

('adam', 0.01, 8, 10)
train_loss: 0.8160265684127808, val_loss: 0.8181719183921814

('adam', 0.01, 8, 50)
train_loss: 0.5677514672279358, val_loss: 0.723906934261322

('adam', 0.01, 8, 100)
train_loss: 0.546086311340332, val_loss: 0.7160431742668152

('adam', 0.01, 16, 10)
train_loss: 0.6342846155166626, val_loss: 0.6887542009353638

('adam', 0.01, 16, 50)
train_loss: 0.5294027924537659, val_loss: 0.7102072834968567

('adam', 0.01, 16, 100)
train_loss: 0.6065022349357605, val_loss: 0.7669041752815247

('adam', 0.01, 32, 10)
train_loss: 0.8432252407073975, val_loss: 0.8093138337135315

('adam', 0.01, 32, 50)
train_loss: 0.5343766808509827, val_loss: 0.7221773862838745

('adam', 0.01, 32, 100)
train_loss: 0.527669370174408, val_loss: 0.7170013785362244

('adam', 0.01, 64, 10)
train_loss: 0.6002829670906067, val_loss: 0.6625197529792786

('adam', 0.01, 64, 50)
train_loss: 0.7553979754447937, val_loss: 0.7833315134048462

('adam', 0.01, 64, 100)
train_loss: 0.5343039035797119, val_loss: 0.7051562666893005

('adam', 0.1, 8, 10)
train_loss: 0.5273080468177795, val_loss: 0.7337292432785034

('adam', 0.1, 8, 50)
train_loss: 0.5287889838218689, val_loss: 0.6874377131462097

('adam', 0.1, 8, 100)
train_loss: 0.5286163687705994, val_loss: 0.7487801313400269

('adam', 0.1, 16, 10)
train_loss: 0.529251217842102, val_loss: 0.6903287768363953

('adam', 0.1, 16, 50)
train_loss: 0.5275484323501587, val_loss: 0.7426241636276245

('adam', 0.1, 16, 100)
train_loss: 0.52711421251297, val_loss: 0.7143959403038025

('adam', 0.1, 32, 10)
train_loss: 0.5643357634544373, val_loss: 0.7723606824874878

('adam', 0.1, 32, 50)
train_loss: 0.5275733470916748, val_loss: 0.7200930714607239

('adam', 0.1, 32, 100)
train_loss: 0.5279816389083862, val_loss: 0.739806056022644

('adam', 0.1, 64, 10)
train_loss: 0.6144151091575623, val_loss: 0.7848711609840393

('adam', 0.1, 64, 50)
train_loss: 0.5400317311286926, val_loss: 0.6893475651741028

('adam', 0.1, 64, 100)
train_loss: 0.52671879529953, val_loss: 0.7242723107337952


---------
BEST MODEL
('adam', 0.001, 16, 10)
val_loss: 0.6573160290718079
---------

Run from 2023-04-10 14:00:17.966796
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.8047583103179932, val_loss: 0.750626266002655

('adam', 0.001, 8, 50)
train_loss: 0.5902507901191711, val_loss: 0.6576350331306458

('adam', 0.001, 8, 100)
train_loss: 0.6585411429405212, val_loss: 0.7111721634864807

('adam', 0.001, 16, 10)
train_loss: 0.5961810350418091, val_loss: 0.650030791759491

('adam', 0.001, 16, 50)
train_loss: 0.6851994395256042, val_loss: 0.7007253766059875

('adam', 0.001, 16, 100)
train_loss: 0.583074688911438, val_loss: 0.6592434644699097

('adam', 0.001, 32, 10)
train_loss: 0.5568130016326904, val_loss: 0.6327465176582336

('adam', 0.001, 32, 50)
train_loss: 0.7316505908966064, val_loss: 0.7191943526268005

('adam', 0.001, 32, 100)
train_loss: 0.5980680584907532, val_loss: 0.663059651851654

('adam', 0.001, 64, 10)
train_loss: 0.7498025894165039, val_loss: 0.7209050059318542

('adam', 0.001, 64, 50)
train_loss: 0.7632448673248291, val_loss: 0.7308779954910278

('adam', 0.001, 64, 100)
train_loss: 0.5727707147598267, val_loss: 0.6469721794128418

('adam', 0.01, 8, 10)
train_loss: 0.7203430533409119, val_loss: 0.7498550415039062

('adam', 0.01, 8, 50)
train_loss: 0.5553639531135559, val_loss: 0.6392747759819031

('adam', 0.01, 8, 100)
train_loss: 0.5554948449134827, val_loss: 0.6468033790588379

('adam', 0.01, 16, 10)
train_loss: 0.5592606663703918, val_loss: 0.6395148038864136

('adam', 0.01, 16, 50)
train_loss: 0.6265836954116821, val_loss: 0.7651886343955994

('adam', 0.01, 16, 100)
train_loss: 0.5996866226196289, val_loss: 0.7402005791664124

('adam', 0.01, 32, 10)
train_loss: 0.554142951965332, val_loss: 0.6363604664802551

('adam', 0.01, 32, 50)
train_loss: 0.6347897052764893, val_loss: 0.746745765209198

('adam', 0.01, 32, 100)
train_loss: 0.5750771164894104, val_loss: 0.6927499771118164

('adam', 0.01, 64, 10)
train_loss: 0.7708814740180969, val_loss: 0.737421989440918

('adam', 0.01, 64, 50)
train_loss: 0.7422647476196289, val_loss: 0.7670567631721497

('adam', 0.01, 64, 100)
train_loss: 0.5712186098098755, val_loss: 0.6822999119758606

('adam', 0.1, 8, 10)
train_loss: 0.5721299052238464, val_loss: 0.7064061760902405

('adam', 0.1, 8, 50)
train_loss: 0.5674694180488586, val_loss: 0.6762042045593262

('adam', 0.1, 8, 100)
train_loss: 0.5536099076271057, val_loss: 0.6309118270874023

('adam', 0.1, 16, 10)
train_loss: 0.554485023021698, val_loss: 0.6295037865638733

('adam', 0.1, 16, 50)
train_loss: 0.5541744828224182, val_loss: 0.6303344368934631

('adam', 0.1, 16, 100)
train_loss: 0.5539474487304688, val_loss: 0.626441478729248

('adam', 0.1, 32, 10)
train_loss: 0.5546382665634155, val_loss: 0.6436758041381836

('adam', 0.1, 32, 50)
train_loss: 0.5547469258308411, val_loss: 0.6398305892944336

('adam', 0.1, 32, 100)
train_loss: 0.5571421384811401, val_loss: 0.6180214285850525

('adam', 0.1, 64, 10)
train_loss: 0.678257167339325, val_loss: 0.8116313815116882

('adam', 0.1, 64, 50)
train_loss: 0.55301833152771, val_loss: 0.6285133361816406

('adam', 0.1, 64, 100)
train_loss: 0.5529786944389343, val_loss: 0.625708281993866


---------
BEST MODEL
('adam', 0.1, 32, 100)
val_loss: 0.6180214285850525
---------

Run from 2023-04-17 09:46:30.995651
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6282179951667786, val_loss: 0.6774181127548218

('adam', 0.001, 8, 50)
train_loss: 0.851402759552002, val_loss: 0.8028011322021484

('adam', 0.001, 8, 100)
train_loss: 0.6207258701324463, val_loss: 0.6976903676986694

('adam', 0.001, 16, 10)
train_loss: 0.6130109429359436, val_loss: 0.6822555661201477

('adam', 0.001, 16, 50)
train_loss: 0.7890492677688599, val_loss: 0.7546746134757996

('adam', 0.001, 16, 100)
train_loss: 0.6710682511329651, val_loss: 0.6964536309242249

('adam', 0.001, 32, 10)
train_loss: 0.6512070298194885, val_loss: 0.679153561592102

('adam', 0.001, 32, 50)
train_loss: 0.603615403175354, val_loss: 0.6999828219413757

('adam', 0.001, 32, 100)
train_loss: 0.6029393076896667, val_loss: 0.7148080468177795

('adam', 0.001, 64, 10)
train_loss: 0.604749321937561, val_loss: 0.6943717002868652

('adam', 0.001, 64, 50)
train_loss: 0.9852101802825928, val_loss: 0.8847445249557495

('adam', 0.001, 64, 100)
train_loss: 0.741175651550293, val_loss: 0.7244688272476196

('adam', 0.01, 8, 10)
train_loss: 0.6180388927459717, val_loss: 0.6938595771789551

('adam', 0.01, 8, 50)
train_loss: 0.6033618450164795, val_loss: 0.7065574526786804

('adam', 0.01, 8, 100)
train_loss: 0.6067032217979431, val_loss: 0.7289772629737854

('adam', 0.01, 16, 10)
train_loss: 0.8311049938201904, val_loss: 0.7839977145195007

('adam', 0.01, 16, 50)
train_loss: 0.603177547454834, val_loss: 0.71080082654953

('adam', 0.01, 16, 100)
train_loss: 0.6356940269470215, val_loss: 0.7233988046646118

('adam', 0.01, 32, 10)
train_loss: 0.8820351362228394, val_loss: 0.8151421546936035

('adam', 0.01, 32, 50)
train_loss: 0.6065557599067688, val_loss: 0.7126228213310242

('adam', 0.01, 32, 100)
train_loss: 0.6137198209762573, val_loss: 0.7222040295600891

('adam', 0.01, 64, 10)
train_loss: 0.6071043610572815, val_loss: 0.6978219151496887

('adam', 0.01, 64, 50)
train_loss: 0.665806770324707, val_loss: 0.7184593081474304

('adam', 0.01, 64, 100)
train_loss: 0.6246762275695801, val_loss: 0.7322240471839905

('adam', 0.1, 8, 10)
train_loss: 0.6078018546104431, val_loss: 0.7373622059822083

('adam', 0.1, 8, 50)
train_loss: 0.6098870038986206, val_loss: 0.7536500692367554

('adam', 0.1, 8, 100)
train_loss: 0.6037481427192688, val_loss: 0.7233165502548218

('adam', 0.1, 16, 10)
train_loss: 0.6012959480285645, val_loss: 0.6959401369094849

('adam', 0.1, 16, 50)
train_loss: 0.6158781051635742, val_loss: 0.6555403470993042

('adam', 0.1, 16, 100)
train_loss: 0.6046162247657776, val_loss: 0.7078473567962646

('adam', 0.1, 32, 10)
train_loss: 0.6645387411117554, val_loss: 0.8390843272209167

('adam', 0.1, 32, 50)
train_loss: 0.6040679216384888, val_loss: 0.7183806300163269

('adam', 0.1, 32, 100)
train_loss: 0.6042070388793945, val_loss: 0.6987838745117188

('adam', 0.1, 64, 10)
train_loss: 0.6572163701057434, val_loss: 0.7810107469558716

('adam', 0.1, 64, 50)
train_loss: 0.6029155254364014, val_loss: 0.7102402448654175

('adam', 0.1, 64, 100)
train_loss: 0.602656364440918, val_loss: 0.7095824480056763

('sgd', 0.001, 8, 10)
train_loss: 0.7609637379646301, val_loss: 0.7302888035774231

('sgd', 0.001, 8, 50)
train_loss: 0.8549638986587524, val_loss: 0.7953087687492371

('sgd', 0.001, 8, 100)
train_loss: 0.8908942937850952, val_loss: 0.8258307576179504

('sgd', 0.001, 16, 10)
train_loss: 0.6084997653961182, val_loss: 0.6859287023544312

('sgd', 0.001, 16, 50)
train_loss: 0.6038782596588135, val_loss: 0.7246707677841187

('sgd', 0.001, 16, 100)
train_loss: 0.6110994219779968, val_loss: 0.6838588714599609

('sgd', 0.001, 32, 10)
train_loss: 0.6839678287506104, val_loss: 0.6892849206924438

('sgd', 0.001, 32, 50)
train_loss: 0.603253960609436, val_loss: 0.7001255750656128

('sgd', 0.001, 32, 100)
train_loss: 0.7224504351615906, val_loss: 0.7101610898971558

('sgd', 0.001, 64, 10)
train_loss: 0.6027512550354004, val_loss: 0.7051191329956055

('sgd', 0.001, 64, 50)
train_loss: 0.6281507611274719, val_loss: 0.67582106590271

('sgd', 0.001, 64, 100)
train_loss: 0.6633623838424683, val_loss: 0.6821916103363037

('sgd', 0.01, 8, 10)
train_loss: 0.7550325393676758, val_loss: 0.7359610199928284

('sgd', 0.01, 8, 50)
train_loss: 0.6053142547607422, val_loss: 0.6957641839981079

('sgd', 0.01, 8, 100)
train_loss: 0.668927788734436, val_loss: 0.7592023611068726

('sgd', 0.01, 16, 10)
train_loss: 0.6036157011985779, val_loss: 0.7227175235748291

('sgd', 0.01, 16, 50)
train_loss: 0.779943585395813, val_loss: 0.7728335857391357

('sgd', 0.01, 16, 100)
train_loss: 0.6031712293624878, val_loss: 0.7021730542182922

('sgd', 0.01, 32, 10)
train_loss: 0.6175368428230286, val_loss: 0.679210364818573

('sgd', 0.01, 32, 50)
train_loss: 0.6027321815490723, val_loss: 0.7058109045028687

('sgd', 0.01, 32, 100)
train_loss: 0.6407501101493835, val_loss: 0.6926029920578003

('sgd', 0.01, 64, 10)
train_loss: 0.6722598075866699, val_loss: 0.6852597594261169

('sgd', 0.01, 64, 50)
train_loss: 0.8254202008247375, val_loss: 0.7774062156677246

('sgd', 0.01, 64, 100)
train_loss: 0.7287183403968811, val_loss: 0.724346399307251

('sgd', 0.1, 8, 10)
train_loss: 0.6046478152275085, val_loss: 0.7061992883682251

('sgd', 0.1, 8, 50)
train_loss: 0.6156222820281982, val_loss: 0.7228801846504211

('sgd', 0.1, 8, 100)
train_loss: 0.6042496562004089, val_loss: 0.7082897424697876

('sgd', 0.1, 16, 10)
train_loss: 0.6145875453948975, val_loss: 0.6987432241439819

('sgd', 0.1, 16, 50)
train_loss: 0.6110569834709167, val_loss: 0.7040364742279053

('sgd', 0.1, 16, 100)
train_loss: 0.6044835448265076, val_loss: 0.6942928433418274

('sgd', 0.1, 32, 10)
train_loss: 0.6513674259185791, val_loss: 0.6977932453155518

('sgd', 0.1, 32, 50)
train_loss: 0.6037456393241882, val_loss: 0.7074524164199829

('sgd', 0.1, 32, 100)
train_loss: 0.6121400594711304, val_loss: 0.7230315208435059

('sgd', 0.1, 64, 10)
train_loss: 0.8347933888435364, val_loss: 0.7868011593818665

('sgd', 0.1, 64, 50)
train_loss: 0.6061909794807434, val_loss: 0.7027676701545715

('sgd', 0.1, 64, 100)
train_loss: 0.6065123081207275, val_loss: 0.7107184529304504


---------
BEST MODEL
('adam', 0.1, 16, 50)
val_loss: 0.6555403470993042
---------

Run from 2023-04-17 16:15:37.766253
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.5747039914131165, val_loss: 0.7314128279685974

('adam', 0.001, 8, 50)
train_loss: 0.6098793148994446, val_loss: 0.7107966542243958

('adam', 0.001, 8, 100)
train_loss: 0.6584374308586121, val_loss: 0.7070050239562988

('adam', 0.001, 16, 10)
train_loss: 0.5776452422142029, val_loss: 0.7281743884086609

('adam', 0.001, 16, 50)
train_loss: 0.9298810958862305, val_loss: 0.7169142961502075

('adam', 0.001, 16, 100)
train_loss: 0.5856500267982483, val_loss: 0.7255662679672241

('adam', 0.001, 32, 10)
train_loss: 0.5843943953514099, val_loss: 0.7217602133750916

('adam', 0.001, 32, 50)
train_loss: 0.775118887424469, val_loss: 0.6959272623062134

('adam', 0.001, 32, 100)
train_loss: 0.8370627760887146, val_loss: 0.7035143971443176

('adam', 0.001, 64, 10)
train_loss: 0.6412129402160645, val_loss: 0.6983251571655273

('adam', 0.001, 64, 50)
train_loss: 0.700728178024292, val_loss: 0.6929594874382019

('adam', 0.001, 64, 100)
train_loss: 0.7294307351112366, val_loss: 0.6934562921524048

('adam', 0.01, 8, 10)
train_loss: 0.581498920917511, val_loss: 0.7347921133041382

('adam', 0.01, 8, 50)
train_loss: 0.5674197673797607, val_loss: 0.7570357322692871

('adam', 0.01, 8, 100)
train_loss: 0.5556867122650146, val_loss: 0.7768579125404358

('adam', 0.01, 16, 10)
train_loss: 0.5717276930809021, val_loss: 0.7427773475646973

('adam', 0.01, 16, 50)
train_loss: 0.6780651807785034, val_loss: 0.7576143145561218

('adam', 0.01, 16, 100)
train_loss: 0.5827142596244812, val_loss: 0.7509596943855286

('adam', 0.01, 32, 10)
train_loss: 0.9939604997634888, val_loss: 0.7276617884635925

('adam', 0.01, 32, 50)
train_loss: 0.5563897490501404, val_loss: 0.7727270126342773

('adam', 0.01, 32, 100)
train_loss: 0.6351230144500732, val_loss: 0.7545876502990723

('adam', 0.01, 64, 10)
train_loss: 0.7541593313217163, val_loss: 0.6943631172180176

('adam', 0.01, 64, 50)
train_loss: 0.7818781137466431, val_loss: 0.7101081609725952

('adam', 0.01, 64, 100)
train_loss: 0.6012899279594421, val_loss: 0.7466278076171875

('adam', 0.1, 8, 10)
train_loss: 0.5709250569343567, val_loss: 0.7504309415817261

('adam', 0.1, 8, 50)
train_loss: 0.5570991635322571, val_loss: 0.7816570997238159

('adam', 0.1, 8, 100)
train_loss: 0.55611652135849, val_loss: 0.789500892162323

('adam', 0.1, 16, 10)
train_loss: 0.5612054467201233, val_loss: 0.7650130987167358

('adam', 0.1, 16, 50)
train_loss: 0.5546260476112366, val_loss: 0.7896468639373779

('adam', 0.1, 16, 100)
train_loss: 0.5559875965118408, val_loss: 0.7892804741859436

('adam', 0.1, 32, 10)
train_loss: 0.5590546131134033, val_loss: 0.7611736059188843

('adam', 0.1, 32, 50)
train_loss: 0.5556875467300415, val_loss: 0.8080446720123291

('adam', 0.1, 32, 100)
train_loss: 0.5561390519142151, val_loss: 0.7829593420028687

('adam', 0.1, 64, 10)
train_loss: 0.5558593273162842, val_loss: 0.7738059759140015

('adam', 0.1, 64, 50)
train_loss: 0.554695725440979, val_loss: 0.7936584949493408

('adam', 0.1, 64, 100)
train_loss: 0.5543773770332336, val_loss: 0.7920986413955688

('sgd', 0.001, 8, 10)
train_loss: 0.5793512463569641, val_loss: 0.7260117530822754

('sgd', 0.001, 8, 50)
train_loss: 0.5778812170028687, val_loss: 0.7276942729949951

('sgd', 0.001, 8, 100)
train_loss: 0.9435847401618958, val_loss: 0.7197303771972656

('sgd', 0.001, 16, 10)
train_loss: 1.0779635906219482, val_loss: 0.7464697360992432

('sgd', 0.001, 16, 50)
train_loss: 0.5805976390838623, val_loss: 0.7249062657356262

('sgd', 0.001, 16, 100)
train_loss: 0.8257448673248291, val_loss: 0.701138436794281

('sgd', 0.001, 32, 10)
train_loss: 0.9197883009910583, val_loss: 0.7155948877334595

('sgd', 0.001, 32, 50)
train_loss: 0.5594845414161682, val_loss: 0.7615658044815063

('sgd', 0.001, 32, 100)
train_loss: 0.5868043899536133, val_loss: 0.7198136448860168

('sgd', 0.001, 64, 10)
train_loss: 0.6001373529434204, val_loss: 0.7114484310150146

('sgd', 0.001, 64, 50)
train_loss: 0.6753588914871216, val_loss: 0.6940014958381653

('sgd', 0.001, 64, 100)
train_loss: 0.6559015512466431, val_loss: 0.6959431767463684

('sgd', 0.01, 8, 10)
train_loss: 0.9374737739562988, val_loss: 0.7178253531455994

('sgd', 0.01, 8, 50)
train_loss: 0.5867137312889099, val_loss: 0.7259554862976074

('sgd', 0.01, 8, 100)
train_loss: 0.591334342956543, val_loss: 0.7336692810058594

('sgd', 0.01, 16, 10)
train_loss: 0.8706501722335815, val_loss: 0.7069298028945923

('sgd', 0.01, 16, 50)
train_loss: 0.8286402225494385, val_loss: 0.7087098956108093

('sgd', 0.01, 16, 100)
train_loss: 0.6573769450187683, val_loss: 0.7112289667129517

('sgd', 0.01, 32, 10)
train_loss: 1.0253568887710571, val_loss: 0.7343810200691223

('sgd', 0.01, 32, 50)
train_loss: 0.5641670823097229, val_loss: 0.7483978867530823

('sgd', 0.01, 32, 100)
train_loss: 0.6393638253211975, val_loss: 0.7047824263572693

('sgd', 0.01, 64, 10)
train_loss: 0.6251438856124878, val_loss: 0.7020699977874756

('sgd', 0.01, 64, 50)
train_loss: 0.5696334838867188, val_loss: 0.7380544543266296

('sgd', 0.01, 64, 100)
train_loss: 0.6727772951126099, val_loss: 0.6958944201469421

('sgd', 0.1, 8, 10)
train_loss: 0.5745288729667664, val_loss: 0.7412228584289551

('sgd', 0.1, 8, 50)
train_loss: 0.5631556510925293, val_loss: 0.7612813711166382

('sgd', 0.1, 8, 100)
train_loss: 0.5570756196975708, val_loss: 0.7783029675483704

('sgd', 0.1, 16, 10)
train_loss: 0.7458989024162292, val_loss: 0.7197275757789612

('sgd', 0.1, 16, 50)
train_loss: 0.5784754753112793, val_loss: 0.7492064833641052

('sgd', 0.1, 16, 100)
train_loss: 0.5656487345695496, val_loss: 0.7552664279937744

('sgd', 0.1, 32, 10)
train_loss: 0.5669741630554199, val_loss: 0.7451849579811096

('sgd', 0.1, 32, 50)
train_loss: 0.6118236184120178, val_loss: 0.7466168403625488

('sgd', 0.1, 32, 100)
train_loss: 0.5639291405677795, val_loss: 0.7575989365577698

('sgd', 0.1, 64, 10)
train_loss: 0.7021383047103882, val_loss: 0.6948866844177246

('sgd', 0.1, 64, 50)
train_loss: 0.570797860622406, val_loss: 0.7428354024887085

('sgd', 0.1, 64, 100)
train_loss: 0.636228621006012, val_loss: 0.7569096088409424


---------
BEST MODEL
('adam', 0.001, 64, 50)
val_loss: 0.6929594874382019
---------
