
Run from 2023-04-02 11:05:54.310247
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6904768347740173, val_loss: 0.692261815071106

('adam', 0.01, 50, 200)
train_loss: 0.6984800100326538, val_loss: 0.6948879957199097

('adam', 0.01, 50, 300)
train_loss: 0.6953356266021729, val_loss: 0.6939175128936768

('adam', 0.01, 100, 100)
train_loss: 0.6990792155265808, val_loss: 0.6954160332679749

('adam', 0.01, 100, 200)
train_loss: 0.6674822568893433, val_loss: 0.6886624693870544

('adam', 0.01, 100, 300)
train_loss: 0.6595231294631958, val_loss: 0.6881386637687683

('adam', 0.01, 200, 100)
train_loss: 0.6797375082969666, val_loss: 0.6908314228057861

('adam', 0.01, 200, 200)
train_loss: 0.6815693974494934, val_loss: 0.6912972331047058

('adam', 0.01, 200, 300)
train_loss: 0.6767332553863525, val_loss: 0.6903382539749146

('adam', 0.1, 50, 100)
train_loss: 0.6719265580177307, val_loss: 0.7119182348251343

('adam', 0.1, 50, 200)
train_loss: 0.6521478891372681, val_loss: 0.6835601925849915

('adam', 0.1, 50, 300)
train_loss: 0.633003294467926, val_loss: 0.6827515363693237

('adam', 0.1, 100, 100)
train_loss: 0.629243791103363, val_loss: 0.6909298300743103

('adam', 0.1, 100, 200)
train_loss: 0.5990846157073975, val_loss: 0.7143122553825378

('adam', 0.1, 100, 300)
train_loss: 0.588657557964325, val_loss: 0.7415622472763062

('adam', 0.1, 200, 100)
train_loss: 0.6288776993751526, val_loss: 0.6913721561431885

('adam', 0.1, 200, 200)
train_loss: 0.596363365650177, val_loss: 0.7193918228149414

('adam', 0.1, 200, 300)
train_loss: 0.5884805917739868, val_loss: 0.7425435781478882

('adam', 0.2, 50, 100)
train_loss: 0.6632428765296936, val_loss: 0.6898252964019775

('adam', 0.2, 50, 200)
train_loss: 0.6300163269042969, val_loss: 0.690480649471283

('adam', 0.2, 50, 300)
train_loss: 0.6163710951805115, val_loss: 0.6859244108200073

('adam', 0.2, 100, 100)
train_loss: 0.6010781526565552, val_loss: 0.711358904838562

('adam', 0.2, 100, 200)
train_loss: 0.5867663621902466, val_loss: 0.7526019811630249

('adam', 0.2, 100, 300)
train_loss: 0.5852292776107788, val_loss: 0.7742265462875366

('adam', 0.2, 200, 100)
train_loss: 0.6041275262832642, val_loss: 0.7072479724884033

('adam', 0.2, 200, 200)
train_loss: 0.5862106084823608, val_loss: 0.7573113441467285

('adam', 0.2, 200, 300)
train_loss: 0.5853135585784912, val_loss: 0.770820140838623


---------
BEST MODEL
('adam', 0.1, 50, 300)
val_loss: 0.6827515363693237
---------

Run from 2023-04-05 15:44:25.983821
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6947352886199951, val_loss: 0.6941676139831543

('adam', 0.001, 8, 50)
train_loss: 0.6967986822128296, val_loss: 0.6951876878738403

('adam', 0.001, 8, 100)
train_loss: 0.7049410939216614, val_loss: 0.6986233592033386

('adam', 0.001, 16, 10)
train_loss: 0.6918162107467651, val_loss: 0.6928545832633972

('adam', 0.001, 16, 50)
train_loss: 0.6902779936790466, val_loss: 0.6934695243835449

('adam', 0.001, 16, 100)
train_loss: 0.6863840818405151, val_loss: 0.6911537051200867

('adam', 0.001, 32, 10)
train_loss: 0.6926907300949097, val_loss: 0.6929537057876587

('adam', 0.001, 32, 50)
train_loss: 0.7019802927970886, val_loss: 0.6985008716583252

('adam', 0.001, 32, 100)
train_loss: 0.6875778436660767, val_loss: 0.6914472579956055

('adam', 0.001, 64, 10)
train_loss: 0.69556725025177, val_loss: 0.696522057056427

('adam', 0.001, 64, 50)
train_loss: 0.6912099123001099, val_loss: 0.6925612688064575

('adam', 0.001, 64, 100)
train_loss: 0.6887965798377991, val_loss: 0.6916689872741699

('adam', 0.01, 8, 10)
train_loss: 0.7007653117179871, val_loss: 0.6960335969924927

('adam', 0.01, 8, 50)
train_loss: 0.7010629177093506, val_loss: 0.6956902742385864

('adam', 0.01, 8, 100)
train_loss: 0.6918925642967224, val_loss: 0.6930391788482666

('adam', 0.01, 16, 10)
train_loss: 0.7030764222145081, val_loss: 0.6969843506813049

('adam', 0.01, 16, 50)
train_loss: 0.68351811170578, val_loss: 0.6902292966842651

('adam', 0.01, 16, 100)
train_loss: 0.6958103775978088, val_loss: 0.694328248500824

('adam', 0.01, 32, 10)
train_loss: 0.7097791433334351, val_loss: 0.7018649578094482

('adam', 0.01, 32, 50)
train_loss: 0.6869331002235413, val_loss: 0.6918191909790039

('adam', 0.01, 32, 100)
train_loss: 0.6815620064735413, val_loss: 0.6906518936157227

('adam', 0.01, 64, 10)
train_loss: 0.6900684833526611, val_loss: 0.6927083730697632

('adam', 0.01, 64, 50)
train_loss: 0.6916064023971558, val_loss: 0.6936254501342773

('adam', 0.01, 64, 100)
train_loss: 0.6981277465820312, val_loss: 0.6951507329940796


Run from 2023-04-05 16:43:59.488528
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6977915167808533, val_loss: 0.698961615562439

('adam', 0.001, 8, 50)
train_loss: 0.7129921317100525, val_loss: 0.7049181461334229

('adam', 0.001, 8, 100)
train_loss: 0.6824528574943542, val_loss: 0.6898015737533569

('adam', 0.001, 16, 10)
train_loss: 0.6933444142341614, val_loss: 0.6933045983314514

('adam', 0.001, 16, 50)
train_loss: 0.7036211490631104, val_loss: 0.6993557214736938

('adam', 0.001, 16, 100)
train_loss: 0.6849091053009033, val_loss: 0.690568745136261

('adam', 0.001, 32, 10)
train_loss: 0.6974121928215027, val_loss: 0.6983295679092407

('adam', 0.001, 32, 50)
train_loss: 0.6910505294799805, val_loss: 0.6923949718475342

('adam', 0.001, 32, 100)
train_loss: 0.6890299916267395, val_loss: 0.6918302774429321

('adam', 0.001, 64, 10)
train_loss: 0.6919055581092834, val_loss: 0.6927326321601868

('adam', 0.001, 64, 50)
train_loss: 0.7088238596916199, val_loss: 0.7032530307769775

('adam', 0.001, 64, 100)
train_loss: 0.7150091528892517, val_loss: 0.7067707777023315

('adam', 0.01, 8, 10)
train_loss: 0.682130753993988, val_loss: 0.68950355052948

('adam', 0.01, 8, 50)
train_loss: 0.6965091228485107, val_loss: 0.6945838928222656

('adam', 0.01, 8, 100)
train_loss: 0.687750518321991, val_loss: 0.6919909715652466

('adam', 0.01, 16, 10)
train_loss: 0.6908597946166992, val_loss: 0.6927893161773682

('adam', 0.01, 16, 50)
train_loss: 0.6766122579574585, val_loss: 0.691474199295044

('adam', 0.01, 16, 100)
train_loss: 0.6760530471801758, val_loss: 0.6887615919113159

('adam', 0.01, 32, 10)
train_loss: 0.6889016032218933, val_loss: 0.6915205717086792

('adam', 0.01, 32, 50)
train_loss: 0.6833110451698303, val_loss: 0.6910475492477417

('adam', 0.01, 32, 100)
train_loss: 0.6905897855758667, val_loss: 0.6926642060279846

('adam', 0.01, 64, 10)
train_loss: 0.6920996904373169, val_loss: 0.694403886795044

('adam', 0.01, 64, 50)
train_loss: 0.6845460534095764, val_loss: 0.6916326880455017

('adam', 0.01, 64, 100)
train_loss: 0.694847047328949, val_loss: 0.6943198442459106

('adam', 0.1, 8, 10)
train_loss: 0.6737032532691956, val_loss: 0.6917401552200317

('adam', 0.1, 8, 50)
train_loss: 0.6522923707962036, val_loss: 0.6872459053993225

('adam', 0.1, 8, 100)
train_loss: 0.6173718571662903, val_loss: 0.7039055824279785

('adam', 0.1, 16, 10)
train_loss: 0.6756471991539001, val_loss: 0.6880687475204468

('adam', 0.1, 16, 50)
train_loss: 0.6536172032356262, val_loss: 0.6856725215911865

('adam', 0.1, 16, 100)
train_loss: 0.6337335109710693, val_loss: 0.6857307553291321

('adam', 0.1, 32, 10)
train_loss: 0.6791397333145142, val_loss: 0.6894264221191406

('adam', 0.1, 32, 50)
train_loss: 0.6629116535186768, val_loss: 0.6895719170570374

('adam', 0.1, 32, 100)
train_loss: 0.6325617432594299, val_loss: 0.6890988945960999

('adam', 0.1, 64, 10)
train_loss: 0.6826027631759644, val_loss: 0.6977953314781189

('adam', 0.1, 64, 50)
train_loss: 0.6583382487297058, val_loss: 0.6882296800613403

('adam', 0.1, 64, 100)
train_loss: 0.6308911442756653, val_loss: 0.6902303695678711


---------
BEST MODEL
('adam', 0.1, 16, 50)
val_loss: 0.6856725215911865
---------

Run from 2023-04-10 10:28:07.518773
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7198767066001892, val_loss: 0.7122851610183716

('adam', 0.001, 8, 50)
train_loss: 0.6934666037559509, val_loss: 0.6936082243919373

('adam', 0.001, 8, 100)
train_loss: 0.7032346129417419, val_loss: 0.6979727745056152

('adam', 0.001, 16, 10)
train_loss: 0.6937199831008911, val_loss: 0.6948586106300354

('adam', 0.001, 16, 50)
train_loss: 0.6974921822547913, val_loss: 0.6958370208740234

('adam', 0.001, 16, 100)
train_loss: 0.7071602940559387, val_loss: 0.700157880783081

('adam', 0.001, 32, 10)
train_loss: 0.695036768913269, val_loss: 0.6961188316345215

('adam', 0.001, 32, 50)
train_loss: 0.6899662017822266, val_loss: 0.6923198103904724

('adam', 0.001, 32, 100)
train_loss: 0.6959359645843506, val_loss: 0.6947424411773682

('adam', 0.001, 64, 10)
train_loss: 0.6975367665290833, val_loss: 0.6958845853805542

('adam', 0.001, 64, 50)
train_loss: 0.7164067029953003, val_loss: 0.7089020013809204

('adam', 0.001, 64, 100)
train_loss: 0.700596809387207, val_loss: 0.6967313289642334

('adam', 0.01, 8, 10)
train_loss: 0.6838204264640808, val_loss: 0.6901438236236572

('adam', 0.01, 8, 50)
train_loss: 0.6808605790138245, val_loss: 0.6906871795654297

('adam', 0.01, 8, 100)
train_loss: 0.6797350645065308, val_loss: 0.6905224323272705

('adam', 0.01, 16, 10)
train_loss: 0.6857345104217529, val_loss: 0.6907044053077698

('adam', 0.01, 16, 50)
train_loss: 0.6876007914543152, val_loss: 0.6931285858154297

('adam', 0.01, 16, 100)
train_loss: 0.673705518245697, val_loss: 0.688435435295105

('adam', 0.01, 32, 10)
train_loss: 0.7036019563674927, val_loss: 0.6978718042373657

('adam', 0.01, 32, 50)
train_loss: 0.6977989673614502, val_loss: 0.6947891712188721

('adam', 0.01, 32, 100)
train_loss: 0.6766381859779358, val_loss: 0.6898617148399353

('adam', 0.01, 64, 10)
train_loss: 0.6896935701370239, val_loss: 0.6923922896385193

('adam', 0.01, 64, 50)
train_loss: 0.6827574968338013, val_loss: 0.6911067962646484

('adam', 0.01, 64, 100)
train_loss: 0.688053548336029, val_loss: 0.6927480697631836

('adam', 0.1, 8, 10)
train_loss: 0.6792778372764587, val_loss: 0.689917802810669

('adam', 0.1, 8, 50)
train_loss: 0.6563049554824829, val_loss: 0.6895112991333008

('adam', 0.1, 8, 100)
train_loss: 0.6255374550819397, val_loss: 0.6915097236633301

('adam', 0.1, 16, 10)
train_loss: 0.6800210475921631, val_loss: 0.6899197697639465

('adam', 0.1, 16, 50)
train_loss: 0.654053807258606, val_loss: 0.685084879398346

('adam', 0.1, 16, 100)
train_loss: 0.6346641778945923, val_loss: 0.6972950100898743

('adam', 0.1, 32, 10)
train_loss: 0.6837401986122131, val_loss: 0.6918528079986572

('adam', 0.1, 32, 50)
train_loss: 0.6637877225875854, val_loss: 0.6874178051948547

('adam', 0.1, 32, 100)
train_loss: 0.6357421875, val_loss: 0.6851150989532471

('adam', 0.1, 64, 10)
train_loss: 0.687205970287323, val_loss: 0.6914808750152588

('adam', 0.1, 64, 50)
train_loss: 0.6547774076461792, val_loss: 0.6880091428756714

('adam', 0.1, 64, 100)
train_loss: 0.6298850774765015, val_loss: 0.6911070346832275


---------
BEST MODEL
('adam', 0.1, 16, 50)
val_loss: 0.685084879398346
---------

Run from 2023-04-10 13:19:43.125117
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7163105607032776, val_loss: 0.7485995292663574

('adam', 0.001, 8, 50)
train_loss: 0.6963627934455872, val_loss: 0.6795196533203125

('adam', 0.001, 8, 100)
train_loss: 0.6938164830207825, val_loss: 0.7023203372955322

('adam', 0.001, 16, 10)
train_loss: 0.7046422362327576, val_loss: 0.7281763553619385

('adam', 0.001, 16, 50)
train_loss: 0.6934093236923218, val_loss: 0.6901915073394775

('adam', 0.001, 16, 100)
train_loss: 0.6983219981193542, val_loss: 0.7197874784469604

('adam', 0.001, 32, 10)
train_loss: 0.6944261789321899, val_loss: 0.7044880390167236

('adam', 0.001, 32, 50)
train_loss: 0.7182150483131409, val_loss: 0.6858265399932861

('adam', 0.001, 32, 100)
train_loss: 0.6958195567131042, val_loss: 0.7109743356704712

('adam', 0.001, 64, 10)
train_loss: 0.751489520072937, val_loss: 0.7069565653800964

('adam', 0.001, 64, 50)
train_loss: 0.7274177074432373, val_loss: 0.7670853137969971

('adam', 0.001, 64, 100)
train_loss: 0.7048758268356323, val_loss: 0.7316455841064453

('adam', 0.01, 8, 10)
train_loss: 0.6967090368270874, val_loss: 0.7148572206497192

('adam', 0.01, 8, 50)
train_loss: 0.693390429019928, val_loss: 0.6989197731018066

('adam', 0.01, 8, 100)
train_loss: 0.6907155513763428, val_loss: 0.680747389793396

('adam', 0.01, 16, 10)
train_loss: 0.6977851390838623, val_loss: 0.719889760017395

('adam', 0.01, 16, 50)
train_loss: 0.6899358034133911, val_loss: 0.6767109632492065

('adam', 0.01, 16, 100)
train_loss: 0.6932255625724792, val_loss: 0.6978089809417725

('adam', 0.01, 32, 10)
train_loss: 0.6921664476394653, val_loss: 0.6849205493927002

('adam', 0.01, 32, 50)
train_loss: 0.6923838257789612, val_loss: 0.6936154365539551

('adam', 0.01, 32, 100)
train_loss: 0.6904429793357849, val_loss: 0.6808636784553528

('adam', 0.01, 64, 10)
train_loss: 0.697076141834259, val_loss: 0.7138288021087646

('adam', 0.01, 64, 50)
train_loss: 0.6912825107574463, val_loss: 0.6858755350112915

('adam', 0.01, 64, 100)
train_loss: 0.689116358757019, val_loss: 0.6708090305328369

('adam', 0.1, 8, 10)
train_loss: 0.6897223591804504, val_loss: 0.6683734655380249

('adam', 0.1, 8, 50)
train_loss: 0.6879276633262634, val_loss: 0.6502457857131958

('adam', 0.1, 8, 100)
train_loss: 0.6895267963409424, val_loss: 0.6564375162124634

('adam', 0.1, 16, 10)
train_loss: 0.6987125873565674, val_loss: 0.7221630215644836

('adam', 0.1, 16, 50)
train_loss: 0.6882040500640869, val_loss: 0.6593883037567139

('adam', 0.1, 16, 100)
train_loss: 0.6901635527610779, val_loss: 0.6495263576507568

('adam', 0.1, 32, 10)
train_loss: 0.6942104697227478, val_loss: 0.6994354128837585

('adam', 0.1, 32, 50)
train_loss: 0.691623330116272, val_loss: 0.6890411376953125

('adam', 0.1, 32, 100)
train_loss: 0.6896069049835205, val_loss: 0.672934889793396

('adam', 0.1, 64, 10)
train_loss: 0.6987128853797913, val_loss: 0.6911380290985107

('adam', 0.1, 64, 50)
train_loss: 0.6935333609580994, val_loss: 0.6983829736709595

('adam', 0.1, 64, 100)
train_loss: 0.6893996596336365, val_loss: 0.6724121570587158


---------
BEST MODEL
('adam', 0.1, 16, 100)
val_loss: 0.6495263576507568
---------

Run from 2023-04-10 14:47:48.812633
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7438097596168518, val_loss: 0.7691056132316589

('adam', 0.001, 8, 50)
train_loss: 0.7059295177459717, val_loss: 0.6834046244621277

('adam', 0.001, 8, 100)
train_loss: 0.6935797929763794, val_loss: 0.6976966857910156

('adam', 0.001, 16, 10)
train_loss: 0.749618411064148, val_loss: 0.7760317325592041

('adam', 0.001, 16, 50)
train_loss: 0.6942303776741028, val_loss: 0.6875151991844177

('adam', 0.001, 16, 100)
train_loss: 0.6928150653839111, val_loss: 0.6935566067695618

('adam', 0.001, 32, 10)
train_loss: 0.6965402364730835, val_loss: 0.7053497433662415

('adam', 0.001, 32, 50)
train_loss: 0.6938713788986206, val_loss: 0.6884075403213501

('adam', 0.001, 32, 100)
train_loss: 0.7024551630020142, val_loss: 0.6839525103569031

('adam', 0.001, 64, 10)
train_loss: 0.7350114583969116, val_loss: 0.7585047483444214

('adam', 0.001, 64, 50)
train_loss: 0.7170775532722473, val_loss: 0.6935596466064453

('adam', 0.001, 64, 100)
train_loss: 0.7120491862297058, val_loss: 0.7307720184326172

('adam', 0.01, 8, 10)
train_loss: 0.6974642872810364, val_loss: 0.672008752822876

('adam', 0.01, 8, 50)
train_loss: 0.6962331533432007, val_loss: 0.703311026096344

('adam', 0.01, 8, 100)
train_loss: 0.6908171772956848, val_loss: 0.6860141158103943

('adam', 0.01, 16, 10)
train_loss: 0.6955884099006653, val_loss: 0.7024597525596619

('adam', 0.01, 16, 50)
train_loss: 0.6900417804718018, val_loss: 0.6858804225921631

('adam', 0.01, 16, 100)
train_loss: 0.6892094016075134, val_loss: 0.6822149753570557

('adam', 0.01, 32, 10)
train_loss: 0.717533528804779, val_loss: 0.6878796815872192

('adam', 0.01, 32, 50)
train_loss: 0.6964191794395447, val_loss: 0.7067554593086243

('adam', 0.01, 32, 100)
train_loss: 0.692133367061615, val_loss: 0.6929099559783936

('adam', 0.01, 64, 10)
train_loss: 0.7362450361251831, val_loss: 0.7004235982894897

('adam', 0.01, 64, 50)
train_loss: 0.6905142068862915, val_loss: 0.6873425245285034

('adam', 0.01, 64, 100)
train_loss: 0.689471960067749, val_loss: 0.6833361387252808

('adam', 0.1, 8, 10)
train_loss: 0.689460039138794, val_loss: 0.6758130788803101

('adam', 0.1, 8, 50)
train_loss: 0.7060856223106384, val_loss: 0.6855126619338989

('adam', 0.1, 8, 100)
train_loss: 0.6918017268180847, val_loss: 0.6617111563682556

('adam', 0.1, 16, 10)
train_loss: 0.6898922324180603, val_loss: 0.6920008659362793

('adam', 0.1, 16, 50)
train_loss: 0.6882781982421875, val_loss: 0.6720895767211914

('adam', 0.1, 16, 100)
train_loss: 0.6852160096168518, val_loss: 0.6615515947341919

('adam', 0.1, 32, 10)
train_loss: 0.6991647481918335, val_loss: 0.7106768488883972

('adam', 0.1, 32, 50)
train_loss: 0.6861116886138916, val_loss: 0.6692367792129517

('adam', 0.1, 32, 100)
train_loss: 0.684615969657898, val_loss: 0.6611772179603577

('adam', 0.1, 64, 10)
train_loss: 0.6955100297927856, val_loss: 0.6961458325386047

('adam', 0.1, 64, 50)
train_loss: 0.6864475607872009, val_loss: 0.6692672967910767

('adam', 0.1, 64, 100)
train_loss: 0.6838402152061462, val_loss: 0.6594989895820618


---------
BEST MODEL
('adam', 0.1, 64, 100)
val_loss: 0.6594989895820618
---------

Run from 2023-04-17 10:05:11.673319
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6951322555541992, val_loss: 0.6879673004150391

('adam', 0.001, 8, 50)
train_loss: 0.7126856446266174, val_loss: 0.6811287999153137

('adam', 0.001, 8, 100)
train_loss: 0.6918413639068604, val_loss: 0.6896598935127258

('adam', 0.001, 16, 10)
train_loss: 0.7423057556152344, val_loss: 0.7687423229217529

('adam', 0.001, 16, 50)
train_loss: 0.6955434083938599, val_loss: 0.683986485004425

('adam', 0.001, 16, 100)
train_loss: 0.70263671875, val_loss: 0.7192192077636719

('adam', 0.001, 32, 10)
train_loss: 0.7236438989639282, val_loss: 0.6965367794036865

('adam', 0.001, 32, 50)
train_loss: 0.7149349451065063, val_loss: 0.6895003318786621

('adam', 0.001, 32, 100)
train_loss: 0.692623496055603, val_loss: 0.6842959523200989

('adam', 0.001, 64, 10)
train_loss: 0.7449038028717041, val_loss: 0.7718709111213684

('adam', 0.001, 64, 50)
train_loss: 0.7413843870162964, val_loss: 0.7051193118095398

('adam', 0.001, 64, 100)
train_loss: 0.6928552985191345, val_loss: 0.6861656308174133

('adam', 0.01, 8, 10)
train_loss: 0.6908623576164246, val_loss: 0.6851530075073242

('adam', 0.01, 8, 50)
train_loss: 0.6879425644874573, val_loss: 0.6749923825263977

('adam', 0.01, 8, 100)
train_loss: 0.6933680176734924, val_loss: 0.6947194933891296

('adam', 0.01, 16, 10)
train_loss: 0.6962105631828308, val_loss: 0.7050307393074036

('adam', 0.01, 16, 50)
train_loss: 0.6861099004745483, val_loss: 0.671931803226471

('adam', 0.01, 16, 100)
train_loss: 0.6989353895187378, val_loss: 0.712838351726532

('adam', 0.01, 32, 10)
train_loss: 0.6914995312690735, val_loss: 0.6877229809761047

('adam', 0.01, 32, 50)
train_loss: 0.6882286071777344, val_loss: 0.6788117289543152

('adam', 0.01, 32, 100)
train_loss: 0.6968552470207214, val_loss: 0.7067694664001465

('adam', 0.01, 64, 10)
train_loss: 0.7142614722251892, val_loss: 0.6880119442939758

('adam', 0.01, 64, 50)
train_loss: 0.6944540739059448, val_loss: 0.6994213461875916

('adam', 0.01, 64, 100)
train_loss: 0.6939998269081116, val_loss: 0.6977488398551941

('adam', 0.1, 8, 10)
train_loss: 0.6879069209098816, val_loss: 0.6711478233337402

('adam', 0.1, 8, 50)
train_loss: 0.6876712441444397, val_loss: 0.6624464988708496

('adam', 0.1, 8, 100)
train_loss: 0.6756182312965393, val_loss: 0.6354982256889343

('adam', 0.1, 16, 10)
train_loss: 0.6865471005439758, val_loss: 0.6730334162712097

('adam', 0.1, 16, 50)
train_loss: 0.6878091096878052, val_loss: 0.6757362484931946

('adam', 0.1, 16, 100)
train_loss: 0.6787392497062683, val_loss: 0.6402772068977356

('adam', 0.1, 32, 10)
train_loss: 0.6987616419792175, val_loss: 0.7113731503486633

('adam', 0.1, 32, 50)
train_loss: 0.6865522861480713, val_loss: 0.6713390350341797

('adam', 0.1, 32, 100)
train_loss: 0.6781336665153503, val_loss: 0.6406065225601196

('adam', 0.1, 64, 10)
train_loss: 0.6949037313461304, val_loss: 0.701973021030426

('adam', 0.1, 64, 50)
train_loss: 0.6898784637451172, val_loss: 0.683899462223053

('adam', 0.1, 64, 100)
train_loss: 0.6790756583213806, val_loss: 0.645664632320404

('sgd', 0.001, 8, 10)
train_loss: 0.6959446668624878, val_loss: 0.7033204436302185

('sgd', 0.001, 8, 50)
train_loss: 0.746999204158783, val_loss: 0.7755823135375977

('sgd', 0.001, 8, 100)
train_loss: 0.7062802910804749, val_loss: 0.6860688328742981

('sgd', 0.001, 16, 10)
train_loss: 0.7750122547149658, val_loss: 0.7282162308692932

('sgd', 0.001, 16, 50)
train_loss: 0.7683232426643372, val_loss: 0.7223984599113464

('sgd', 0.001, 16, 100)
train_loss: 0.7025283575057983, val_loss: 0.6873448491096497

('sgd', 0.001, 32, 10)
train_loss: 0.7010061144828796, val_loss: 0.7128337025642395

('sgd', 0.001, 32, 50)
train_loss: 0.6988517642021179, val_loss: 0.6881726384162903

('sgd', 0.001, 32, 100)
train_loss: 0.6933213472366333, val_loss: 0.6956308484077454

('sgd', 0.001, 64, 10)
train_loss: 0.7716387510299683, val_loss: 0.8036807179450989

('sgd', 0.001, 64, 50)
train_loss: 0.7075948119163513, val_loss: 0.6901343464851379

('sgd', 0.001, 64, 100)
train_loss: 0.6961394548416138, val_loss: 0.7038083672523499

('sgd', 0.01, 8, 10)
train_loss: 0.6987667083740234, val_loss: 0.6857059001922607

('sgd', 0.01, 8, 50)
train_loss: 0.6988239288330078, val_loss: 0.7114272713661194

('sgd', 0.01, 8, 100)
train_loss: 0.6948277354240417, val_loss: 0.6998436450958252

('sgd', 0.01, 16, 10)
train_loss: 0.7045981287956238, val_loss: 0.718773365020752

('sgd', 0.01, 16, 50)
train_loss: 0.7218984961509705, val_loss: 0.7479755282402039

('sgd', 0.01, 16, 100)
train_loss: 0.6918489933013916, val_loss: 0.6891360282897949

('sgd', 0.01, 32, 10)
train_loss: 0.698330819606781, val_loss: 0.6878664493560791

('sgd', 0.01, 32, 50)
train_loss: 0.7046263217926025, val_loss: 0.7198914885520935

('sgd', 0.01, 32, 100)
train_loss: 0.695084273815155, val_loss: 0.6812019348144531

('sgd', 0.01, 64, 10)
train_loss: 0.7638294100761414, val_loss: 0.7201969027519226

('sgd', 0.01, 64, 50)
train_loss: 0.7354984879493713, val_loss: 0.7001721262931824

('sgd', 0.01, 64, 100)
train_loss: 0.7174547910690308, val_loss: 0.7391467690467834

('sgd', 0.1, 8, 10)
train_loss: 0.6900309324264526, val_loss: 0.6747695803642273

('sgd', 0.1, 8, 50)
train_loss: 0.6903483867645264, val_loss: 0.6808739304542542

('sgd', 0.1, 8, 100)
train_loss: 0.6890200972557068, val_loss: 0.6740705370903015

('sgd', 0.1, 16, 10)
train_loss: 0.7113454341888428, val_loss: 0.7354876399040222

('sgd', 0.1, 16, 50)
train_loss: 0.6973646879196167, val_loss: 0.7052533030509949

('sgd', 0.1, 16, 100)
train_loss: 0.6889816522598267, val_loss: 0.6815854907035828

('sgd', 0.1, 32, 10)
train_loss: 0.7171619534492493, val_loss: 0.7395320534706116

('sgd', 0.1, 32, 50)
train_loss: 0.6960699558258057, val_loss: 0.7040255665779114

('sgd', 0.1, 32, 100)
train_loss: 0.6847447156906128, val_loss: 0.6670107841491699

('sgd', 0.1, 64, 10)
train_loss: 0.6947991847991943, val_loss: 0.7006630301475525

('sgd', 0.1, 64, 50)
train_loss: 0.6906277537345886, val_loss: 0.6848104596138

('sgd', 0.1, 64, 100)
train_loss: 0.6887599229812622, val_loss: 0.6804604530334473


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.6354982256889343
---------

Run from 2023-04-17 16:58:36.388815
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7024329900741577, val_loss: 0.7074575424194336

('adam', 0.001, 8, 50)
train_loss: 0.6917918920516968, val_loss: 0.7003743648529053

('adam', 0.001, 8, 100)
train_loss: 0.6851765513420105, val_loss: 0.6969708204269409

('adam', 0.001, 16, 10)
train_loss: 0.743664026260376, val_loss: 0.7497672438621521

('adam', 0.001, 16, 50)
train_loss: 0.7213936448097229, val_loss: 0.7052894830703735

('adam', 0.001, 16, 100)
train_loss: 0.7018724083900452, val_loss: 0.6927721500396729

('adam', 0.001, 32, 10)
train_loss: 0.7051297426223755, val_loss: 0.6983146667480469

('adam', 0.001, 32, 50)
train_loss: 0.6920604109764099, val_loss: 0.6963306665420532

('adam', 0.001, 32, 100)
train_loss: 0.6996772885322571, val_loss: 0.6931108832359314

('adam', 0.001, 64, 10)
train_loss: 0.693629264831543, val_loss: 0.6930707097053528

('adam', 0.001, 64, 50)
train_loss: 0.745216965675354, val_loss: 0.7261123061180115

('adam', 0.001, 64, 100)
train_loss: 0.7115706205368042, val_loss: 0.7193384170532227

('adam', 0.01, 8, 10)
train_loss: 0.6841537952423096, val_loss: 0.6981068253517151

('adam', 0.01, 8, 50)
train_loss: 0.676981508731842, val_loss: 0.6998510360717773

('adam', 0.01, 8, 100)
train_loss: 0.6756483912467957, val_loss: 0.700312614440918

('adam', 0.01, 16, 10)
train_loss: 0.6915960907936096, val_loss: 0.6937647461891174

('adam', 0.01, 16, 50)
train_loss: 0.6964594721794128, val_loss: 0.6920462846755981

('adam', 0.01, 16, 100)
train_loss: 0.6965954899787903, val_loss: 0.6919863820075989

('adam', 0.01, 32, 10)
train_loss: 0.694300651550293, val_loss: 0.6927565336227417

('adam', 0.01, 32, 50)
train_loss: 0.6853915452957153, val_loss: 0.6959652304649353

('adam', 0.01, 32, 100)
train_loss: 0.6935381293296814, val_loss: 0.6930716037750244

('adam', 0.01, 64, 10)
train_loss: 0.6998996138572693, val_loss: 0.705844521522522

('adam', 0.01, 64, 50)
train_loss: 0.6837835907936096, val_loss: 0.6965426206588745

('adam', 0.01, 64, 100)
train_loss: 0.6813300848007202, val_loss: 0.6976314783096313

('adam', 0.1, 8, 10)
train_loss: 0.6777045726776123, val_loss: 0.70066237449646

('adam', 0.1, 8, 50)
train_loss: 0.6558593511581421, val_loss: 0.71841961145401

('adam', 0.1, 8, 100)
train_loss: 0.6439233422279358, val_loss: 0.7253789901733398

('adam', 0.1, 16, 10)
train_loss: 0.6771345734596252, val_loss: 0.6997131705284119

('adam', 0.1, 16, 50)
train_loss: 0.6610410213470459, val_loss: 0.7078543901443481

('adam', 0.1, 16, 100)
train_loss: 0.6519867181777954, val_loss: 0.7204980254173279

('adam', 0.1, 32, 10)
train_loss: 0.6992146968841553, val_loss: 0.6921194195747375

('adam', 0.1, 32, 50)
train_loss: 0.6749159693717957, val_loss: 0.700862467288971

('adam', 0.1, 32, 100)
train_loss: 0.6603356599807739, val_loss: 0.7098853588104248

('adam', 0.1, 64, 10)
train_loss: 0.6860277652740479, val_loss: 0.7127954959869385

('adam', 0.1, 64, 50)
train_loss: 0.6700576543807983, val_loss: 0.7026870250701904

('adam', 0.1, 64, 100)
train_loss: 0.6368013620376587, val_loss: 0.732806921005249

('sgd', 0.001, 8, 10)
train_loss: 0.6996952891349792, val_loss: 0.6954236030578613

('sgd', 0.001, 8, 50)
train_loss: 0.7338465452194214, val_loss: 0.741206169128418

('sgd', 0.001, 8, 100)
train_loss: 0.758994996547699, val_loss: 0.7336125373840332

('sgd', 0.001, 16, 10)
train_loss: 0.7015281915664673, val_loss: 0.6965535879135132

('sgd', 0.001, 16, 50)
train_loss: 0.7356724143028259, val_loss: 0.7199732065200806

('sgd', 0.001, 16, 100)
train_loss: 0.6943120956420898, val_loss: 0.697729229927063

('sgd', 0.001, 32, 10)
train_loss: 0.7044476866722107, val_loss: 0.7090823650360107

('sgd', 0.001, 32, 50)
train_loss: 0.6928961277008057, val_loss: 0.6939291954040527

('sgd', 0.001, 32, 100)
train_loss: 0.727573573589325, val_loss: 0.7140325903892517

('sgd', 0.001, 64, 10)
train_loss: 0.7742249965667725, val_loss: 0.7507985234260559

('sgd', 0.001, 64, 50)
train_loss: 0.7398343682289124, val_loss: 0.745819091796875

('sgd', 0.001, 64, 100)
train_loss: 0.7009397149085999, val_loss: 0.7053748369216919

('sgd', 0.01, 8, 10)
train_loss: 0.696819543838501, val_loss: 0.6936349868774414

('sgd', 0.01, 8, 50)
train_loss: 0.6914175152778625, val_loss: 0.6937245726585388

('sgd', 0.01, 8, 100)
train_loss: 0.6829755306243896, val_loss: 0.6968706250190735

('sgd', 0.01, 16, 10)
train_loss: 0.6941829323768616, val_loss: 0.6972714066505432

('sgd', 0.01, 16, 50)
train_loss: 0.6908313632011414, val_loss: 0.6945660710334778

('sgd', 0.01, 16, 100)
train_loss: 0.6879520416259766, val_loss: 0.695441722869873

('sgd', 0.01, 32, 10)
train_loss: 0.7172892093658447, val_loss: 0.7065736055374146

('sgd', 0.01, 32, 50)
train_loss: 0.6980672478675842, val_loss: 0.705024778842926

('sgd', 0.01, 32, 100)
train_loss: 0.6907445192337036, val_loss: 0.6976318955421448

('sgd', 0.01, 64, 10)
train_loss: 0.7430003881454468, val_loss: 0.7258249521255493

('sgd', 0.01, 64, 50)
train_loss: 0.7361074090003967, val_loss: 0.7187896966934204

('sgd', 0.01, 64, 100)
train_loss: 0.6938878297805786, val_loss: 0.6930224299430847

('sgd', 0.1, 8, 10)
train_loss: 0.6818211078643799, val_loss: 0.6993192434310913

('sgd', 0.1, 8, 50)
train_loss: 0.6785269379615784, val_loss: 0.700308084487915

('sgd', 0.1, 8, 100)
train_loss: 0.6803933382034302, val_loss: 0.6986614465713501

('sgd', 0.1, 16, 10)
train_loss: 0.685465395450592, val_loss: 0.6998993754386902

('sgd', 0.1, 16, 50)
train_loss: 0.6874632239341736, val_loss: 0.6955670714378357

('sgd', 0.1, 16, 100)
train_loss: 0.6891788244247437, val_loss: 0.6947942972183228

('sgd', 0.1, 32, 10)
train_loss: 0.6912143230438232, val_loss: 0.6969337463378906

('sgd', 0.1, 32, 50)
train_loss: 0.6876533627510071, val_loss: 0.6951389312744141

('sgd', 0.1, 32, 100)
train_loss: 0.6978391408920288, val_loss: 0.6916954517364502

('sgd', 0.1, 64, 10)
train_loss: 0.692184567451477, val_loss: 0.6940465569496155

('sgd', 0.1, 64, 50)
train_loss: 0.7111679911613464, val_loss: 0.6911085844039917

('sgd', 0.1, 64, 100)
train_loss: 0.7104664444923401, val_loss: 0.6879921555519104


---------
BEST MODEL
('sgd', 0.1, 64, 100)
val_loss: 0.6879921555519104
---------
