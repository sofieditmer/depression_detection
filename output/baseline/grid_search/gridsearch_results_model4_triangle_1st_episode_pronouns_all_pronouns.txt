
Run from 2023-04-02 10:53:43.282903
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6933166980743408, val_loss: 0.6947740912437439

('adam', 0.01, 50, 200)
train_loss: 0.6899277567863464, val_loss: 0.6918802261352539

('adam', 0.01, 50, 300)
train_loss: 0.6792169213294983, val_loss: 0.68589186668396

('adam', 0.01, 100, 100)
train_loss: 0.691505491733551, val_loss: 0.6931798458099365

('adam', 0.01, 100, 200)
train_loss: 0.6912267208099365, val_loss: 0.6929477453231812

('adam', 0.01, 100, 300)
train_loss: 0.6770294308662415, val_loss: 0.6855117082595825

('adam', 0.01, 200, 100)
train_loss: 0.6909951567649841, val_loss: 0.6927489042282104

('adam', 0.01, 200, 200)
train_loss: 0.688906192779541, val_loss: 0.6910826563835144

('adam', 0.01, 200, 300)
train_loss: 0.6797498464584351, val_loss: 0.6860389709472656

('adam', 0.1, 50, 100)
train_loss: 0.6685663461685181, val_loss: 0.689258873462677

('adam', 0.1, 50, 200)
train_loss: 0.6629936099052429, val_loss: 0.7049592137336731

('adam', 0.1, 50, 300)
train_loss: 0.6623293161392212, val_loss: 0.7147183418273926

('adam', 0.1, 100, 100)
train_loss: 0.6695801019668579, val_loss: 0.6881798505783081

('adam', 0.1, 100, 200)
train_loss: 0.6635450720787048, val_loss: 0.7015194892883301

('adam', 0.1, 100, 300)
train_loss: 0.6624019145965576, val_loss: 0.7120711803436279

('adam', 0.1, 200, 100)
train_loss: 0.6674917340278625, val_loss: 0.6907122135162354

('adam', 0.1, 200, 200)
train_loss: 0.6626845598220825, val_loss: 0.7077503204345703

('adam', 0.1, 200, 300)
train_loss: 0.6623638272285461, val_loss: 0.7131557464599609

('adam', 0.2, 50, 100)
train_loss: 0.6628151535987854, val_loss: 0.7065827250480652

('adam', 0.2, 50, 200)
train_loss: 0.6623201966285706, val_loss: 0.715580403804779

('adam', 0.2, 50, 300)
train_loss: 0.6623175144195557, val_loss: 0.7163357138633728

('adam', 0.2, 100, 100)
train_loss: 0.6624894738197327, val_loss: 0.7104518413543701

('adam', 0.2, 100, 200)
train_loss: 0.6623175740242004, val_loss: 0.7162244319915771

('adam', 0.2, 100, 300)
train_loss: 0.6623175740242004, val_loss: 0.7163355350494385

('adam', 0.2, 200, 100)
train_loss: 0.662503182888031, val_loss: 0.7102304100990295

('adam', 0.2, 200, 200)
train_loss: 0.6623186469078064, val_loss: 0.7158592343330383

('adam', 0.2, 200, 300)
train_loss: 0.6623175144195557, val_loss: 0.7163354158401489


---------
BEST MODEL
('adam', 0.01, 100, 300)
val_loss: 0.6855117082595825
---------

Run from 2023-04-05 15:17:53.165102
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6887403130531311, val_loss: 0.689352810382843

('adam', 0.001, 8, 50)
train_loss: 0.6875967383384705, val_loss: 0.6889276504516602

('adam', 0.001, 8, 100)
train_loss: 0.6986920833587646, val_loss: 0.6992390155792236

('adam', 0.001, 16, 10)
train_loss: 0.695008397102356, val_loss: 0.6952712535858154

('adam', 0.001, 16, 50)
train_loss: 0.6900945901870728, val_loss: 0.6906739473342896

('adam', 0.001, 16, 100)
train_loss: 0.693868100643158, val_loss: 0.6947962045669556

('adam', 0.001, 32, 10)
train_loss: 0.699374258518219, val_loss: 0.7005549669265747

('adam', 0.001, 32, 50)
train_loss: 0.6916050910949707, val_loss: 0.692000150680542

('adam', 0.001, 32, 100)
train_loss: 0.6920673251152039, val_loss: 0.6928683519363403

('adam', 0.001, 64, 10)
train_loss: 0.6953315138816833, val_loss: 0.6957962512969971

('adam', 0.001, 64, 50)
train_loss: 0.6923367977142334, val_loss: 0.6932217478752136

('adam', 0.001, 64, 100)
train_loss: 0.6936643719673157, val_loss: 0.6950576305389404

('adam', 0.01, 8, 10)
train_loss: 0.6919052004814148, val_loss: 0.6922208666801453

('adam', 0.01, 8, 50)
train_loss: 0.6851630210876465, val_loss: 0.6877203583717346

('adam', 0.01, 8, 100)
train_loss: 0.6810255646705627, val_loss: 0.6858893036842346

('adam', 0.01, 16, 10)
train_loss: 0.693306565284729, val_loss: 0.6945239901542664

('adam', 0.01, 16, 50)
train_loss: 0.6858141422271729, val_loss: 0.6896229386329651

('adam', 0.01, 16, 100)
train_loss: 0.6909114122390747, val_loss: 0.6908489465713501

('adam', 0.01, 32, 10)
train_loss: 0.687420129776001, val_loss: 0.6898621916770935

('adam', 0.01, 32, 50)
train_loss: 0.6901800036430359, val_loss: 0.6921117305755615

('adam', 0.01, 32, 100)
train_loss: 0.6820434331893921, val_loss: 0.6867358684539795

('adam', 0.01, 64, 10)
train_loss: 0.6988996863365173, val_loss: 0.700141429901123

('adam', 0.01, 64, 50)
train_loss: 0.6846630573272705, val_loss: 0.6880703568458557

('adam', 0.01, 64, 100)
train_loss: 0.6877157688140869, val_loss: 0.6901887655258179

('adam', 0.1, 8, 10)
train_loss: 0.6902800798416138, val_loss: 0.6904217004776001

('adam', 0.1, 8, 50)
train_loss: 0.6783698797225952, val_loss: 0.6880364418029785

('adam', 0.1, 8, 100)
train_loss: 0.6634148955345154, val_loss: 0.7018536329269409

('adam', 0.1, 16, 10)
train_loss: 0.6908315420150757, val_loss: 0.6912321448326111

('adam', 0.1, 16, 50)
train_loss: 0.6749143004417419, val_loss: 0.6855787634849548

('adam', 0.1, 16, 100)
train_loss: 0.6746633648872375, val_loss: 0.6945052742958069

('adam', 0.1, 32, 10)
train_loss: 0.6918704509735107, val_loss: 0.694164514541626

('adam', 0.1, 32, 50)
train_loss: 0.6742876172065735, val_loss: 0.6859543323516846

('adam', 0.1, 32, 100)
train_loss: 0.6674516201019287, val_loss: 0.694267213344574

('adam', 0.1, 64, 10)
train_loss: 0.695773720741272, val_loss: 0.6973360180854797

('adam', 0.1, 64, 50)
train_loss: 0.6735357642173767, val_loss: 0.685903787612915

('adam', 0.1, 64, 100)
train_loss: 0.666608989238739, val_loss: 0.6922129392623901


---------
BEST MODEL
('adam', 0.1, 16, 50)
val_loss: 0.6855787634849548
---------

Run from 2023-04-05 16:17:51.060320
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6921522617340088, val_loss: 0.6923900842666626

('adam', 0.001, 8, 50)
train_loss: 0.6886928081512451, val_loss: 0.6895822286605835

('adam', 0.001, 8, 100)
train_loss: 0.6920398473739624, val_loss: 0.6927773952484131

('adam', 0.001, 16, 10)
train_loss: 0.6967204213142395, val_loss: 0.6972808837890625

('adam', 0.001, 16, 50)
train_loss: 0.6920230984687805, val_loss: 0.692272424697876

('adam', 0.001, 16, 100)
train_loss: 0.6944765448570251, val_loss: 0.6953787803649902

('adam', 0.001, 32, 10)
train_loss: 0.6935076713562012, val_loss: 0.6936495304107666

('adam', 0.001, 32, 50)
train_loss: 0.6906514763832092, val_loss: 0.6912482380867004

('adam', 0.001, 32, 100)
train_loss: 0.6945857405662537, val_loss: 0.6953716278076172

('adam', 0.001, 64, 10)
train_loss: 0.6910479068756104, val_loss: 0.6912054419517517

('adam', 0.001, 64, 50)
train_loss: 0.6917489767074585, val_loss: 0.6926083564758301

('adam', 0.001, 64, 100)
train_loss: 0.6912302374839783, val_loss: 0.6926592588424683

('adam', 0.01, 8, 10)
train_loss: 0.6857821941375732, val_loss: 0.6881694793701172

('adam', 0.01, 8, 50)
train_loss: 0.6878904104232788, val_loss: 0.6901280879974365

('adam', 0.01, 8, 100)
train_loss: 0.686537504196167, val_loss: 0.6884828805923462

('adam', 0.01, 16, 10)
train_loss: 0.6935144662857056, val_loss: 0.6938672661781311

('adam', 0.01, 16, 50)
train_loss: 0.6943462491035461, val_loss: 0.6962734460830688

('adam', 0.01, 16, 100)
train_loss: 0.6894272565841675, val_loss: 0.6911892890930176

('adam', 0.01, 32, 10)
train_loss: 0.69102942943573, val_loss: 0.692247211933136

('adam', 0.01, 32, 50)
train_loss: 0.6854498386383057, val_loss: 0.6885683536529541

('adam', 0.01, 32, 100)
train_loss: 0.6835978627204895, val_loss: 0.6874426603317261

('adam', 0.01, 64, 10)
train_loss: 0.6951285600662231, val_loss: 0.6968733072280884

('adam', 0.01, 64, 50)
train_loss: 0.6856864094734192, val_loss: 0.688681960105896

('adam', 0.01, 64, 100)
train_loss: 0.683551013469696, val_loss: 0.687595009803772

('adam', 0.1, 8, 10)
train_loss: 0.6846608519554138, val_loss: 0.6879764199256897

('adam', 0.1, 8, 50)
train_loss: 0.6720491647720337, val_loss: 0.6917401552200317

('adam', 0.1, 8, 100)
train_loss: 0.6651101112365723, val_loss: 0.6990877985954285

('adam', 0.1, 16, 10)
train_loss: 0.685894250869751, val_loss: 0.6897262334823608

('adam', 0.1, 16, 50)
train_loss: 0.6868613958358765, val_loss: 0.6950477957725525

('adam', 0.1, 16, 100)
train_loss: 0.6730747222900391, val_loss: 0.6978203058242798

('adam', 0.1, 32, 10)
train_loss: 0.6893433332443237, val_loss: 0.6917487382888794

('adam', 0.1, 32, 50)
train_loss: 0.6733881235122681, val_loss: 0.6858717203140259

('adam', 0.1, 32, 100)
train_loss: 0.6688028573989868, val_loss: 0.6904404163360596

('adam', 0.1, 64, 10)
train_loss: 0.6923251152038574, val_loss: 0.6937962770462036

('adam', 0.1, 64, 50)
train_loss: 0.6727968454360962, val_loss: 0.6860677003860474

('adam', 0.1, 64, 100)
train_loss: 0.6680656671524048, val_loss: 0.6898919939994812


---------
BEST MODEL
('adam', 0.1, 32, 50)
val_loss: 0.6858717203140259
---------

Run from 2023-04-10 09:46:25.937316
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6892871856689453, val_loss: 0.6896486878395081

('adam', 0.001, 8, 50)
train_loss: 0.6866737604141235, val_loss: 0.6883331537246704

('adam', 0.001, 8, 100)
train_loss: 0.6951019167900085, val_loss: 0.6963331699371338

('adam', 0.001, 16, 10)
train_loss: 0.6904044151306152, val_loss: 0.6905720233917236

('adam', 0.001, 16, 50)
train_loss: 0.688159704208374, val_loss: 0.6890804171562195

('adam', 0.001, 16, 100)
train_loss: 0.6901029944419861, val_loss: 0.690681517124176

('adam', 0.001, 32, 10)
train_loss: 0.6895391941070557, val_loss: 0.6898322105407715

('adam', 0.001, 32, 50)
train_loss: 0.6885781288146973, val_loss: 0.6894466280937195

('adam', 0.001, 32, 100)
train_loss: 0.6912646293640137, val_loss: 0.6921310424804688

('adam', 0.001, 64, 10)
train_loss: 0.697794497013092, val_loss: 0.6987664699554443

('adam', 0.001, 64, 50)
train_loss: 0.6898281574249268, val_loss: 0.6907168626785278

('adam', 0.001, 64, 100)
train_loss: 0.6870083808898926, val_loss: 0.6888782978057861

('adam', 0.01, 8, 10)
train_loss: 0.6949707865715027, val_loss: 0.6968255639076233

('adam', 0.01, 8, 50)
train_loss: 0.6905782222747803, val_loss: 0.6911864280700684

('adam', 0.01, 8, 100)
train_loss: 0.6801918745040894, val_loss: 0.6869216561317444

('adam', 0.01, 16, 10)
train_loss: 0.6859610676765442, val_loss: 0.6883304715156555

('adam', 0.01, 16, 50)
train_loss: 0.6913694739341736, val_loss: 0.6917979717254639

('adam', 0.01, 16, 100)
train_loss: 0.6882449984550476, val_loss: 0.6899988651275635

('adam', 0.01, 32, 10)
train_loss: 0.6965721845626831, val_loss: 0.6973530054092407

('adam', 0.01, 32, 50)
train_loss: 0.6911893486976624, val_loss: 0.6929765939712524

('adam', 0.01, 32, 100)
train_loss: 0.6818774342536926, val_loss: 0.6867377161979675

('adam', 0.01, 64, 10)
train_loss: 0.6943984031677246, val_loss: 0.6961658000946045

('adam', 0.01, 64, 50)
train_loss: 0.6861081719398499, val_loss: 0.6889575719833374

('adam', 0.01, 64, 100)
train_loss: 0.6864939332008362, val_loss: 0.6893465518951416

('adam', 0.1, 8, 10)
train_loss: 0.6907198429107666, val_loss: 0.688868522644043

('adam', 0.1, 8, 50)
train_loss: 0.6703271269798279, val_loss: 0.6877493262290955

('adam', 0.1, 8, 100)
train_loss: 0.6659512519836426, val_loss: 0.7013454437255859

('adam', 0.1, 16, 10)
train_loss: 0.6914544105529785, val_loss: 0.6953704953193665

('adam', 0.1, 16, 50)
train_loss: 0.6733489036560059, val_loss: 0.6871082186698914

('adam', 0.1, 16, 100)
train_loss: 0.6660001873970032, val_loss: 0.6943824291229248

('adam', 0.1, 32, 10)
train_loss: 0.6837462782859802, val_loss: 0.68705153465271

('adam', 0.1, 32, 50)
train_loss: 0.6770262718200684, val_loss: 0.6855716705322266

('adam', 0.1, 32, 100)
train_loss: 0.6654517650604248, val_loss: 0.6951825618743896

('adam', 0.1, 64, 10)
train_loss: 0.6933583617210388, val_loss: 0.694628119468689

('adam', 0.1, 64, 50)
train_loss: 0.6728234887123108, val_loss: 0.6860596537590027

('adam', 0.1, 64, 100)
train_loss: 0.6683178544044495, val_loss: 0.6895580887794495


---------
BEST MODEL
('adam', 0.1, 32, 50)
val_loss: 0.6855716705322266
---------

Run from 2023-04-10 12:49:06.264217
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6922624111175537, val_loss: 0.6912432909011841

('adam', 0.001, 8, 50)
train_loss: 0.6977776885032654, val_loss: 0.7049711346626282

('adam', 0.001, 8, 100)
train_loss: 0.6868582367897034, val_loss: 0.6830585598945618

('adam', 0.001, 16, 10)
train_loss: 0.6936680674552917, val_loss: 0.6949144601821899

('adam', 0.001, 16, 50)
train_loss: 0.6898583173751831, val_loss: 0.6855947971343994

('adam', 0.001, 16, 100)
train_loss: 0.6924480199813843, val_loss: 0.6931442022323608

('adam', 0.001, 32, 10)
train_loss: 0.6991636753082275, val_loss: 0.7085164785385132

('adam', 0.001, 32, 50)
train_loss: 0.69672691822052, val_loss: 0.7026305794715881

('adam', 0.001, 32, 100)
train_loss: 0.6919587850570679, val_loss: 0.6917188763618469

('adam', 0.001, 64, 10)
train_loss: 0.6924626231193542, val_loss: 0.6917572021484375

('adam', 0.001, 64, 50)
train_loss: 0.6896421313285828, val_loss: 0.6861639022827148

('adam', 0.001, 64, 100)
train_loss: 0.7028754353523254, val_loss: 0.7162503600120544

('adam', 0.01, 8, 10)
train_loss: 0.6897408962249756, val_loss: 0.6852248311042786

('adam', 0.01, 8, 50)
train_loss: 0.6863334774971008, val_loss: 0.6831312775611877

('adam', 0.01, 8, 100)
train_loss: 0.685634434223175, val_loss: 0.6820410490036011

('adam', 0.01, 16, 10)
train_loss: 0.694938600063324, val_loss: 0.6984922885894775

('adam', 0.01, 16, 50)
train_loss: 0.6859176754951477, val_loss: 0.6825570464134216

('adam', 0.01, 16, 100)
train_loss: 0.6865320801734924, val_loss: 0.6831124424934387

('adam', 0.01, 32, 10)
train_loss: 0.6882362961769104, val_loss: 0.6859025955200195

('adam', 0.01, 32, 50)
train_loss: 0.6858264207839966, val_loss: 0.6823927760124207

('adam', 0.01, 32, 100)
train_loss: 0.6861763000488281, val_loss: 0.6829560995101929

('adam', 0.01, 64, 10)
train_loss: 0.6939147710800171, val_loss: 0.6973336338996887

('adam', 0.01, 64, 50)
train_loss: 0.6887109875679016, val_loss: 0.6869701743125916

('adam', 0.01, 64, 100)
train_loss: 0.6844912767410278, val_loss: 0.6809228658676147

('adam', 0.1, 8, 10)
train_loss: 0.68567955493927, val_loss: 0.6808555126190186

('adam', 0.1, 8, 50)
train_loss: 0.6876124739646912, val_loss: 0.6821047067642212

('adam', 0.1, 8, 100)
train_loss: 0.684461236000061, val_loss: 0.6815191507339478

('adam', 0.1, 16, 10)
train_loss: 0.7038952708244324, val_loss: 0.6994024515151978

('adam', 0.1, 16, 50)
train_loss: 0.6880702376365662, val_loss: 0.6848986148834229

('adam', 0.1, 16, 100)
train_loss: 0.6930261254310608, val_loss: 0.6902340054512024

('adam', 0.1, 32, 10)
train_loss: 0.686707079410553, val_loss: 0.683565616607666

('adam', 0.1, 32, 50)
train_loss: 0.6865714192390442, val_loss: 0.6823192834854126

('adam', 0.1, 32, 100)
train_loss: 0.6831510663032532, val_loss: 0.6821060180664062

('adam', 0.1, 64, 10)
train_loss: 0.6896722316741943, val_loss: 0.6881268620491028

('adam', 0.1, 64, 50)
train_loss: 0.6831284165382385, val_loss: 0.6821980476379395

('adam', 0.1, 64, 100)
train_loss: 0.6831237077713013, val_loss: 0.6818326711654663


---------
BEST MODEL
('adam', 0.1, 8, 10)
val_loss: 0.6808555126190186
---------

Run from 2023-04-10 14:05:33.634112
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6887524127960205, val_loss: 0.697257936000824

('adam', 0.001, 8, 50)
train_loss: 0.706978440284729, val_loss: 0.6879101991653442

('adam', 0.001, 8, 100)
train_loss: 0.6820715069770813, val_loss: 0.7027496099472046

('adam', 0.001, 16, 10)
train_loss: 0.7025038003921509, val_loss: 0.6891319155693054

('adam', 0.001, 16, 50)
train_loss: 0.6967408657073975, val_loss: 0.6911846399307251

('adam', 0.001, 16, 100)
train_loss: 0.6889013051986694, val_loss: 0.6958073377609253

('adam', 0.001, 32, 10)
train_loss: 0.6876741051673889, val_loss: 0.7000546455383301

('adam', 0.001, 32, 50)
train_loss: 0.6854893565177917, val_loss: 0.7004225850105286

('adam', 0.001, 32, 100)
train_loss: 0.707091212272644, val_loss: 0.6879896521568298

('adam', 0.001, 64, 10)
train_loss: 0.6888439655303955, val_loss: 0.6974254846572876

('adam', 0.001, 64, 50)
train_loss: 0.7009800672531128, val_loss: 0.6896197199821472

('adam', 0.001, 64, 100)
train_loss: 0.6894010901451111, val_loss: 0.6954002380371094

('adam', 0.01, 8, 10)
train_loss: 0.6816408038139343, val_loss: 0.7026764154434204

('adam', 0.01, 8, 50)
train_loss: 0.6833950877189636, val_loss: 0.7017269730567932

('adam', 0.01, 8, 100)
train_loss: 0.6807348728179932, val_loss: 0.703473687171936

('adam', 0.01, 16, 10)
train_loss: 0.7073269486427307, val_loss: 0.6878238916397095

('adam', 0.01, 16, 50)
train_loss: 0.6933131217956543, val_loss: 0.6936919689178467

('adam', 0.01, 16, 100)
train_loss: 0.6937331557273865, val_loss: 0.6933428645133972

('adam', 0.01, 32, 10)
train_loss: 0.6833762526512146, val_loss: 0.7019083499908447

('adam', 0.01, 32, 50)
train_loss: 0.678276777267456, val_loss: 0.7077151536941528

('adam', 0.01, 32, 100)
train_loss: 0.6773565411567688, val_loss: 0.7086831331253052

('adam', 0.01, 64, 10)
train_loss: 0.704407811164856, val_loss: 0.6881209015846252

('adam', 0.01, 64, 50)
train_loss: 0.6858109831809998, val_loss: 0.6986114978790283

('adam', 0.01, 64, 100)
train_loss: 0.6760280728340149, val_loss: 0.7116442322731018

('adam', 0.1, 8, 10)
train_loss: 0.6752134561538696, val_loss: 0.7146848440170288

('adam', 0.1, 8, 50)
train_loss: 0.6704016923904419, val_loss: 0.722415030002594

('adam', 0.1, 8, 100)
train_loss: 0.6738409399986267, val_loss: 0.7299126982688904

('adam', 0.1, 16, 10)
train_loss: 0.692646861076355, val_loss: 0.69483882188797

('adam', 0.1, 16, 50)
train_loss: 0.678612470626831, val_loss: 0.7405716180801392

('adam', 0.1, 16, 100)
train_loss: 0.6809572577476501, val_loss: 0.7325910925865173

('adam', 0.1, 32, 10)
train_loss: 0.6814000010490417, val_loss: 0.7062731981277466

('adam', 0.1, 32, 50)
train_loss: 0.6725790500640869, val_loss: 0.7274643182754517

('adam', 0.1, 32, 100)
train_loss: 0.6722575426101685, val_loss: 0.7291069626808167

('adam', 0.1, 64, 10)
train_loss: 0.6995790004730225, val_loss: 0.690466046333313

('adam', 0.1, 64, 50)
train_loss: 0.6727926135063171, val_loss: 0.7233691215515137

('adam', 0.1, 64, 100)
train_loss: 0.6722503900527954, val_loss: 0.7310008406639099


---------
BEST MODEL
('adam', 0.01, 16, 10)
val_loss: 0.6878238916397095
---------

Run from 2023-04-17 09:48:40.081541
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6960346698760986, val_loss: 0.696441650390625

('adam', 0.001, 8, 50)
train_loss: 0.6850097179412842, val_loss: 0.6858423352241516

('adam', 0.001, 8, 100)
train_loss: 0.6895813941955566, val_loss: 0.6901416182518005

('adam', 0.001, 16, 10)
train_loss: 0.7033421993255615, val_loss: 0.7056336998939514

('adam', 0.001, 16, 50)
train_loss: 0.6822125911712646, val_loss: 0.6837456822395325

('adam', 0.001, 16, 100)
train_loss: 0.7091254591941833, val_loss: 0.7123656272888184

('adam', 0.001, 32, 10)
train_loss: 0.6923264861106873, val_loss: 0.6923688054084778

('adam', 0.001, 32, 50)
train_loss: 0.6939003467559814, val_loss: 0.694359540939331

('adam', 0.001, 32, 100)
train_loss: 0.6801498532295227, val_loss: 0.6822329163551331

('adam', 0.001, 64, 10)
train_loss: 0.6966850757598877, val_loss: 0.697399914264679

('adam', 0.001, 64, 50)
train_loss: 0.7083856463432312, val_loss: 0.7112295031547546

('adam', 0.001, 64, 100)
train_loss: 0.687971293926239, val_loss: 0.6888046264648438

('adam', 0.01, 8, 10)
train_loss: 0.706536054611206, val_loss: 0.7088985443115234

('adam', 0.01, 8, 50)
train_loss: 0.671074628829956, val_loss: 0.6747331619262695

('adam', 0.01, 8, 100)
train_loss: 0.6719433069229126, val_loss: 0.6753169894218445

('adam', 0.01, 16, 10)
train_loss: 0.7018811702728271, val_loss: 0.703042209148407

('adam', 0.01, 16, 50)
train_loss: 0.6877220273017883, val_loss: 0.6884245872497559

('adam', 0.01, 16, 100)
train_loss: 0.6695130467414856, val_loss: 0.6741383671760559

('adam', 0.01, 32, 10)
train_loss: 0.6987255811691284, val_loss: 0.6996323466300964

('adam', 0.01, 32, 50)
train_loss: 0.6901477575302124, val_loss: 0.6908962726593018

('adam', 0.01, 32, 100)
train_loss: 0.6709691882133484, val_loss: 0.6754164099693298

('adam', 0.01, 64, 10)
train_loss: 0.7009499073028564, val_loss: 0.702679455280304

('adam', 0.01, 64, 50)
train_loss: 0.6719810962677002, val_loss: 0.6755049824714661

('adam', 0.01, 64, 100)
train_loss: 0.69224613904953, val_loss: 0.6933091282844543

('adam', 0.1, 8, 10)
train_loss: 0.6844678521156311, val_loss: 0.6791698336601257

('adam', 0.1, 8, 50)
train_loss: 0.6606543064117432, val_loss: 0.6905801296234131

('adam', 0.1, 8, 100)
train_loss: 0.6605411767959595, val_loss: 0.6888124346733093

('adam', 0.1, 16, 10)
train_loss: 0.6729952096939087, val_loss: 0.6765387654304504

('adam', 0.1, 16, 50)
train_loss: 0.6594303846359253, val_loss: 0.6863805651664734

('adam', 0.1, 16, 100)
train_loss: 0.6561059355735779, val_loss: 0.6948238015174866

('adam', 0.1, 32, 10)
train_loss: 0.696965754032135, val_loss: 0.6958503127098083

('adam', 0.1, 32, 50)
train_loss: 0.6589294672012329, val_loss: 0.6765792965888977

('adam', 0.1, 32, 100)
train_loss: 0.656506359577179, val_loss: 0.6938104629516602

('adam', 0.1, 64, 10)
train_loss: 0.6862437129020691, val_loss: 0.6864020228385925

('adam', 0.1, 64, 50)
train_loss: 0.6576560735702515, val_loss: 0.6784985661506653

('adam', 0.1, 64, 100)
train_loss: 0.6560245156288147, val_loss: 0.6888152956962585

('sgd', 0.001, 8, 10)
train_loss: 0.6892644762992859, val_loss: 0.6892637610435486

('sgd', 0.001, 8, 50)
train_loss: 0.703294575214386, val_loss: 0.7054111957550049

('sgd', 0.001, 8, 100)
train_loss: 0.7006431818008423, val_loss: 0.7021772861480713

('sgd', 0.001, 16, 10)
train_loss: 0.6834219098091125, val_loss: 0.6854956746101379

('sgd', 0.001, 16, 50)
train_loss: 0.7072685956954956, val_loss: 0.7108839154243469

('sgd', 0.001, 16, 100)
train_loss: 0.7151135206222534, val_loss: 0.7218053936958313

('sgd', 0.001, 32, 10)
train_loss: 0.6993427872657776, val_loss: 0.7004382014274597

('sgd', 0.001, 32, 50)
train_loss: 0.705381453037262, val_loss: 0.7083269953727722

('sgd', 0.001, 32, 100)
train_loss: 0.7060340642929077, val_loss: 0.709193229675293

('sgd', 0.001, 64, 10)
train_loss: 0.6846821904182434, val_loss: 0.68598872423172

('sgd', 0.001, 64, 50)
train_loss: 0.6969231367111206, val_loss: 0.6974790692329407

('sgd', 0.001, 64, 100)
train_loss: 0.6812686324119568, val_loss: 0.6853997111320496

('sgd', 0.01, 8, 10)
train_loss: 0.6824690103530884, val_loss: 0.6842370629310608

('sgd', 0.01, 8, 50)
train_loss: 0.7032991051673889, val_loss: 0.7057569026947021

('sgd', 0.01, 8, 100)
train_loss: 0.684874951839447, val_loss: 0.6858280301094055

('sgd', 0.01, 16, 10)
train_loss: 0.6890829205513, val_loss: 0.689061164855957

('sgd', 0.01, 16, 50)
train_loss: 0.7145088315010071, val_loss: 0.720111608505249

('sgd', 0.01, 16, 100)
train_loss: 0.6840418577194214, val_loss: 0.685101330280304

('sgd', 0.01, 32, 10)
train_loss: 0.6841340065002441, val_loss: 0.6855688095092773

('sgd', 0.01, 32, 50)
train_loss: 0.6812248229980469, val_loss: 0.6836842894554138

('sgd', 0.01, 32, 100)
train_loss: 0.681174099445343, val_loss: 0.6830856204032898

('sgd', 0.01, 64, 10)
train_loss: 0.6891751885414124, val_loss: 0.6892008185386658

('sgd', 0.01, 64, 50)
train_loss: 0.6972630023956299, val_loss: 0.6979814171791077

('sgd', 0.01, 64, 100)
train_loss: 0.687179684638977, val_loss: 0.6875901222229004

('sgd', 0.1, 8, 10)
train_loss: 0.6912803649902344, val_loss: 0.6912944316864014

('sgd', 0.1, 8, 50)
train_loss: 0.6990597248077393, val_loss: 0.70014888048172

('sgd', 0.1, 8, 100)
train_loss: 0.6992450952529907, val_loss: 0.6979933381080627

('sgd', 0.1, 16, 10)
train_loss: 0.6880882382392883, val_loss: 0.6883830428123474

('sgd', 0.1, 16, 50)
train_loss: 0.6837611794471741, val_loss: 0.684675395488739

('sgd', 0.1, 16, 100)
train_loss: 0.689103901386261, val_loss: 0.6893848776817322

('sgd', 0.1, 32, 10)
train_loss: 0.7147861123085022, val_loss: 0.7198953628540039

('sgd', 0.1, 32, 50)
train_loss: 0.6892123222351074, val_loss: 0.6894925236701965

('sgd', 0.1, 32, 100)
train_loss: 0.6899875402450562, val_loss: 0.6911121010780334

('sgd', 0.1, 64, 10)
train_loss: 0.6829753518104553, val_loss: 0.6844661831855774

('sgd', 0.1, 64, 50)
train_loss: 0.6886929273605347, val_loss: 0.6894464492797852

('sgd', 0.1, 64, 100)
train_loss: 0.6979711651802063, val_loss: 0.6993728280067444


---------
BEST MODEL
('adam', 0.01, 16, 100)
val_loss: 0.6741383671760559
---------

Run from 2023-04-17 16:20:08.221886
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7158055305480957, val_loss: 0.6871189475059509

('adam', 0.001, 8, 50)
train_loss: 0.6883704662322998, val_loss: 0.6950749158859253

('adam', 0.001, 8, 100)
train_loss: 0.6755518913269043, val_loss: 0.7021188735961914

('adam', 0.001, 16, 10)
train_loss: 0.708392322063446, val_loss: 0.6885534524917603

('adam', 0.001, 16, 50)
train_loss: 0.7065109014511108, val_loss: 0.6888232827186584

('adam', 0.001, 16, 100)
train_loss: 0.6863992810249329, val_loss: 0.6959648132324219

('adam', 0.001, 32, 10)
train_loss: 0.6819603443145752, val_loss: 0.700831413269043

('adam', 0.001, 32, 50)
train_loss: 0.679101824760437, val_loss: 0.701309323310852

('adam', 0.001, 32, 100)
train_loss: 0.7018601894378662, val_loss: 0.6901363134384155

('adam', 0.001, 64, 10)
train_loss: 0.6806449294090271, val_loss: 0.7038706541061401

('adam', 0.001, 64, 50)
train_loss: 0.6869648694992065, val_loss: 0.6957210302352905

('adam', 0.001, 64, 100)
train_loss: 0.6912639737129211, val_loss: 0.69416743516922

('adam', 0.01, 8, 10)
train_loss: 0.7094847559928894, val_loss: 0.688086211681366

('adam', 0.01, 8, 50)
train_loss: 0.6735702753067017, val_loss: 0.7045577764511108

('adam', 0.01, 8, 100)
train_loss: 0.6676782369613647, val_loss: 0.7091237306594849

('adam', 0.01, 16, 10)
train_loss: 0.6786392331123352, val_loss: 0.7002389430999756

('adam', 0.01, 16, 50)
train_loss: 0.6775092482566833, val_loss: 0.7016446590423584

('adam', 0.01, 16, 100)
train_loss: 0.6665425896644592, val_loss: 0.7110569477081299

('adam', 0.01, 32, 10)
train_loss: 0.6788445711135864, val_loss: 0.7002267837524414

('adam', 0.01, 32, 50)
train_loss: 0.69525146484375, val_loss: 0.6924249529838562

('adam', 0.01, 32, 100)
train_loss: 0.6741513013839722, val_loss: 0.7040181159973145

('adam', 0.01, 64, 10)
train_loss: 0.7019545435905457, val_loss: 0.6902161836624146

('adam', 0.01, 64, 50)
train_loss: 0.6964240074157715, val_loss: 0.692165732383728

('adam', 0.01, 64, 100)
train_loss: 0.6804653406143188, val_loss: 0.7001832723617554

('adam', 0.1, 8, 10)
train_loss: 0.667777955532074, val_loss: 0.7216025590896606

('adam', 0.1, 8, 50)
train_loss: 0.6551663875579834, val_loss: 0.7515421509742737

('adam', 0.1, 8, 100)
train_loss: 0.6542426347732544, val_loss: 0.7484437227249146

('adam', 0.1, 16, 10)
train_loss: 0.6633308529853821, val_loss: 0.7167272567749023

('adam', 0.1, 16, 50)
train_loss: 0.6526015996932983, val_loss: 0.7496756315231323

('adam', 0.1, 16, 100)
train_loss: 0.6534009575843811, val_loss: 0.7527561187744141

('adam', 0.1, 32, 10)
train_loss: 0.6660924553871155, val_loss: 0.7125340104103088

('adam', 0.1, 32, 50)
train_loss: 0.6559814214706421, val_loss: 0.729200005531311

('adam', 0.1, 32, 100)
train_loss: 0.6522941589355469, val_loss: 0.751444935798645

('adam', 0.1, 64, 10)
train_loss: 0.6686117053031921, val_loss: 0.7092937231063843

('adam', 0.1, 64, 50)
train_loss: 0.6574339866638184, val_loss: 0.7267826199531555

('adam', 0.1, 64, 100)
train_loss: 0.6521283388137817, val_loss: 0.7501840591430664

('sgd', 0.001, 8, 10)
train_loss: 0.7206939458847046, val_loss: 0.6865426301956177

('sgd', 0.001, 8, 50)
train_loss: 0.7191649079322815, val_loss: 0.6866308450698853

('sgd', 0.001, 8, 100)
train_loss: 0.7016069293022156, val_loss: 0.6902341842651367

('sgd', 0.001, 16, 10)
train_loss: 0.6824578642845154, val_loss: 0.7010642290115356

('sgd', 0.001, 16, 50)
train_loss: 0.7146055102348328, val_loss: 0.6874016523361206

('sgd', 0.001, 16, 100)
train_loss: 0.6927682757377625, val_loss: 0.6932699680328369

('sgd', 0.001, 32, 10)
train_loss: 0.7034876346588135, val_loss: 0.6897456645965576

('sgd', 0.001, 32, 50)
train_loss: 0.7166377305984497, val_loss: 0.6871210932731628

('sgd', 0.001, 32, 100)
train_loss: 0.6988168954849243, val_loss: 0.6910659074783325

('sgd', 0.001, 64, 10)
train_loss: 0.6825355887413025, val_loss: 0.7010936737060547

('sgd', 0.001, 64, 50)
train_loss: 0.7121150493621826, val_loss: 0.6878999471664429

('sgd', 0.001, 64, 100)
train_loss: 0.6843677759170532, val_loss: 0.6985576152801514

('sgd', 0.01, 8, 10)
train_loss: 0.7027111053466797, val_loss: 0.6899092197418213

('sgd', 0.01, 8, 50)
train_loss: 0.6821531653404236, val_loss: 0.6983855962753296

('sgd', 0.01, 8, 100)
train_loss: 0.7060236930847168, val_loss: 0.6889054775238037

('sgd', 0.01, 16, 10)
train_loss: 0.6855577230453491, val_loss: 0.6972357630729675

('sgd', 0.01, 16, 50)
train_loss: 0.681747317314148, val_loss: 0.698506236076355

('sgd', 0.01, 16, 100)
train_loss: 0.6894555687904358, val_loss: 0.6946812868118286

('sgd', 0.01, 32, 10)
train_loss: 0.6849865317344666, val_loss: 0.6978519558906555

('sgd', 0.01, 32, 50)
train_loss: 0.6878823041915894, val_loss: 0.6955132484436035

('sgd', 0.01, 32, 100)
train_loss: 0.7091046571731567, val_loss: 0.6881551146507263

('sgd', 0.01, 64, 10)
train_loss: 0.7019200325012207, val_loss: 0.690155029296875

('sgd', 0.01, 64, 50)
train_loss: 0.6997237801551819, val_loss: 0.6907917261123657

('sgd', 0.01, 64, 100)
train_loss: 0.7204800248146057, val_loss: 0.6859655976295471

('sgd', 0.1, 8, 10)
train_loss: 0.679835855960846, val_loss: 0.7012063264846802

('sgd', 0.1, 8, 50)
train_loss: 0.6763132810592651, val_loss: 0.705329179763794

('sgd', 0.1, 8, 100)
train_loss: 0.6981377601623535, val_loss: 0.6924397945404053

('sgd', 0.1, 16, 10)
train_loss: 0.6740564703941345, val_loss: 0.7035032510757446

('sgd', 0.1, 16, 50)
train_loss: 0.6799578666687012, val_loss: 0.7011895179748535

('sgd', 0.1, 16, 100)
train_loss: 0.68808513879776, val_loss: 0.6956677436828613

('sgd', 0.1, 32, 10)
train_loss: 0.6793123483657837, val_loss: 0.7000473737716675

('sgd', 0.1, 32, 50)
train_loss: 0.6839492321014404, val_loss: 0.6977503895759583

('sgd', 0.1, 32, 100)
train_loss: 0.692375123500824, val_loss: 0.693718671798706

('sgd', 0.1, 64, 10)
train_loss: 0.7038683295249939, val_loss: 0.6896075010299683

('sgd', 0.1, 64, 50)
train_loss: 0.6846699714660645, val_loss: 0.6969267129898071

('sgd', 0.1, 64, 100)
train_loss: 0.6943986415863037, val_loss: 0.6928353309631348


---------
BEST MODEL
('sgd', 0.01, 64, 100)
val_loss: 0.6859655976295471
---------
