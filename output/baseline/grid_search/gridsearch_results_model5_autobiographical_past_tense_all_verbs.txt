
Run from 2023-04-02 11:07:51.923878
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6977338790893555, val_loss: 0.6986410617828369

('adam', 0.01, 50, 200)
train_loss: 0.6995610594749451, val_loss: 0.6975870132446289

('adam', 0.01, 50, 300)
train_loss: 0.6435174345970154, val_loss: 0.6636559963226318

('adam', 0.01, 100, 100)
train_loss: 0.644364595413208, val_loss: 0.6651291251182556

('adam', 0.01, 100, 200)
train_loss: 0.7023705840110779, val_loss: 0.7012718915939331

('adam', 0.01, 100, 300)
train_loss: 0.5814379453659058, val_loss: 0.6371710896492004

('adam', 0.01, 200, 100)
train_loss: 0.6327714323997498, val_loss: 0.6619626879692078

('adam', 0.01, 200, 200)
train_loss: 0.6055222749710083, val_loss: 0.6462444067001343

('adam', 0.01, 200, 300)
train_loss: 0.6859411001205444, val_loss: 0.6906826496124268

('adam', 0.1, 50, 100)
train_loss: 0.5903254151344299, val_loss: 0.6513880491256714

('adam', 0.1, 50, 200)
train_loss: 0.5454580783843994, val_loss: 0.652981162071228

('adam', 0.1, 50, 300)
train_loss: 0.5208355188369751, val_loss: 0.6197494268417358

('adam', 0.1, 100, 100)
train_loss: 0.5580489039421082, val_loss: 0.6291362047195435

('adam', 0.1, 100, 200)
train_loss: 0.4850724935531616, val_loss: 0.6506658792495728

('adam', 0.1, 100, 300)
train_loss: 0.47549915313720703, val_loss: 0.6813781261444092

('adam', 0.1, 200, 100)
train_loss: 0.5147457122802734, val_loss: 0.6267491579055786

('adam', 0.1, 200, 200)
train_loss: 0.48878252506256104, val_loss: 0.6450572609901428

('adam', 0.1, 200, 300)
train_loss: 0.47492969036102295, val_loss: 0.6858170628547668

('adam', 0.2, 50, 100)
train_loss: 0.5506681203842163, val_loss: 0.6199219822883606

('adam', 0.2, 50, 200)
train_loss: 0.508474588394165, val_loss: 0.6622951030731201

('adam', 0.2, 50, 300)
train_loss: 0.499062716960907, val_loss: 0.7168828248977661

('adam', 0.2, 100, 100)
train_loss: 0.48109811544418335, val_loss: 0.6595951318740845

('adam', 0.2, 100, 200)
train_loss: 0.4738958477973938, val_loss: 0.7002784013748169

('adam', 0.2, 100, 300)
train_loss: 0.4738588035106659, val_loss: 0.7014623880386353

('adam', 0.2, 200, 100)
train_loss: 0.49459949135780334, val_loss: 0.6392329931259155

('adam', 0.2, 200, 200)
train_loss: 0.4738767445087433, val_loss: 0.7008549571037292

('adam', 0.2, 200, 300)
train_loss: 0.47376468777656555, val_loss: 0.7073819637298584


---------
BEST MODEL
('adam', 0.1, 50, 300)
val_loss: 0.6197494268417358
---------

Run from 2023-04-05 16:47:52.638970
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7099580764770508, val_loss: 0.7035363912582397

('adam', 0.001, 8, 50)
train_loss: 0.6708102822303772, val_loss: 0.6851726770401001

('adam', 0.001, 8, 100)
train_loss: 0.6637423634529114, val_loss: 0.6799132823944092

('adam', 0.001, 16, 10)
train_loss: 0.6736789345741272, val_loss: 0.7007622122764587

('adam', 0.001, 16, 50)
train_loss: 0.6877782344818115, val_loss: 0.6908937692642212

('adam', 0.001, 16, 100)
train_loss: 0.6595006585121155, val_loss: 0.6842092871665955

('adam', 0.001, 32, 10)
train_loss: 0.677619993686676, val_loss: 0.7114651799201965

('adam', 0.001, 32, 50)
train_loss: 0.6707096099853516, val_loss: 0.7034165859222412

('adam', 0.001, 32, 100)
train_loss: 0.7581450939178467, val_loss: 0.7411125898361206

('adam', 0.001, 64, 10)
train_loss: 0.6767854690551758, val_loss: 0.708336353302002

('adam', 0.001, 64, 50)
train_loss: 0.6744142770767212, val_loss: 0.6860263347625732

('adam', 0.001, 64, 100)
train_loss: 0.7578821778297424, val_loss: 0.7412285804748535

('adam', 0.01, 8, 10)
train_loss: 0.6736441850662231, val_loss: 0.6847817897796631

('adam', 0.01, 8, 50)
train_loss: 0.63468337059021, val_loss: 0.663637638092041

('adam', 0.01, 8, 100)
train_loss: 0.6177084445953369, val_loss: 0.652087926864624

('adam', 0.01, 16, 10)
train_loss: 0.6706224083900452, val_loss: 0.6828233003616333

('adam', 0.01, 16, 50)
train_loss: 0.6674817204475403, val_loss: 0.6806792616844177

('adam', 0.01, 16, 100)
train_loss: 0.6397994756698608, val_loss: 0.6665297150611877

('adam', 0.01, 32, 10)
train_loss: 0.6641966700553894, val_loss: 0.6890974044799805

('adam', 0.01, 32, 50)
train_loss: 0.6481050252914429, val_loss: 0.6695842742919922

('adam', 0.01, 32, 100)
train_loss: 0.6324385404586792, val_loss: 0.6616160869598389

('adam', 0.01, 64, 10)
train_loss: 0.7849765419960022, val_loss: 0.7628713846206665

('adam', 0.01, 64, 50)
train_loss: 0.6892737150192261, val_loss: 0.6922597289085388

('adam', 0.01, 64, 100)
train_loss: 0.6300844550132751, val_loss: 0.6590480804443359

('adam', 0.1, 8, 10)
train_loss: 0.65584397315979, val_loss: 0.6624054312705994

('adam', 0.1, 8, 50)
train_loss: 0.5354439616203308, val_loss: 0.6251909732818604

('adam', 0.1, 8, 100)
train_loss: 0.4828805923461914, val_loss: 0.640519917011261

('adam', 0.1, 16, 10)
train_loss: 0.686922013759613, val_loss: 0.6894186735153198

('adam', 0.1, 16, 50)
train_loss: 0.5571685433387756, val_loss: 0.6247161626815796

('adam', 0.1, 16, 100)
train_loss: 0.5076935291290283, val_loss: 0.6191527247428894

('adam', 0.1, 32, 10)
train_loss: 0.6581743955612183, val_loss: 0.6754187345504761

('adam', 0.1, 32, 50)
train_loss: 0.5906563997268677, val_loss: 0.6379507780075073

('adam', 0.1, 32, 100)
train_loss: 0.5072288513183594, val_loss: 0.6292763352394104

('adam', 0.1, 64, 10)
train_loss: 0.707817792892456, val_loss: 0.7099599242210388

('adam', 0.1, 64, 50)
train_loss: 0.5963706374168396, val_loss: 0.6416810750961304

('adam', 0.1, 64, 100)
train_loss: 0.5124679803848267, val_loss: 0.6259775161743164


---------
BEST MODEL
('adam', 0.1, 16, 100)
val_loss: 0.6191527247428894
---------

Run from 2023-04-10 10:33:45.093962
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6961126923561096, val_loss: 0.6948360204696655

('adam', 0.001, 8, 50)
train_loss: 0.6835982203483582, val_loss: 0.688728928565979

('adam', 0.001, 8, 100)
train_loss: 0.6932504177093506, val_loss: 0.694467306137085

('adam', 0.001, 16, 10)
train_loss: 0.6723899841308594, val_loss: 0.6958865523338318

('adam', 0.001, 16, 50)
train_loss: 0.6756718754768372, val_loss: 0.6863815784454346

('adam', 0.001, 16, 100)
train_loss: 0.6730452179908752, val_loss: 0.6847197413444519

('adam', 0.001, 32, 10)
train_loss: 0.6928361654281616, val_loss: 0.6929609775543213

('adam', 0.001, 32, 50)
train_loss: 0.6682997941970825, val_loss: 0.6941819190979004

('adam', 0.001, 32, 100)
train_loss: 0.7228049039840698, val_loss: 0.7130358219146729

('adam', 0.001, 64, 10)
train_loss: 0.6751533150672913, val_loss: 0.7038102746009827

('adam', 0.001, 64, 50)
train_loss: 0.7125967741012573, val_loss: 0.7056668996810913

('adam', 0.001, 64, 100)
train_loss: 0.6678371429443359, val_loss: 0.6971210241317749

('adam', 0.01, 8, 10)
train_loss: 0.6660153269767761, val_loss: 0.6800938844680786

('adam', 0.01, 8, 50)
train_loss: 0.6836513876914978, val_loss: 0.6899533867835999

('adam', 0.01, 8, 100)
train_loss: 0.6103975772857666, val_loss: 0.6495249271392822

('adam', 0.01, 16, 10)
train_loss: 0.6992882490158081, val_loss: 0.6971490979194641

('adam', 0.01, 16, 50)
train_loss: 0.6611875891685486, val_loss: 0.6757959127426147

('adam', 0.01, 16, 100)
train_loss: 0.6731259226799011, val_loss: 0.6827466487884521

('adam', 0.01, 32, 10)
train_loss: 0.7736542224884033, val_loss: 0.7525079250335693

('adam', 0.01, 32, 50)
train_loss: 0.6594676971435547, val_loss: 0.6755070686340332

('adam', 0.01, 32, 100)
train_loss: 0.708512544631958, val_loss: 0.7056474089622498

('adam', 0.01, 64, 10)
train_loss: 0.6681751608848572, val_loss: 0.6856223344802856

('adam', 0.01, 64, 50)
train_loss: 0.7012408375740051, val_loss: 0.7009249925613403

('adam', 0.01, 64, 100)
train_loss: 0.6962074637413025, val_loss: 0.6970044374465942

('adam', 0.1, 8, 10)
train_loss: 0.656825602054596, val_loss: 0.6664913892745972

('adam', 0.1, 8, 50)
train_loss: 0.5097842812538147, val_loss: 0.632333517074585

('adam', 0.1, 8, 100)
train_loss: 0.4899951219558716, val_loss: 0.6389267444610596

('adam', 0.1, 16, 10)
train_loss: 0.6237408518791199, val_loss: 0.6576089859008789

('adam', 0.1, 16, 50)
train_loss: 0.5651981234550476, val_loss: 0.6677793264389038

('adam', 0.1, 16, 100)
train_loss: 0.5078815817832947, val_loss: 0.6261028051376343

('adam', 0.1, 32, 10)
train_loss: 0.7011193633079529, val_loss: 0.7012947797775269

('adam', 0.1, 32, 50)
train_loss: 0.5542249083518982, val_loss: 0.6291829347610474

('adam', 0.1, 32, 100)
train_loss: 0.5116572976112366, val_loss: 0.6257513761520386

('adam', 0.1, 64, 10)
train_loss: 0.6417051553726196, val_loss: 0.6613218784332275

('adam', 0.1, 64, 50)
train_loss: 0.5739873647689819, val_loss: 0.6316512227058411

('adam', 0.1, 64, 100)
train_loss: 0.5070165395736694, val_loss: 0.628303050994873


---------
BEST MODEL
('adam', 0.1, 32, 100)
val_loss: 0.6257513761520386
---------

Run from 2023-04-10 13:23:46.525263
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7461707592010498, val_loss: 0.7501504421234131

('adam', 0.001, 8, 50)
train_loss: 0.6804236173629761, val_loss: 0.6761490106582642

('adam', 0.001, 8, 100)
train_loss: 0.7187582850456238, val_loss: 0.7288374900817871

('adam', 0.001, 16, 10)
train_loss: 0.696775496006012, val_loss: 0.6852232813835144

('adam', 0.001, 16, 50)
train_loss: 0.6786454319953918, val_loss: 0.6731821894645691

('adam', 0.001, 16, 100)
train_loss: 0.6803101301193237, val_loss: 0.6760178804397583

('adam', 0.001, 32, 10)
train_loss: 0.6878060102462769, val_loss: 0.6867647171020508

('adam', 0.001, 32, 50)
train_loss: 0.6809524893760681, val_loss: 0.6720116138458252

('adam', 0.001, 32, 100)
train_loss: 0.6752268075942993, val_loss: 0.666684091091156

('adam', 0.001, 64, 10)
train_loss: 0.6817681193351746, val_loss: 0.6771761178970337

('adam', 0.001, 64, 50)
train_loss: 0.689773678779602, val_loss: 0.6791139841079712

('adam', 0.001, 64, 100)
train_loss: 0.6780436038970947, val_loss: 0.6723767518997192

('adam', 0.01, 8, 10)
train_loss: 0.6735876798629761, val_loss: 0.6648463010787964

('adam', 0.01, 8, 50)
train_loss: 0.6589693427085876, val_loss: 0.6417161822319031

('adam', 0.01, 8, 100)
train_loss: 0.6891893148422241, val_loss: 0.6875249147415161

('adam', 0.01, 16, 10)
train_loss: 0.6794561743736267, val_loss: 0.674444317817688

('adam', 0.01, 16, 50)
train_loss: 0.6657216548919678, val_loss: 0.6525201201438904

('adam', 0.01, 16, 100)
train_loss: 0.650120735168457, val_loss: 0.6284009218215942

('adam', 0.01, 32, 10)
train_loss: 0.7196184396743774, val_loss: 0.7260297536849976

('adam', 0.01, 32, 50)
train_loss: 0.7150391936302185, val_loss: 0.7293597459793091

('adam', 0.01, 32, 100)
train_loss: 0.7023335695266724, val_loss: 0.7097917795181274

('adam', 0.01, 64, 10)
train_loss: 0.6817865967750549, val_loss: 0.6708394289016724

('adam', 0.01, 64, 50)
train_loss: 0.7165283560752869, val_loss: 0.7290012836456299

('adam', 0.01, 64, 100)
train_loss: 0.688724160194397, val_loss: 0.6883578300476074

('adam', 0.1, 8, 10)
train_loss: 0.6666216254234314, val_loss: 0.651492714881897

('adam', 0.1, 8, 50)
train_loss: 0.5979835987091064, val_loss: 0.5377128720283508

('adam', 0.1, 8, 100)
train_loss: 0.5810344815254211, val_loss: 0.5167428851127625

('adam', 0.1, 16, 10)
train_loss: 0.6926832795143127, val_loss: 0.6865553259849548

('adam', 0.1, 16, 50)
train_loss: 0.6107242107391357, val_loss: 0.565529465675354

('adam', 0.1, 16, 100)
train_loss: 0.5862247347831726, val_loss: 0.5304882526397705

('adam', 0.1, 32, 10)
train_loss: 0.6965340971946716, val_loss: 0.6969289183616638

('adam', 0.1, 32, 50)
train_loss: 0.6073074340820312, val_loss: 0.5600214600563049

('adam', 0.1, 32, 100)
train_loss: 0.6029504537582397, val_loss: 0.5550198554992676

('adam', 0.1, 64, 10)
train_loss: 0.6995887756347656, val_loss: 0.7019851207733154

('adam', 0.1, 64, 50)
train_loss: 0.6064794063568115, val_loss: 0.5599129796028137

('adam', 0.1, 64, 100)
train_loss: 0.6090252995491028, val_loss: 0.5640716552734375


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5167428851127625
---------

Run from 2023-04-10 14:53:20.081412
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7143312096595764, val_loss: 0.7172350287437439

('adam', 0.001, 8, 50)
train_loss: 0.7343423366546631, val_loss: 0.742073655128479

('adam', 0.001, 8, 100)
train_loss: 0.6903347373008728, val_loss: 0.6908023357391357

('adam', 0.001, 16, 10)
train_loss: 0.686338484287262, val_loss: 0.6764200329780579

('adam', 0.001, 16, 50)
train_loss: 0.6920973658561707, val_loss: 0.6923427581787109

('adam', 0.001, 16, 100)
train_loss: 0.675572395324707, val_loss: 0.669477641582489

('adam', 0.001, 32, 10)
train_loss: 0.6885700225830078, val_loss: 0.6878269910812378

('adam', 0.001, 32, 50)
train_loss: 0.6769241094589233, val_loss: 0.6715835332870483

('adam', 0.001, 32, 100)
train_loss: 0.6911764144897461, val_loss: 0.6915760636329651

('adam', 0.001, 64, 10)
train_loss: 0.7877131104469299, val_loss: 0.7943940758705139

('adam', 0.001, 64, 50)
train_loss: 0.6768022775650024, val_loss: 0.6696614027023315

('adam', 0.001, 64, 100)
train_loss: 0.6779200434684753, val_loss: 0.6728492975234985

('adam', 0.01, 8, 10)
train_loss: 0.7296804189682007, val_loss: 0.7396738529205322

('adam', 0.01, 8, 50)
train_loss: 0.6483880281448364, val_loss: 0.623661458492279

('adam', 0.01, 8, 100)
train_loss: 0.6791538596153259, val_loss: 0.673687219619751

('adam', 0.01, 16, 10)
train_loss: 0.7346615195274353, val_loss: 0.7438623905181885

('adam', 0.01, 16, 50)
train_loss: 0.6517448425292969, val_loss: 0.6302300095558167

('adam', 0.01, 16, 100)
train_loss: 0.6673953533172607, val_loss: 0.6556631922721863

('adam', 0.01, 32, 10)
train_loss: 0.6734885573387146, val_loss: 0.65972900390625

('adam', 0.01, 32, 50)
train_loss: 0.6644680500030518, val_loss: 0.651300311088562

('adam', 0.01, 32, 100)
train_loss: 0.6748207807540894, val_loss: 0.6674506664276123

('adam', 0.01, 64, 10)
train_loss: 0.7259368896484375, val_loss: 0.7303760647773743

('adam', 0.01, 64, 50)
train_loss: 0.6602055430412292, val_loss: 0.6444574594497681

('adam', 0.01, 64, 100)
train_loss: 0.6942697763442993, val_loss: 0.6969538331031799

('adam', 0.1, 8, 10)
train_loss: 0.6748867034912109, val_loss: 0.6521773338317871

('adam', 0.1, 8, 50)
train_loss: 0.5877610445022583, val_loss: 0.4910638928413391

('adam', 0.1, 8, 100)
train_loss: 0.5798665285110474, val_loss: 0.4428510367870331

('adam', 0.1, 16, 10)
train_loss: 0.6776888370513916, val_loss: 0.6623579859733582

('adam', 0.1, 16, 50)
train_loss: 0.6010730266571045, val_loss: 0.5229252576828003

('adam', 0.1, 16, 100)
train_loss: 0.5885242819786072, val_loss: 0.48296090960502625

('adam', 0.1, 32, 10)
train_loss: 0.6934299468994141, val_loss: 0.6915683746337891

('adam', 0.1, 32, 50)
train_loss: 0.6000059247016907, val_loss: 0.5239421725273132

('adam', 0.1, 32, 100)
train_loss: 0.5915567874908447, val_loss: 0.5020972490310669

('adam', 0.1, 64, 10)
train_loss: 0.7007522583007812, val_loss: 0.7030795216560364

('adam', 0.1, 64, 50)
train_loss: 0.6081054210662842, val_loss: 0.5435132384300232

('adam', 0.1, 64, 100)
train_loss: 0.5855346918106079, val_loss: 0.483146607875824


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.4428510367870331
---------

Run from 2023-04-17 10:07:23.233612
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7911632657051086, val_loss: 0.7971632480621338

('adam', 0.001, 8, 50)
train_loss: 0.7074005007743835, val_loss: 0.7097757458686829

('adam', 0.001, 8, 100)
train_loss: 0.6744993925094604, val_loss: 0.6723427772521973

('adam', 0.001, 16, 10)
train_loss: 0.6783939599990845, val_loss: 0.6788678169250488

('adam', 0.001, 16, 50)
train_loss: 0.687140166759491, val_loss: 0.6870347857475281

('adam', 0.001, 16, 100)
train_loss: 0.6838264465332031, val_loss: 0.6833187937736511

('adam', 0.001, 32, 10)
train_loss: 0.6801140308380127, val_loss: 0.6811607480049133

('adam', 0.001, 32, 50)
train_loss: 0.6814886927604675, val_loss: 0.6808398365974426

('adam', 0.001, 32, 100)
train_loss: 0.6910073757171631, val_loss: 0.6915025115013123

('adam', 0.001, 64, 10)
train_loss: 0.7053500413894653, val_loss: 0.7058660984039307

('adam', 0.001, 64, 50)
train_loss: 0.7444642186164856, val_loss: 0.7478155493736267

('adam', 0.001, 64, 100)
train_loss: 0.6957388520240784, val_loss: 0.6970863342285156

('adam', 0.01, 8, 10)
train_loss: 0.7042866945266724, val_loss: 0.7075086236000061

('adam', 0.01, 8, 50)
train_loss: 0.6875841617584229, val_loss: 0.6881847977638245

('adam', 0.01, 8, 100)
train_loss: 0.6236008405685425, val_loss: 0.6071786880493164

('adam', 0.01, 16, 10)
train_loss: 0.6713329553604126, val_loss: 0.6683750152587891

('adam', 0.01, 16, 50)
train_loss: 0.6527450084686279, val_loss: 0.6440064311027527

('adam', 0.01, 16, 100)
train_loss: 0.6612945795059204, val_loss: 0.6545249819755554

('adam', 0.01, 32, 10)
train_loss: 0.7418888211250305, val_loss: 0.7452373504638672

('adam', 0.01, 32, 50)
train_loss: 0.6529039740562439, val_loss: 0.6444644331932068

('adam', 0.01, 32, 100)
train_loss: 0.6705690026283264, val_loss: 0.6666190028190613

('adam', 0.01, 64, 10)
train_loss: 0.7029668688774109, val_loss: 0.7043041586875916

('adam', 0.01, 64, 50)
train_loss: 0.692902684211731, val_loss: 0.6943820118904114

('adam', 0.01, 64, 100)
train_loss: 0.6593669652938843, val_loss: 0.6512834429740906

('adam', 0.1, 8, 10)
train_loss: 0.6494060158729553, val_loss: 0.6351191401481628

('adam', 0.1, 8, 50)
train_loss: 0.5781909227371216, val_loss: 0.5468013286590576

('adam', 0.1, 8, 100)
train_loss: 0.5418892502784729, val_loss: 0.4898146688938141

('adam', 0.1, 16, 10)
train_loss: 0.6437867879867554, val_loss: 0.631985604763031

('adam', 0.1, 16, 50)
train_loss: 0.5728069543838501, val_loss: 0.536010205745697

('adam', 0.1, 16, 100)
train_loss: 0.5489844679832458, val_loss: 0.4984400272369385

('adam', 0.1, 32, 10)
train_loss: 0.7015419006347656, val_loss: 0.7024553418159485

('adam', 0.1, 32, 50)
train_loss: 0.587981641292572, val_loss: 0.5568879246711731

('adam', 0.1, 32, 100)
train_loss: 0.5491696000099182, val_loss: 0.5032581090927124

('adam', 0.1, 64, 10)
train_loss: 0.6542617678642273, val_loss: 0.6431983709335327

('adam', 0.1, 64, 50)
train_loss: 0.6003430485725403, val_loss: 0.5730269551277161

('adam', 0.1, 64, 100)
train_loss: 0.5508233308792114, val_loss: 0.5031834244728088

('sgd', 0.001, 8, 10)
train_loss: 0.6848210692405701, val_loss: 0.6870489716529846

('sgd', 0.001, 8, 50)
train_loss: 0.6788105368614197, val_loss: 0.6796920895576477

('sgd', 0.001, 8, 100)
train_loss: 0.6767915487289429, val_loss: 0.6772952079772949

('sgd', 0.001, 16, 10)
train_loss: 0.6878776550292969, val_loss: 0.6877840161323547

('sgd', 0.001, 16, 50)
train_loss: 0.6836007237434387, val_loss: 0.6856300234794617

('sgd', 0.001, 16, 100)
train_loss: 0.6783488392829895, val_loss: 0.6790899634361267

('sgd', 0.001, 32, 10)
train_loss: 0.7009667754173279, val_loss: 0.701225996017456

('sgd', 0.001, 32, 50)
train_loss: 0.7753309607505798, val_loss: 0.7801609039306641

('sgd', 0.001, 32, 100)
train_loss: 0.7123143672943115, val_loss: 0.7132830023765564

('sgd', 0.001, 64, 10)
train_loss: 0.7052593231201172, val_loss: 0.7056968808174133

('sgd', 0.001, 64, 50)
train_loss: 0.6824663877487183, val_loss: 0.6842557787895203

('sgd', 0.001, 64, 100)
train_loss: 0.6916983723640442, val_loss: 0.6916854381561279

('sgd', 0.01, 8, 10)
train_loss: 0.7024242281913757, val_loss: 0.7031311988830566

('sgd', 0.01, 8, 50)
train_loss: 0.6673873662948608, val_loss: 0.6637038588523865

('sgd', 0.01, 8, 100)
train_loss: 0.663369357585907, val_loss: 0.6573271751403809

('sgd', 0.01, 16, 10)
train_loss: 0.7412794828414917, val_loss: 0.7440517544746399

('sgd', 0.01, 16, 50)
train_loss: 0.748950719833374, val_loss: 0.7558806538581848

('sgd', 0.01, 16, 100)
train_loss: 0.7367154359817505, val_loss: 0.7454623579978943

('sgd', 0.01, 32, 10)
train_loss: 0.6799885034561157, val_loss: 0.6810228228569031

('sgd', 0.01, 32, 50)
train_loss: 0.687529444694519, val_loss: 0.6873558163642883

('sgd', 0.01, 32, 100)
train_loss: 0.7134658098220825, val_loss: 0.7163505554199219

('sgd', 0.01, 64, 10)
train_loss: 0.7394884824752808, val_loss: 0.7418116927146912

('sgd', 0.01, 64, 50)
train_loss: 0.6848811507225037, val_loss: 0.6846993565559387

('sgd', 0.01, 64, 100)
train_loss: 0.6766015291213989, val_loss: 0.6760643124580383

('sgd', 0.1, 8, 10)
train_loss: 0.6609782576560974, val_loss: 0.6530570387840271

('sgd', 0.1, 8, 50)
train_loss: 0.6898044347763062, val_loss: 0.6898687481880188

('sgd', 0.1, 8, 100)
train_loss: 0.6557947397232056, val_loss: 0.6456477046012878

('sgd', 0.1, 16, 10)
train_loss: 0.683456540107727, val_loss: 0.6825847625732422

('sgd', 0.1, 16, 50)
train_loss: 0.7153106331825256, val_loss: 0.7209978103637695

('sgd', 0.1, 16, 100)
train_loss: 0.6495775580406189, val_loss: 0.6382346153259277

('sgd', 0.1, 32, 10)
train_loss: 0.6967290639877319, val_loss: 0.6975166201591492

('sgd', 0.1, 32, 50)
train_loss: 0.6823464632034302, val_loss: 0.6807700991630554

('sgd', 0.1, 32, 100)
train_loss: 0.6951887607574463, val_loss: 0.6958355903625488

('sgd', 0.1, 64, 10)
train_loss: 0.6782373785972595, val_loss: 0.6775478720664978

('sgd', 0.1, 64, 50)
train_loss: 0.6767016053199768, val_loss: 0.6745672821998596

('sgd', 0.1, 64, 100)
train_loss: 0.6872423887252808, val_loss: 0.6873520016670227


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.4898146688938141
---------

Run from 2023-04-17 17:03:43.284883
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6905966997146606, val_loss: 0.6904277801513672

('adam', 0.001, 8, 50)
train_loss: 0.6677795648574829, val_loss: 0.6667954921722412

('adam', 0.001, 8, 100)
train_loss: 0.701863706111908, val_loss: 0.7046269178390503

('adam', 0.001, 16, 10)
train_loss: 0.6835000514984131, val_loss: 0.6909639239311218

('adam', 0.001, 16, 50)
train_loss: 0.7370272278785706, val_loss: 0.7431825399398804

('adam', 0.001, 16, 100)
train_loss: 0.6824131011962891, val_loss: 0.6818853616714478

('adam', 0.001, 32, 10)
train_loss: 0.754703164100647, val_loss: 0.761070966720581

('adam', 0.001, 32, 50)
train_loss: 0.6717517971992493, val_loss: 0.6720592379570007

('adam', 0.001, 32, 100)
train_loss: 0.6745374202728271, val_loss: 0.673279345035553

('adam', 0.001, 64, 10)
train_loss: 0.6757392883300781, val_loss: 0.6774410009384155

('adam', 0.001, 64, 50)
train_loss: 0.6831493377685547, val_loss: 0.6821556091308594

('adam', 0.001, 64, 100)
train_loss: 0.7243366241455078, val_loss: 0.7284504175186157

('adam', 0.01, 8, 10)
train_loss: 0.6967092752456665, val_loss: 0.6984050869941711

('adam', 0.01, 8, 50)
train_loss: 0.692829966545105, val_loss: 0.6945122480392456

('adam', 0.01, 8, 100)
train_loss: 0.6193514466285706, val_loss: 0.6000003218650818

('adam', 0.01, 16, 10)
train_loss: 0.6580846905708313, val_loss: 0.6557063460350037

('adam', 0.01, 16, 50)
train_loss: 0.6812583208084106, val_loss: 0.6804893612861633

('adam', 0.01, 16, 100)
train_loss: 0.645224928855896, val_loss: 0.6354126930236816

('adam', 0.01, 32, 10)
train_loss: 0.6897707581520081, val_loss: 0.690308153629303

('adam', 0.01, 32, 50)
train_loss: 0.6457217931747437, val_loss: 0.6362723708152771

('adam', 0.01, 32, 100)
train_loss: 0.6941874623298645, val_loss: 0.696290135383606

('adam', 0.01, 64, 10)
train_loss: 0.7898045182228088, val_loss: 0.8003072142601013

('adam', 0.01, 64, 50)
train_loss: 0.6527870893478394, val_loss: 0.6462177038192749

('adam', 0.01, 64, 100)
train_loss: 0.722368597984314, val_loss: 0.7302649021148682

('adam', 0.1, 8, 10)
train_loss: 0.623938262462616, val_loss: 0.6020545959472656

('adam', 0.1, 8, 50)
train_loss: 0.5482320785522461, val_loss: 0.49116837978363037

('adam', 0.1, 8, 100)
train_loss: 0.5235549807548523, val_loss: 0.41300082206726074

('adam', 0.1, 16, 10)
train_loss: 0.6825575232505798, val_loss: 0.6768813729286194

('adam', 0.1, 16, 50)
train_loss: 0.5484058260917664, val_loss: 0.4900500774383545

('adam', 0.1, 16, 100)
train_loss: 0.5256855487823486, val_loss: 0.4408559203147888

('adam', 0.1, 32, 10)
train_loss: 0.712968647480011, val_loss: 0.7146976590156555

('adam', 0.1, 32, 50)
train_loss: 0.5966619849205017, val_loss: 0.5672695636749268

('adam', 0.1, 32, 100)
train_loss: 0.545157253742218, val_loss: 0.4850754141807556

('adam', 0.1, 64, 10)
train_loss: 0.717217743396759, val_loss: 0.7228863835334778

('adam', 0.1, 64, 50)
train_loss: 0.5785631537437439, val_loss: 0.5386654138565063

('adam', 0.1, 64, 100)
train_loss: 0.5321630835533142, val_loss: 0.4540560841560364

('sgd', 0.001, 8, 10)
train_loss: 0.6774187684059143, val_loss: 0.6813132762908936

('sgd', 0.001, 8, 50)
train_loss: 0.7093173265457153, val_loss: 0.7107188701629639

('sgd', 0.001, 8, 100)
train_loss: 0.6788012981414795, val_loss: 0.6785345077514648

('sgd', 0.001, 16, 10)
train_loss: 0.7615277171134949, val_loss: 0.7687423229217529

('sgd', 0.001, 16, 50)
train_loss: 0.7796993851661682, val_loss: 0.7898827791213989

('sgd', 0.001, 16, 100)
train_loss: 0.674156129360199, val_loss: 0.6764103770256042

('sgd', 0.001, 32, 10)
train_loss: 0.6801003217697144, val_loss: 0.680293619632721

('sgd', 0.001, 32, 50)
train_loss: 0.803796648979187, val_loss: 0.8171054720878601

('sgd', 0.001, 32, 100)
train_loss: 0.705339789390564, val_loss: 0.7062700986862183

('sgd', 0.001, 64, 10)
train_loss: 0.6771824955940247, val_loss: 0.6806333065032959

('sgd', 0.001, 64, 50)
train_loss: 0.6762416958808899, val_loss: 0.6787795424461365

('sgd', 0.001, 64, 100)
train_loss: 0.7191933989524841, val_loss: 0.7214025259017944

('sgd', 0.01, 8, 10)
train_loss: 0.694108247756958, val_loss: 0.6941649317741394

('sgd', 0.01, 8, 50)
train_loss: 0.671679675579071, val_loss: 0.6690791845321655

('sgd', 0.01, 8, 100)
train_loss: 0.6752658486366272, val_loss: 0.6729256510734558

('sgd', 0.01, 16, 10)
train_loss: 0.6911340355873108, val_loss: 0.6910855174064636

('sgd', 0.01, 16, 50)
train_loss: 0.6709766983985901, val_loss: 0.6697208881378174

('sgd', 0.01, 16, 100)
train_loss: 0.6618738174438477, val_loss: 0.6584685444831848

('sgd', 0.01, 32, 10)
train_loss: 0.6776801943778992, val_loss: 0.6781013011932373

('sgd', 0.01, 32, 50)
train_loss: 0.7172802090644836, val_loss: 0.720183253288269

('sgd', 0.01, 32, 100)
train_loss: 0.6634842157363892, val_loss: 0.6633318662643433

('sgd', 0.01, 64, 10)
train_loss: 0.6829259991645813, val_loss: 0.6898102164268494

('sgd', 0.01, 64, 50)
train_loss: 0.7578395009040833, val_loss: 0.7655524611473083

('sgd', 0.01, 64, 100)
train_loss: 0.7134689092636108, val_loss: 0.7159451842308044

('sgd', 0.1, 8, 10)
train_loss: 0.6841758489608765, val_loss: 0.6819736957550049

('sgd', 0.1, 8, 50)
train_loss: 0.6718015670776367, val_loss: 0.6651833057403564

('sgd', 0.1, 8, 100)
train_loss: 0.65728759765625, val_loss: 0.645086944103241

('sgd', 0.1, 16, 10)
train_loss: 0.7117512822151184, val_loss: 0.7149282693862915

('sgd', 0.1, 16, 50)
train_loss: 0.6660547852516174, val_loss: 0.6594843864440918

('sgd', 0.1, 16, 100)
train_loss: 0.6759708523750305, val_loss: 0.6735337972640991

('sgd', 0.1, 32, 10)
train_loss: 0.6738375425338745, val_loss: 0.6725871562957764

('sgd', 0.1, 32, 50)
train_loss: 0.7306207418441772, val_loss: 0.7397572994232178

('sgd', 0.1, 32, 100)
train_loss: 0.6558781862258911, val_loss: 0.6483715772628784

('sgd', 0.1, 64, 10)
train_loss: 0.6771457195281982, val_loss: 0.6767197847366333

('sgd', 0.1, 64, 50)
train_loss: 0.659277617931366, val_loss: 0.6547540426254272

('sgd', 0.1, 64, 100)
train_loss: 0.685540497303009, val_loss: 0.6854667067527771


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.41300082206726074
---------
