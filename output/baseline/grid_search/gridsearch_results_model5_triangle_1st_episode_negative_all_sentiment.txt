
Run from 2023-04-02 10:57:52.385381
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6921173930168152, val_loss: 0.6974010467529297

('adam', 0.01, 50, 200)
train_loss: 0.6921244263648987, val_loss: 0.7007673382759094

('adam', 0.01, 50, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989232897758484

('adam', 0.01, 100, 100)
train_loss: 0.6922105550765991, val_loss: 0.6955645084381104

('adam', 0.01, 100, 200)
train_loss: 0.6920937299728394, val_loss: 0.6989189982414246

('adam', 0.01, 100, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989190578460693

('adam', 0.01, 200, 100)
train_loss: 0.6943498253822327, val_loss: 0.7171592116355896

('adam', 0.01, 200, 200)
train_loss: 0.6920937299728394, val_loss: 0.6989192962646484

('adam', 0.01, 200, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989191174507141

('adam', 0.1, 50, 100)
train_loss: 0.6920943260192871, val_loss: 0.69866943359375

('adam', 0.1, 50, 200)
train_loss: 0.6920937895774841, val_loss: 0.6989195346832275

('adam', 0.1, 50, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989190578460693

('adam', 0.1, 100, 100)
train_loss: 0.6920937299728394, val_loss: 0.6989137530326843

('adam', 0.1, 100, 200)
train_loss: 0.6920937299728394, val_loss: 0.6989187598228455

('adam', 0.1, 100, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989190578460693

('adam', 0.1, 200, 100)
train_loss: 0.6920937299728394, val_loss: 0.6988881826400757

('adam', 0.1, 200, 200)
train_loss: 0.6920937299728394, val_loss: 0.6989201903343201

('adam', 0.1, 200, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989191174507141

('adam', 0.2, 50, 100)
train_loss: 0.6920939683914185, val_loss: 0.6990096569061279

('adam', 0.2, 50, 200)
train_loss: 0.6920937895774841, val_loss: 0.6989178657531738

('adam', 0.2, 50, 300)
train_loss: 0.6920937895774841, val_loss: 0.6989190578460693

('adam', 0.2, 100, 100)
train_loss: 0.692094087600708, val_loss: 0.6987870931625366

('adam', 0.2, 100, 200)
train_loss: 0.6920937299728394, val_loss: 0.6989177465438843

('adam', 0.2, 100, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989190578460693

('adam', 0.2, 200, 100)
train_loss: 0.692094087600708, val_loss: 0.6990777254104614

('adam', 0.2, 200, 200)
train_loss: 0.6920937895774841, val_loss: 0.6989198923110962

('adam', 0.2, 200, 300)
train_loss: 0.6920937299728394, val_loss: 0.6989190578460693


---------
BEST MODEL
('adam', 0.01, 100, 100)
val_loss: 0.6955645084381104
---------

Run from 2023-04-05 15:26:35.089895
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7178497910499573, val_loss: 0.682745099067688

('adam', 0.001, 8, 50)
train_loss: 0.6949533820152283, val_loss: 0.7110605239868164

('adam', 0.001, 8, 100)
train_loss: 0.6932488083839417, val_loss: 0.6890258193016052

('adam', 0.001, 16, 10)
train_loss: 0.6931793689727783, val_loss: 0.6959428787231445

('adam', 0.001, 16, 50)
train_loss: 0.6935572028160095, val_loss: 0.6877561807632446

('adam', 0.001, 16, 100)
train_loss: 0.7049227952957153, val_loss: 0.7402780055999756

('adam', 0.001, 32, 10)
train_loss: 0.7208551168441772, val_loss: 0.7654416561126709

('adam', 0.001, 32, 50)
train_loss: 0.6932029724121094, val_loss: 0.7017080783843994

('adam', 0.001, 32, 100)
train_loss: 0.6938042640686035, val_loss: 0.7083253264427185

('adam', 0.001, 64, 10)
train_loss: 0.7208976149559021, val_loss: 0.7649804949760437

('adam', 0.001, 64, 50)
train_loss: 0.6938379406929016, val_loss: 0.687125563621521

('adam', 0.001, 64, 100)
train_loss: 0.7063470482826233, val_loss: 0.677111804485321

('adam', 0.01, 8, 10)
train_loss: 0.6927346587181091, val_loss: 0.7030081748962402

('adam', 0.01, 8, 50)
train_loss: 0.6947675943374634, val_loss: 0.7177557945251465

('adam', 0.01, 8, 100)
train_loss: 0.6923115253448486, val_loss: 0.698161244392395

('adam', 0.01, 16, 10)
train_loss: 0.7040026783943176, val_loss: 0.6780816912651062

('adam', 0.01, 16, 50)
train_loss: 0.6989986896514893, val_loss: 0.6798003315925598

('adam', 0.01, 16, 100)
train_loss: 0.6933416724205017, val_loss: 0.689697265625

('adam', 0.01, 32, 10)
train_loss: 0.7177462577819824, val_loss: 0.6777955293655396

('adam', 0.01, 32, 50)
train_loss: 0.6924377679824829, val_loss: 0.6952922940254211

('adam', 0.01, 32, 100)
train_loss: 0.6921528577804565, val_loss: 0.7018047571182251

('adam', 0.01, 64, 10)
train_loss: 0.7099335193634033, val_loss: 0.7490501999855042

('adam', 0.01, 64, 50)
train_loss: 0.6923768520355225, val_loss: 0.7047106027603149

('adam', 0.01, 64, 100)
train_loss: 0.694617748260498, val_loss: 0.6852686405181885

('adam', 0.1, 8, 10)
train_loss: 0.6946967244148254, val_loss: 0.7018144130706787

('adam', 0.1, 8, 50)
train_loss: 0.6956120133399963, val_loss: 0.7012362480163574

('adam', 0.1, 8, 100)
train_loss: 0.6925225853919983, val_loss: 0.6938351392745972

('adam', 0.1, 16, 10)
train_loss: 0.6931901574134827, val_loss: 0.7020581364631653

('adam', 0.1, 16, 50)
train_loss: 0.6958032846450806, val_loss: 0.7098255157470703

('adam', 0.1, 16, 100)
train_loss: 0.7027762532234192, val_loss: 0.7026931047439575

('adam', 0.1, 32, 10)
train_loss: 0.6925010681152344, val_loss: 0.7009420394897461

('adam', 0.1, 32, 50)
train_loss: 0.6921571493148804, val_loss: 0.6989288926124573

('adam', 0.1, 32, 100)
train_loss: 0.693021833896637, val_loss: 0.69637131690979

('adam', 0.1, 64, 10)
train_loss: 0.6978598833084106, val_loss: 0.6824183464050293

('adam', 0.1, 64, 50)
train_loss: 0.692131757736206, val_loss: 0.7010225057601929

('adam', 0.1, 64, 100)
train_loss: 0.6920937299728394, val_loss: 0.698962926864624


---------
BEST MODEL
('adam', 0.001, 64, 100)
val_loss: 0.677111804485321
---------

Run from 2023-04-05 16:27:18.814575
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.717755913734436, val_loss: 0.7600765228271484

('adam', 0.001, 8, 50)
train_loss: 0.7054240703582764, val_loss: 0.6781477928161621

('adam', 0.001, 8, 100)
train_loss: 0.6932996511459351, val_loss: 0.708012044429779

('adam', 0.001, 16, 10)
train_loss: 0.6931207180023193, val_loss: 0.696692705154419

('adam', 0.001, 16, 50)
train_loss: 0.6948541402816772, val_loss: 0.6844120025634766

('adam', 0.001, 16, 100)
train_loss: 0.7112323641777039, val_loss: 0.7564305067062378

('adam', 0.001, 32, 10)
train_loss: 0.6943662762641907, val_loss: 0.7026668190956116

('adam', 0.001, 32, 50)
train_loss: 0.693687915802002, val_loss: 0.7039988040924072

('adam', 0.001, 32, 100)
train_loss: 0.6927741169929504, val_loss: 0.6914676427841187

('adam', 0.001, 64, 10)
train_loss: 0.6980053782463074, val_loss: 0.7152353525161743

('adam', 0.001, 64, 50)
train_loss: 0.7108367085456848, val_loss: 0.7485518455505371

('adam', 0.001, 64, 100)
train_loss: 0.7077361345291138, val_loss: 0.7456105947494507

('adam', 0.01, 8, 10)
train_loss: 0.6933583617210388, val_loss: 0.705919086933136

('adam', 0.01, 8, 50)
train_loss: 0.6982402205467224, val_loss: 0.6798571944236755

('adam', 0.01, 8, 100)
train_loss: 0.6936189532279968, val_loss: 0.6963869333267212

('adam', 0.01, 16, 10)
train_loss: 0.7005882859230042, val_loss: 0.727440357208252

('adam', 0.01, 16, 50)
train_loss: 0.692169725894928, val_loss: 0.7014153003692627

('adam', 0.01, 16, 100)
train_loss: 0.6953157186508179, val_loss: 0.7182967662811279

('adam', 0.01, 32, 10)
train_loss: 0.6936059594154358, val_loss: 0.6879041194915771

('adam', 0.01, 32, 50)
train_loss: 0.6920918226242065, val_loss: 0.699618935585022

('adam', 0.01, 32, 100)
train_loss: 0.6921483874320984, val_loss: 0.6975618600845337

('adam', 0.01, 64, 10)
train_loss: 0.7015552520751953, val_loss: 0.7306930422782898

('adam', 0.01, 64, 50)
train_loss: 0.692094087600708, val_loss: 0.6991109848022461

('adam', 0.01, 64, 100)
train_loss: 0.6936787962913513, val_loss: 0.6876497268676758

('adam', 0.1, 8, 10)
train_loss: 0.7062105536460876, val_loss: 0.7024522423744202

('adam', 0.1, 8, 50)
train_loss: 0.6934043169021606, val_loss: 0.6975357532501221

('adam', 0.1, 8, 100)
train_loss: 0.6930503249168396, val_loss: 0.6960675716400146

('adam', 0.1, 16, 10)
train_loss: 0.6942064166069031, val_loss: 0.6921247243881226

('adam', 0.1, 16, 50)
train_loss: 0.6927105188369751, val_loss: 0.7016109228134155

('adam', 0.1, 16, 100)
train_loss: 0.6925455331802368, val_loss: 0.6958053112030029

('adam', 0.1, 32, 10)
train_loss: 0.6932185292243958, val_loss: 0.6977345943450928

('adam', 0.1, 32, 50)
train_loss: 0.6938529014587402, val_loss: 0.703627347946167

('adam', 0.1, 32, 100)
train_loss: 0.6929174661636353, val_loss: 0.6963063478469849

('adam', 0.1, 64, 10)
train_loss: 0.6954984068870544, val_loss: 0.6869969367980957

('adam', 0.1, 64, 50)
train_loss: 0.6921444535255432, val_loss: 0.7008411288261414

('adam', 0.1, 64, 100)
train_loss: 0.6920937299728394, val_loss: 0.6989566087722778


---------
BEST MODEL
('adam', 0.001, 8, 50)
val_loss: 0.6781477928161621
---------

Run from 2023-04-10 09:57:20.474554
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6994107961654663, val_loss: 0.7193490266799927

('adam', 0.001, 8, 50)
train_loss: 0.7147112488746643, val_loss: 0.6796278357505798

('adam', 0.001, 8, 100)
train_loss: 0.6948621869087219, val_loss: 0.6849808692932129

('adam', 0.001, 16, 10)
train_loss: 0.7158955335617065, val_loss: 0.7565105557441711

('adam', 0.001, 16, 50)
train_loss: 0.7041967511177063, val_loss: 0.6797606945037842

('adam', 0.001, 16, 100)
train_loss: 0.6965301036834717, val_loss: 0.682819664478302

('adam', 0.001, 32, 10)
train_loss: 0.7117123603820801, val_loss: 0.6810693740844727

('adam', 0.001, 32, 50)
train_loss: 0.6929782629013062, val_loss: 0.6907440423965454

('adam', 0.001, 32, 100)
train_loss: 0.6958364844322205, val_loss: 0.6831374764442444

('adam', 0.001, 64, 10)
train_loss: 0.7041219472885132, val_loss: 0.6799932718276978

('adam', 0.001, 64, 50)
train_loss: 0.6965173482894897, val_loss: 0.7145966291427612

('adam', 0.001, 64, 100)
train_loss: 0.700752317905426, val_loss: 0.6782575845718384

('adam', 0.01, 8, 10)
train_loss: 0.699969470500946, val_loss: 0.7313995957374573

('adam', 0.01, 8, 50)
train_loss: 0.6930981278419495, val_loss: 0.707737922668457

('adam', 0.01, 8, 100)
train_loss: 0.6936159729957581, val_loss: 0.6931123733520508

('adam', 0.01, 16, 10)
train_loss: 0.707093358039856, val_loss: 0.6777262687683105

('adam', 0.01, 16, 50)
train_loss: 0.6975818872451782, val_loss: 0.681065559387207

('adam', 0.01, 16, 100)
train_loss: 0.6926776766777039, val_loss: 0.7042145729064941

('adam', 0.01, 32, 10)
train_loss: 0.692403256893158, val_loss: 0.6932803988456726

('adam', 0.01, 32, 50)
train_loss: 0.6984570026397705, val_loss: 0.7324886322021484

('adam', 0.01, 32, 100)
train_loss: 0.6952470541000366, val_loss: 0.6837301254272461

('adam', 0.01, 64, 10)
train_loss: 0.7204993963241577, val_loss: 0.7697072625160217

('adam', 0.01, 64, 50)
train_loss: 0.706257700920105, val_loss: 0.6753928661346436

('adam', 0.01, 64, 100)
train_loss: 0.6966267228126526, val_loss: 0.7263782024383545

('adam', 0.1, 8, 10)
train_loss: 0.6979087591171265, val_loss: 0.7068049311637878

('adam', 0.1, 8, 50)
train_loss: 0.6945449709892273, val_loss: 0.7005187273025513

('adam', 0.1, 8, 100)
train_loss: 0.7023255228996277, val_loss: 0.7099722623825073

('adam', 0.1, 16, 10)
train_loss: 0.689725935459137, val_loss: 0.6991857290267944

('adam', 0.1, 16, 50)
train_loss: 0.6943437457084656, val_loss: 0.7055267095565796

('adam', 0.1, 16, 100)
train_loss: 0.699135422706604, val_loss: 0.6996198296546936

('adam', 0.1, 32, 10)
train_loss: 0.69236159324646, val_loss: 0.6949964761734009

('adam', 0.1, 32, 50)
train_loss: 0.6925860643386841, val_loss: 0.7026228904724121

('adam', 0.1, 32, 100)
train_loss: 0.6930477023124695, val_loss: 0.6991723775863647

('adam', 0.1, 64, 10)
train_loss: 0.6991163492202759, val_loss: 0.6807560920715332

('adam', 0.1, 64, 50)
train_loss: 0.6920966506004333, val_loss: 0.6984460353851318

('adam', 0.1, 64, 100)
train_loss: 0.6920937895774841, val_loss: 0.6989032626152039


---------
BEST MODEL
('adam', 0.01, 64, 50)
val_loss: 0.6753928661346436
---------

Run from 2023-04-10 12:56:34.733523
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6989829540252686, val_loss: 0.6754422187805176

('adam', 0.001, 8, 50)
train_loss: 0.685030460357666, val_loss: 0.7388718128204346

('adam', 0.001, 8, 100)
train_loss: 0.6964199542999268, val_loss: 0.6822457313537598

('adam', 0.001, 16, 10)
train_loss: 0.6883471608161926, val_loss: 0.7679121494293213

('adam', 0.001, 16, 50)
train_loss: 0.6851428747177124, val_loss: 0.7743477821350098

('adam', 0.001, 16, 100)
train_loss: 0.6844981908798218, val_loss: 0.7371203899383545

('adam', 0.001, 32, 10)
train_loss: 0.6893228888511658, val_loss: 0.7125550508499146

('adam', 0.001, 32, 50)
train_loss: 0.6856032609939575, val_loss: 0.7403647899627686

('adam', 0.001, 32, 100)
train_loss: 0.6864927411079407, val_loss: 0.812160074710846

('adam', 0.001, 64, 10)
train_loss: 0.6960656046867371, val_loss: 0.8304808139801025

('adam', 0.001, 64, 50)
train_loss: 0.6855849623680115, val_loss: 0.7524536848068237

('adam', 0.001, 64, 100)
train_loss: 0.7182316780090332, val_loss: 0.6420314311981201

('adam', 0.01, 8, 10)
train_loss: 0.6822032928466797, val_loss: 0.7741450071334839

('adam', 0.01, 8, 50)
train_loss: 0.6804841756820679, val_loss: 0.78387850522995

('adam', 0.01, 8, 100)
train_loss: 0.6819065809249878, val_loss: 0.7738391757011414

('adam', 0.01, 16, 10)
train_loss: 0.7078916430473328, val_loss: 0.6596946716308594

('adam', 0.01, 16, 50)
train_loss: 0.6805305480957031, val_loss: 0.7879164218902588

('adam', 0.01, 16, 100)
train_loss: 0.6878558993339539, val_loss: 0.714326024055481

('adam', 0.01, 32, 10)
train_loss: 0.6837007403373718, val_loss: 0.7607918977737427

('adam', 0.01, 32, 50)
train_loss: 0.6915501356124878, val_loss: 0.6972514986991882

('adam', 0.01, 32, 100)
train_loss: 0.689474880695343, val_loss: 0.7053141593933105

('adam', 0.01, 64, 10)
train_loss: 0.6852483153343201, val_loss: 0.7297004461288452

('adam', 0.01, 64, 50)
train_loss: 0.6803586483001709, val_loss: 0.7785021662712097

('adam', 0.01, 64, 100)
train_loss: 0.6848597526550293, val_loss: 0.7270002365112305

('adam', 0.1, 8, 10)
train_loss: 0.6807398796081543, val_loss: 0.7874976396560669

('adam', 0.1, 8, 50)
train_loss: 0.6888014078140259, val_loss: 0.7701289653778076

('adam', 0.1, 8, 100)
train_loss: 0.6807283759117126, val_loss: 0.7950085401535034

('adam', 0.1, 16, 10)
train_loss: 0.6812137365341187, val_loss: 0.7913955450057983

('adam', 0.1, 16, 50)
train_loss: 0.6847487688064575, val_loss: 0.8231797814369202

('adam', 0.1, 16, 100)
train_loss: 0.6818943619728088, val_loss: 0.8052945137023926

('adam', 0.1, 32, 10)
train_loss: 0.6828014850616455, val_loss: 0.7652760744094849

('adam', 0.1, 32, 50)
train_loss: 0.6806934475898743, val_loss: 0.7858245372772217

('adam', 0.1, 32, 100)
train_loss: 0.6818093657493591, val_loss: 0.7730758190155029

('adam', 0.1, 64, 10)
train_loss: 0.6936366558074951, val_loss: 0.6959272623062134

('adam', 0.1, 64, 50)
train_loss: 0.6805064678192139, val_loss: 0.807093620300293

('adam', 0.1, 64, 100)
train_loss: 0.6802214980125427, val_loss: 0.7893168926239014


---------
BEST MODEL
('adam', 0.001, 64, 100)
val_loss: 0.6420314311981201
---------

Run from 2023-04-10 14:16:28.514399
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6973514556884766, val_loss: 0.6847162246704102

('adam', 0.001, 8, 50)
train_loss: 0.6925234794616699, val_loss: 0.6880514621734619

('adam', 0.001, 8, 100)
train_loss: 0.7015344500541687, val_loss: 0.6851797103881836

('adam', 0.001, 16, 10)
train_loss: 0.7023089528083801, val_loss: 0.7225388884544373

('adam', 0.001, 16, 50)
train_loss: 0.6924731731414795, val_loss: 0.6884030103683472

('adam', 0.001, 16, 100)
train_loss: 0.6952276825904846, val_loss: 0.7068034410476685

('adam', 0.001, 32, 10)
train_loss: 0.7001636028289795, val_loss: 0.7159749269485474

('adam', 0.001, 32, 50)
train_loss: 0.7002905011177063, val_loss: 0.719523549079895

('adam', 0.001, 32, 100)
train_loss: 0.6993168592453003, val_loss: 0.7193078398704529

('adam', 0.001, 64, 10)
train_loss: 0.6970250010490417, val_loss: 0.7074849605560303

('adam', 0.001, 64, 50)
train_loss: 0.6941255331039429, val_loss: 0.6838370561599731

('adam', 0.001, 64, 100)
train_loss: 0.7165326476097107, val_loss: 0.7617946863174438

('adam', 0.01, 8, 10)
train_loss: 0.6963305473327637, val_loss: 0.7124767303466797

('adam', 0.01, 8, 50)
train_loss: 0.6927743554115295, val_loss: 0.697532057762146

('adam', 0.01, 8, 100)
train_loss: 0.6930784583091736, val_loss: 0.6930426359176636

('adam', 0.01, 16, 10)
train_loss: 0.695746898651123, val_loss: 0.7088810801506042

('adam', 0.01, 16, 50)
train_loss: 0.692274808883667, val_loss: 0.6920492649078369

('adam', 0.01, 16, 100)
train_loss: 0.6927217245101929, val_loss: 0.6903969049453735

('adam', 0.01, 32, 10)
train_loss: 0.6934196949005127, val_loss: 0.6974519491195679

('adam', 0.01, 32, 50)
train_loss: 0.692503035068512, val_loss: 0.6878818273544312

('adam', 0.01, 32, 100)
train_loss: 0.6928095817565918, val_loss: 0.6874368190765381

('adam', 0.01, 64, 10)
train_loss: 0.6929978728294373, val_loss: 0.6983586549758911

('adam', 0.01, 64, 50)
train_loss: 0.6930229067802429, val_loss: 0.6992994546890259

('adam', 0.01, 64, 100)
train_loss: 0.6944184303283691, val_loss: 0.7063523530960083

('adam', 0.1, 8, 10)
train_loss: 0.6935699582099915, val_loss: 0.6895939707756042

('adam', 0.1, 8, 50)
train_loss: 0.6967964172363281, val_loss: 0.6921106576919556

('adam', 0.1, 8, 100)
train_loss: 0.6925680041313171, val_loss: 0.6973990201950073

('adam', 0.1, 16, 10)
train_loss: 0.7022653222084045, val_loss: 0.728718638420105

('adam', 0.1, 16, 50)
train_loss: 0.6943212747573853, val_loss: 0.6952236890792847

('adam', 0.1, 16, 100)
train_loss: 0.7016786336898804, val_loss: 0.6912673711776733

('adam', 0.1, 32, 10)
train_loss: 0.6931533217430115, val_loss: 0.687249481678009

('adam', 0.1, 32, 50)
train_loss: 0.6925207376480103, val_loss: 0.6925367116928101

('adam', 0.1, 32, 100)
train_loss: 0.6926594376564026, val_loss: 0.6928945779800415

('adam', 0.1, 64, 10)
train_loss: 0.6926357746124268, val_loss: 0.6898919343948364

('adam', 0.1, 64, 50)
train_loss: 0.692311704158783, val_loss: 0.6907721757888794

('adam', 0.1, 64, 100)
train_loss: 0.6922463774681091, val_loss: 0.6912972927093506


---------
BEST MODEL
('adam', 0.001, 64, 50)
val_loss: 0.6838370561599731
---------

Run from 2023-04-17 09:52:48.143676
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7179251909255981, val_loss: 0.6986260414123535

('adam', 0.001, 8, 50)
train_loss: 0.6915206909179688, val_loss: 0.6985719203948975

('adam', 0.001, 8, 100)
train_loss: 0.7080122828483582, val_loss: 0.6936694979667664

('adam', 0.001, 16, 10)
train_loss: 0.6927492618560791, val_loss: 0.7030755877494812

('adam', 0.001, 16, 50)
train_loss: 0.7286973595619202, val_loss: 0.7028172612190247

('adam', 0.001, 16, 100)
train_loss: 0.7077839374542236, val_loss: 0.693556010723114

('adam', 0.001, 32, 10)
train_loss: 0.6970643401145935, val_loss: 0.7119429111480713

('adam', 0.001, 32, 50)
train_loss: 0.6913837790489197, val_loss: 0.697029173374176

('adam', 0.001, 32, 100)
train_loss: 0.6959194540977478, val_loss: 0.7099587321281433

('adam', 0.001, 64, 10)
train_loss: 0.691926121711731, val_loss: 0.6944417953491211

('adam', 0.001, 64, 50)
train_loss: 0.6922949552536011, val_loss: 0.7017880082130432

('adam', 0.001, 64, 100)
train_loss: 0.6926500797271729, val_loss: 0.6935744285583496

('adam', 0.01, 8, 10)
train_loss: 0.6921143531799316, val_loss: 0.7003431916236877

('adam', 0.01, 8, 50)
train_loss: 0.6915930509567261, val_loss: 0.6966473460197449

('adam', 0.01, 8, 100)
train_loss: 0.6919607520103455, val_loss: 0.6971545219421387

('adam', 0.01, 16, 10)
train_loss: 0.6925610899925232, val_loss: 0.6939809322357178

('adam', 0.01, 16, 50)
train_loss: 0.6915543675422668, val_loss: 0.6978049874305725

('adam', 0.01, 16, 100)
train_loss: 0.6915757656097412, val_loss: 0.6984699368476868

('adam', 0.01, 32, 10)
train_loss: 0.7041128277778625, val_loss: 0.692531406879425

('adam', 0.01, 32, 50)
train_loss: 0.6915761828422546, val_loss: 0.6961005330085754

('adam', 0.01, 32, 100)
train_loss: 0.69156813621521, val_loss: 0.6979592442512512

('adam', 0.01, 64, 10)
train_loss: 0.6925000548362732, val_loss: 0.7016814351081848

('adam', 0.01, 64, 50)
train_loss: 0.6924997568130493, val_loss: 0.7021211981773376

('adam', 0.01, 64, 100)
train_loss: 0.6913814544677734, val_loss: 0.6970503330230713

('adam', 0.1, 8, 10)
train_loss: 0.6959298849105835, val_loss: 0.6998708844184875

('adam', 0.1, 8, 50)
train_loss: 0.6920278072357178, val_loss: 0.6973316669464111

('adam', 0.1, 8, 100)
train_loss: 0.6922211050987244, val_loss: 0.6967166066169739

('adam', 0.1, 16, 10)
train_loss: 0.6924080848693848, val_loss: 0.6969251036643982

('adam', 0.1, 16, 50)
train_loss: 0.6915675401687622, val_loss: 0.6970353722572327

('adam', 0.1, 16, 100)
train_loss: 0.6923574209213257, val_loss: 0.6962518692016602

('adam', 0.1, 32, 10)
train_loss: 0.6925761699676514, val_loss: 0.6966876983642578

('adam', 0.1, 32, 50)
train_loss: 0.6914775371551514, val_loss: 0.6972765922546387

('adam', 0.1, 32, 100)
train_loss: 0.6915059089660645, val_loss: 0.6969470977783203

('adam', 0.1, 64, 10)
train_loss: 0.691840648651123, val_loss: 0.6979551911354065

('adam', 0.1, 64, 50)
train_loss: 0.6913852095603943, val_loss: 0.6969701647758484

('adam', 0.1, 64, 100)
train_loss: 0.691382884979248, val_loss: 0.6972053647041321

('sgd', 0.001, 8, 10)
train_loss: 0.6938772797584534, val_loss: 0.6927540898323059

('sgd', 0.001, 8, 50)
train_loss: 0.6918333768844604, val_loss: 0.694671630859375

('sgd', 0.001, 8, 100)
train_loss: 0.7076116800308228, val_loss: 0.69414883852005

('sgd', 0.001, 16, 10)
train_loss: 0.7142333984375, val_loss: 0.6973340511322021

('sgd', 0.001, 16, 50)
train_loss: 0.7401434183120728, val_loss: 0.710496187210083

('sgd', 0.001, 16, 100)
train_loss: 0.7243106961250305, val_loss: 0.701699435710907

('sgd', 0.001, 32, 10)
train_loss: 0.7088572978973389, val_loss: 0.6951429843902588

('sgd', 0.001, 32, 50)
train_loss: 0.6914846301078796, val_loss: 0.6958351135253906

('sgd', 0.001, 32, 100)
train_loss: 0.6960354447364807, val_loss: 0.6922073364257812

('sgd', 0.001, 64, 10)
train_loss: 0.6935383081436157, val_loss: 0.6929149031639099

('sgd', 0.001, 64, 50)
train_loss: 0.6969621777534485, val_loss: 0.6922292709350586

('sgd', 0.001, 64, 100)
train_loss: 0.7186166048049927, val_loss: 0.6992724537849426

('sgd', 0.01, 8, 10)
train_loss: 0.6915271282196045, val_loss: 0.6980893015861511

('sgd', 0.01, 8, 50)
train_loss: 0.6920823454856873, val_loss: 0.6991563439369202

('sgd', 0.01, 8, 100)
train_loss: 0.7016054391860962, val_loss: 0.7183544039726257

('sgd', 0.01, 16, 10)
train_loss: 0.6972389221191406, val_loss: 0.7122633457183838

('sgd', 0.01, 16, 50)
train_loss: 0.6982519626617432, val_loss: 0.7138870358467102

('sgd', 0.01, 16, 100)
train_loss: 0.7120468020439148, val_loss: 0.694983184337616

('sgd', 0.01, 32, 10)
train_loss: 0.7416872382164001, val_loss: 0.7113187909126282

('sgd', 0.01, 32, 50)
train_loss: 0.6916823387145996, val_loss: 0.6995450854301453

('sgd', 0.01, 32, 100)
train_loss: 0.6942474246025085, val_loss: 0.6923243403434753

('sgd', 0.01, 64, 10)
train_loss: 0.6927021145820618, val_loss: 0.7029587626457214

('sgd', 0.01, 64, 50)
train_loss: 0.7072383165359497, val_loss: 0.6941940188407898

('sgd', 0.01, 64, 100)
train_loss: 0.703611433506012, val_loss: 0.7226125597953796

('sgd', 0.1, 8, 10)
train_loss: 0.6930724382400513, val_loss: 0.6942686438560486

('sgd', 0.1, 8, 50)
train_loss: 0.6991041898727417, val_loss: 0.6923500895500183

('sgd', 0.1, 8, 100)
train_loss: 0.6932812333106995, val_loss: 0.6960809826850891

('sgd', 0.1, 16, 10)
train_loss: 0.7157847881317139, val_loss: 0.6961431503295898

('sgd', 0.1, 16, 50)
train_loss: 0.6928433179855347, val_loss: 0.6943849921226501

('sgd', 0.1, 16, 100)
train_loss: 0.6945199370384216, val_loss: 0.693043053150177

('sgd', 0.1, 32, 10)
train_loss: 0.7066883444786072, val_loss: 0.725227415561676

('sgd', 0.1, 32, 50)
train_loss: 0.6946097016334534, val_loss: 0.6926477551460266

('sgd', 0.1, 32, 100)
train_loss: 0.7026038765907288, val_loss: 0.6933128833770752

('sgd', 0.1, 64, 10)
train_loss: 0.6934283971786499, val_loss: 0.6928276419639587

('sgd', 0.1, 64, 50)
train_loss: 0.6915366649627686, val_loss: 0.695543110370636

('sgd', 0.1, 64, 100)
train_loss: 0.6914194226264954, val_loss: 0.6978554725646973


---------
BEST MODEL
('sgd', 0.001, 32, 100)
val_loss: 0.6922073364257812
---------

Run from 2023-04-17 16:29:26.316983
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7044457197189331, val_loss: 0.703789472579956

('adam', 0.001, 8, 50)
train_loss: 0.694892406463623, val_loss: 0.6998434066772461

('adam', 0.001, 8, 100)
train_loss: 0.7062821984291077, val_loss: 0.7268527746200562

('adam', 0.001, 16, 10)
train_loss: 0.6937671899795532, val_loss: 0.6893296241760254

('adam', 0.001, 16, 50)
train_loss: 0.6984302401542664, val_loss: 0.6937713027000427

('adam', 0.001, 16, 100)
train_loss: 0.6952957510948181, val_loss: 0.7015101313591003

('adam', 0.001, 32, 10)
train_loss: 0.6985057592391968, val_loss: 0.7068443298339844

('adam', 0.001, 32, 50)
train_loss: 0.6998777985572815, val_loss: 0.6950872540473938

('adam', 0.001, 32, 100)
train_loss: 0.6925661563873291, val_loss: 0.6888065934181213

('adam', 0.001, 64, 10)
train_loss: 0.6924307346343994, val_loss: 0.6911412477493286

('adam', 0.001, 64, 50)
train_loss: 0.7159762382507324, val_loss: 0.7458510994911194

('adam', 0.001, 64, 100)
train_loss: 0.694611132144928, val_loss: 0.6998920440673828

('adam', 0.01, 8, 10)
train_loss: 0.6988268494606018, val_loss: 0.709542453289032

('adam', 0.01, 8, 50)
train_loss: 0.6924189925193787, val_loss: 0.6907160878181458

('adam', 0.01, 8, 100)
train_loss: 0.6924730539321899, val_loss: 0.6892211437225342

('adam', 0.01, 16, 10)
train_loss: 0.6929537057876587, val_loss: 0.6941446661949158

('adam', 0.01, 16, 50)
train_loss: 0.6925695538520813, val_loss: 0.6893173456192017

('adam', 0.01, 16, 100)
train_loss: 0.6934877038002014, val_loss: 0.6970033049583435

('adam', 0.01, 32, 10)
train_loss: 0.7045873403549194, val_loss: 0.7006223797798157

('adam', 0.01, 32, 50)
train_loss: 0.6947054266929626, val_loss: 0.6996155977249146

('adam', 0.01, 32, 100)
train_loss: 0.6933676600456238, val_loss: 0.6966903209686279

('adam', 0.01, 64, 10)
train_loss: 0.6949623823165894, val_loss: 0.7006415724754333

('adam', 0.01, 64, 50)
train_loss: 0.7038127183914185, val_loss: 0.7196229100227356

('adam', 0.01, 64, 100)
train_loss: 0.6921766996383667, val_loss: 0.6907527446746826

('adam', 0.1, 8, 10)
train_loss: 0.7006139159202576, val_loss: 0.7049208283424377

('adam', 0.1, 8, 50)
train_loss: 0.6916047930717468, val_loss: 0.6953537464141846

('adam', 0.1, 8, 100)
train_loss: 0.6974297165870667, val_loss: 0.6980388164520264

('adam', 0.1, 16, 10)
train_loss: 0.6927725672721863, val_loss: 0.6918762922286987

('adam', 0.1, 16, 50)
train_loss: 0.6922210454940796, val_loss: 0.6907868981361389

('adam', 0.1, 16, 100)
train_loss: 0.6926233768463135, val_loss: 0.6899497509002686

('adam', 0.1, 32, 10)
train_loss: 0.6925861835479736, val_loss: 0.6898053884506226

('adam', 0.1, 32, 50)
train_loss: 0.6923190951347351, val_loss: 0.6902797818183899

('adam', 0.1, 32, 100)
train_loss: 0.6924851536750793, val_loss: 0.6909517049789429

('adam', 0.1, 64, 10)
train_loss: 0.6923402547836304, val_loss: 0.6909784078598022

('adam', 0.1, 64, 50)
train_loss: 0.6921970248222351, val_loss: 0.6911913156509399

('adam', 0.1, 64, 100)
train_loss: 0.6921766996383667, val_loss: 0.6907722353935242

('sgd', 0.001, 8, 10)
train_loss: 0.6928658485412598, val_loss: 0.6889365911483765

('sgd', 0.001, 8, 50)
train_loss: 0.7074642181396484, val_loss: 0.7270304560661316

('sgd', 0.001, 8, 100)
train_loss: 0.692934513092041, val_loss: 0.6887751817703247

('sgd', 0.001, 16, 10)
train_loss: 0.6957213282585144, val_loss: 0.7000030279159546

('sgd', 0.001, 16, 50)
train_loss: 0.692325234413147, val_loss: 0.6891419887542725

('sgd', 0.001, 16, 100)
train_loss: 0.7078130841255188, val_loss: 0.7277818918228149

('sgd', 0.001, 32, 10)
train_loss: 0.6933009028434753, val_loss: 0.6891001462936401

('sgd', 0.001, 32, 50)
train_loss: 0.6952568292617798, val_loss: 0.6989163756370544

('sgd', 0.001, 32, 100)
train_loss: 0.6951011419296265, val_loss: 0.6908210515975952

('sgd', 0.001, 64, 10)
train_loss: 0.7081781625747681, val_loss: 0.7102274894714355

('sgd', 0.001, 64, 50)
train_loss: 0.6966882944107056, val_loss: 0.7023510336875916

('sgd', 0.001, 64, 100)
train_loss: 0.7033838033676147, val_loss: 0.7025404572486877

('sgd', 0.01, 8, 10)
train_loss: 0.7042438983917236, val_loss: 0.7022581100463867

('sgd', 0.01, 8, 50)
train_loss: 0.6935127377510071, val_loss: 0.6878721714019775

('sgd', 0.01, 8, 100)
train_loss: 0.6969748139381409, val_loss: 0.6887998580932617

('sgd', 0.01, 16, 10)
train_loss: 0.7073252201080322, val_loss: 0.7080436944961548

('sgd', 0.01, 16, 50)
train_loss: 0.6922687292098999, val_loss: 0.6902483701705933

('sgd', 0.01, 16, 100)
train_loss: 0.6929471492767334, val_loss: 0.6942293643951416

('sgd', 0.01, 32, 10)
train_loss: 0.6995720267295837, val_loss: 0.6966985464096069

('sgd', 0.01, 32, 50)
train_loss: 0.693315327167511, val_loss: 0.6889405250549316

('sgd', 0.01, 32, 100)
train_loss: 0.693504273891449, val_loss: 0.6887458562850952

('sgd', 0.01, 64, 10)
train_loss: 0.6924145817756653, val_loss: 0.6889241337776184

('sgd', 0.01, 64, 50)
train_loss: 0.6959603428840637, val_loss: 0.6916396021842957

('sgd', 0.01, 64, 100)
train_loss: 0.6923247575759888, val_loss: 0.6891621351242065

('sgd', 0.1, 8, 10)
train_loss: 0.6967127323150635, val_loss: 0.7003165483474731

('sgd', 0.1, 8, 50)
train_loss: 0.6934313774108887, val_loss: 0.6922638416290283

('sgd', 0.1, 8, 100)
train_loss: 0.696047842502594, val_loss: 0.7025306224822998

('sgd', 0.1, 16, 10)
train_loss: 0.6956372261047363, val_loss: 0.7000074982643127

('sgd', 0.1, 16, 50)
train_loss: 0.7009936571121216, val_loss: 0.6909084320068359

('sgd', 0.1, 16, 100)
train_loss: 0.6938229203224182, val_loss: 0.6888121962547302

('sgd', 0.1, 32, 10)
train_loss: 0.6988593935966492, val_loss: 0.6939009428024292

('sgd', 0.1, 32, 50)
train_loss: 0.6925697326660156, val_loss: 0.6930469870567322

('sgd', 0.1, 32, 100)
train_loss: 0.695634126663208, val_loss: 0.6884214878082275

('sgd', 0.1, 64, 10)
train_loss: 0.7185166478157043, val_loss: 0.7508761882781982

('sgd', 0.1, 64, 50)
train_loss: 0.7060952186584473, val_loss: 0.7263147234916687

('sgd', 0.1, 64, 100)
train_loss: 0.6948819756507874, val_loss: 0.7010937333106995


---------
BEST MODEL
('sgd', 0.01, 8, 50)
val_loss: 0.6878721714019775
---------
