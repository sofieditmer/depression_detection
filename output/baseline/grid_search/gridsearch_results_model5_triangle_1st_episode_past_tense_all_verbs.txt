
Run from 2023-04-02 10:55:46.465159
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6984050273895264, val_loss: 0.7185481786727905

('adam', 0.01, 50, 200)
train_loss: 0.6907652020454407, val_loss: 0.6843258142471313

('adam', 0.01, 50, 300)
train_loss: 0.6895155906677246, val_loss: 0.6679577827453613

('adam', 0.01, 100, 100)
train_loss: 0.6912487745285034, val_loss: 0.6874961853027344

('adam', 0.01, 100, 200)
train_loss: 0.6895143985748291, val_loss: 0.6683946847915649

('adam', 0.01, 100, 300)
train_loss: 0.6896154880523682, val_loss: 0.6725727319717407

('adam', 0.01, 200, 100)
train_loss: 0.7003494501113892, val_loss: 0.7251826524734497

('adam', 0.01, 200, 200)
train_loss: 0.6895143985748291, val_loss: 0.6684201955795288

('adam', 0.01, 200, 300)
train_loss: 0.6895143985748291, val_loss: 0.6683955192565918

('adam', 0.1, 50, 100)
train_loss: 0.6895154714584351, val_loss: 0.668777346611023

('adam', 0.1, 50, 200)
train_loss: 0.6895143389701843, val_loss: 0.6683962941169739

('adam', 0.1, 50, 300)
train_loss: 0.6895143985748291, val_loss: 0.6683957576751709

('adam', 0.1, 100, 100)
train_loss: 0.6895148754119873, val_loss: 0.6684448719024658

('adam', 0.1, 100, 200)
train_loss: 0.6895143985748291, val_loss: 0.6683954000473022

('adam', 0.1, 100, 300)
train_loss: 0.6895143389701843, val_loss: 0.6683958172798157

('adam', 0.1, 200, 100)
train_loss: 0.6895152926445007, val_loss: 0.6687655448913574

('adam', 0.1, 200, 200)
train_loss: 0.6895144581794739, val_loss: 0.6683946847915649

('adam', 0.1, 200, 300)
train_loss: 0.6895143985748291, val_loss: 0.6683957576751709

('adam', 0.2, 50, 100)
train_loss: 0.6895144581794739, val_loss: 0.668437123298645

('adam', 0.2, 50, 200)
train_loss: 0.6895143389701843, val_loss: 0.6683954000473022

('adam', 0.2, 50, 300)
train_loss: 0.6895143985748291, val_loss: 0.6683957576751709

('adam', 0.2, 100, 100)
train_loss: 0.6895149350166321, val_loss: 0.6686757802963257

('adam', 0.2, 100, 200)
train_loss: 0.6895143985748291, val_loss: 0.6683951616287231

('adam', 0.2, 100, 300)
train_loss: 0.6895143985748291, val_loss: 0.6683958172798157

('adam', 0.2, 200, 100)
train_loss: 0.6895144581794739, val_loss: 0.6683707237243652

('adam', 0.2, 200, 200)
train_loss: 0.6895143985748291, val_loss: 0.6683958768844604

('adam', 0.2, 200, 300)
train_loss: 0.6895143389701843, val_loss: 0.6683957576751709


---------
BEST MODEL
('adam', 0.01, 50, 300)
val_loss: 0.6679577827453613
---------

Run from 2023-04-05 15:22:28.453236
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6993080973625183, val_loss: 0.6703540086746216

('adam', 0.001, 8, 50)
train_loss: 0.6946917772293091, val_loss: 0.706611156463623

('adam', 0.001, 8, 100)
train_loss: 0.7054773569107056, val_loss: 0.7488176226615906

('adam', 0.001, 16, 10)
train_loss: 0.7099127769470215, val_loss: 0.7579175233840942

('adam', 0.001, 16, 50)
train_loss: 0.697053074836731, val_loss: 0.6671182513237

('adam', 0.001, 16, 100)
train_loss: 0.6922914981842041, val_loss: 0.6843801736831665

('adam', 0.001, 32, 10)
train_loss: 0.6945955753326416, val_loss: 0.7062057256698608

('adam', 0.001, 32, 50)
train_loss: 0.6951125860214233, val_loss: 0.6718928813934326

('adam', 0.001, 32, 100)
train_loss: 0.6953799724578857, val_loss: 0.7101844549179077

('adam', 0.001, 64, 10)
train_loss: 0.7123993039131165, val_loss: 0.7645092010498047

('adam', 0.001, 64, 50)
train_loss: 0.6984540820121765, val_loss: 0.7235307097434998

('adam', 0.001, 64, 100)
train_loss: 0.6951370239257812, val_loss: 0.7089327573776245

('adam', 0.01, 8, 10)
train_loss: 0.6960798501968384, val_loss: 0.7115395069122314

('adam', 0.01, 8, 50)
train_loss: 0.693088173866272, val_loss: 0.6958187818527222

('adam', 0.01, 8, 100)
train_loss: 0.693534791469574, val_loss: 0.6964994668960571

('adam', 0.01, 16, 10)
train_loss: 0.6980355978012085, val_loss: 0.6600364446640015

('adam', 0.01, 16, 50)
train_loss: 0.6943904161453247, val_loss: 0.7053344249725342

('adam', 0.01, 16, 100)
train_loss: 0.690595805644989, val_loss: 0.6772151589393616

('adam', 0.01, 32, 10)
train_loss: 0.6950539946556091, val_loss: 0.7079260945320129

('adam', 0.01, 32, 50)
train_loss: 0.6907495856285095, val_loss: 0.6792048811912537

('adam', 0.01, 32, 100)
train_loss: 0.6905075907707214, val_loss: 0.6747426986694336

('adam', 0.01, 64, 10)
train_loss: 0.6916817426681519, val_loss: 0.6879652738571167

('adam', 0.01, 64, 50)
train_loss: 0.6906741857528687, val_loss: 0.6787136197090149

('adam', 0.01, 64, 100)
train_loss: 0.6904637813568115, val_loss: 0.67337965965271

('adam', 0.1, 8, 10)
train_loss: 0.6924870014190674, val_loss: 0.6890310049057007

('adam', 0.1, 8, 50)
train_loss: 0.6929752230644226, val_loss: 0.6756483316421509

('adam', 0.1, 8, 100)
train_loss: 0.6938657164573669, val_loss: 0.6793524026870728

('adam', 0.1, 16, 10)
train_loss: 0.6974456906318665, val_loss: 0.6935703754425049

('adam', 0.1, 16, 50)
train_loss: 0.6981528997421265, val_loss: 0.6767655611038208

('adam', 0.1, 16, 100)
train_loss: 0.6900076270103455, val_loss: 0.6701302528381348

('adam', 0.1, 32, 10)
train_loss: 0.6949939727783203, val_loss: 0.6876503229141235

('adam', 0.1, 32, 50)
train_loss: 0.6907354593276978, val_loss: 0.6758165955543518

('adam', 0.1, 32, 100)
train_loss: 0.6917449831962585, val_loss: 0.6697739362716675

('adam', 0.1, 64, 10)
train_loss: 0.6905549764633179, val_loss: 0.6742157936096191

('adam', 0.1, 64, 50)
train_loss: 0.6905402541160583, val_loss: 0.6689102649688721

('adam', 0.1, 64, 100)
train_loss: 0.6904574632644653, val_loss: 0.6724436283111572


---------
BEST MODEL
('adam', 0.01, 16, 10)
val_loss: 0.6600364446640015
---------

Run from 2023-04-05 16:22:34.223498
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7072717547416687, val_loss: 0.6675523519515991

('adam', 0.001, 8, 50)
train_loss: 0.6934078335762024, val_loss: 0.6714079976081848

('adam', 0.001, 8, 100)
train_loss: 0.692604124546051, val_loss: 0.695279598236084

('adam', 0.001, 16, 10)
train_loss: 0.702247142791748, val_loss: 0.735797643661499

('adam', 0.001, 16, 50)
train_loss: 0.694599449634552, val_loss: 0.6738905906677246

('adam', 0.001, 16, 100)
train_loss: 0.6934394836425781, val_loss: 0.6980984210968018

('adam', 0.001, 32, 10)
train_loss: 0.7109701037406921, val_loss: 0.7608602046966553

('adam', 0.001, 32, 50)
train_loss: 0.6925653219223022, val_loss: 0.6915395259857178

('adam', 0.001, 32, 100)
train_loss: 0.7070206999778748, val_loss: 0.7529678344726562

('adam', 0.001, 64, 10)
train_loss: 0.6980321407318115, val_loss: 0.6731957793235779

('adam', 0.001, 64, 50)
train_loss: 0.7113890647888184, val_loss: 0.7637683153152466

('adam', 0.001, 64, 100)
train_loss: 0.6934746503829956, val_loss: 0.7005642056465149

('adam', 0.01, 8, 10)
train_loss: 0.6968479752540588, val_loss: 0.7160755395889282

('adam', 0.01, 8, 50)
train_loss: 0.6907896995544434, val_loss: 0.6672571301460266

('adam', 0.01, 8, 100)
train_loss: 0.691016674041748, val_loss: 0.6733496189117432

('adam', 0.01, 16, 10)
train_loss: 0.7040966749191284, val_loss: 0.7434507608413696

('adam', 0.01, 16, 50)
train_loss: 0.6910915970802307, val_loss: 0.683196485042572

('adam', 0.01, 16, 100)
train_loss: 0.6908208727836609, val_loss: 0.6754710674285889

('adam', 0.01, 32, 10)
train_loss: 0.6927415728569031, val_loss: 0.6893752813339233

('adam', 0.01, 32, 50)
train_loss: 0.6906334161758423, val_loss: 0.6770370602607727

('adam', 0.01, 32, 100)
train_loss: 0.6904343962669373, val_loss: 0.6733354330062866

('adam', 0.01, 64, 10)
train_loss: 0.6940160393714905, val_loss: 0.7032053470611572

('adam', 0.01, 64, 50)
train_loss: 0.6907135248184204, val_loss: 0.6794456839561462

('adam', 0.01, 64, 100)
train_loss: 0.690471351146698, val_loss: 0.6738690137863159

('adam', 0.1, 8, 10)
train_loss: 0.6923848986625671, val_loss: 0.6649729013442993

('adam', 0.1, 8, 50)
train_loss: 0.7001084685325623, val_loss: 0.6820098161697388

('adam', 0.1, 8, 100)
train_loss: 0.6899301409721375, val_loss: 0.6706560850143433

('adam', 0.1, 16, 10)
train_loss: 0.6901053786277771, val_loss: 0.6687052249908447

('adam', 0.1, 16, 50)
train_loss: 0.7009410262107849, val_loss: 0.6923390626907349

('adam', 0.1, 16, 100)
train_loss: 0.693480908870697, val_loss: 0.6670899987220764

('adam', 0.1, 32, 10)
train_loss: 0.6924625635147095, val_loss: 0.6683772206306458

('adam', 0.1, 32, 50)
train_loss: 0.6907888650894165, val_loss: 0.6712638735771179

('adam', 0.1, 32, 100)
train_loss: 0.691242516040802, val_loss: 0.6758646368980408

('adam', 0.1, 64, 10)
train_loss: 0.6911654472351074, val_loss: 0.6721023321151733

('adam', 0.1, 64, 50)
train_loss: 0.6904773712158203, val_loss: 0.6742661595344543

('adam', 0.1, 64, 100)
train_loss: 0.6904574036598206, val_loss: 0.6722374558448792


---------
BEST MODEL
('adam', 0.1, 8, 10)
val_loss: 0.6649729013442993
---------

Run from 2023-04-10 09:52:02.035845
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.695168137550354, val_loss: 0.7092969417572021

('adam', 0.001, 8, 50)
train_loss: 0.6974749565124512, val_loss: 0.7194918394088745

('adam', 0.001, 8, 100)
train_loss: 0.7067220211029053, val_loss: 0.7524505853652954

('adam', 0.001, 16, 10)
train_loss: 0.7141740322113037, val_loss: 0.7693722248077393

('adam', 0.001, 16, 50)
train_loss: 0.7102262377738953, val_loss: 0.7610934972763062

('adam', 0.001, 16, 100)
train_loss: 0.6996010541915894, val_loss: 0.7273985147476196

('adam', 0.001, 32, 10)
train_loss: 0.7072385549545288, val_loss: 0.7509778738021851

('adam', 0.001, 32, 50)
train_loss: 0.6927413940429688, val_loss: 0.6792908906936646

('adam', 0.001, 32, 100)
train_loss: 0.7092158794403076, val_loss: 0.7596145868301392

('adam', 0.001, 64, 10)
train_loss: 0.6968380212783813, val_loss: 0.7168227434158325

('adam', 0.001, 64, 50)
train_loss: 0.7006415724754333, val_loss: 0.7316753268241882

('adam', 0.001, 64, 100)
train_loss: 0.6916675567626953, val_loss: 0.6869061589241028

('adam', 0.01, 8, 10)
train_loss: 0.6934959292411804, val_loss: 0.6991523504257202

('adam', 0.01, 8, 50)
train_loss: 0.69300377368927, val_loss: 0.6946360468864441

('adam', 0.01, 8, 100)
train_loss: 0.6926077008247375, val_loss: 0.6965312957763672

('adam', 0.01, 16, 10)
train_loss: 0.693088173866272, val_loss: 0.692339301109314

('adam', 0.01, 16, 50)
train_loss: 0.7028610110282898, val_loss: 0.7278139591217041

('adam', 0.01, 16, 100)
train_loss: 0.6954258680343628, val_loss: 0.7099746465682983

('adam', 0.01, 32, 10)
train_loss: 0.7068886756896973, val_loss: 0.7510318160057068

('adam', 0.01, 32, 50)
train_loss: 0.6909433007240295, val_loss: 0.6805151700973511

('adam', 0.01, 32, 100)
train_loss: 0.691249668598175, val_loss: 0.6831169128417969

('adam', 0.01, 64, 10)
train_loss: 0.6947453022003174, val_loss: 0.668888509273529

('adam', 0.01, 64, 50)
train_loss: 0.6926917433738708, val_loss: 0.6958438754081726

('adam', 0.01, 64, 100)
train_loss: 0.6940977573394775, val_loss: 0.7035516500473022

('adam', 0.1, 8, 10)
train_loss: 0.6920738816261292, val_loss: 0.6764432191848755

('adam', 0.1, 8, 50)
train_loss: 0.6948094367980957, val_loss: 0.684749960899353

('adam', 0.1, 8, 100)
train_loss: 0.6916838884353638, val_loss: 0.686252772808075

('adam', 0.1, 16, 10)
train_loss: 0.6921331286430359, val_loss: 0.6657041907310486

('adam', 0.1, 16, 50)
train_loss: 0.6909264326095581, val_loss: 0.6790077090263367

('adam', 0.1, 16, 100)
train_loss: 0.7111555337905884, val_loss: 0.6767656803131104

('adam', 0.1, 32, 10)
train_loss: 0.6930058002471924, val_loss: 0.6964863538742065

('adam', 0.1, 32, 50)
train_loss: 0.6934220790863037, val_loss: 0.6747232675552368

('adam', 0.1, 32, 100)
train_loss: 0.6912171840667725, val_loss: 0.6719639301300049

('adam', 0.1, 64, 10)
train_loss: 0.6925045251846313, val_loss: 0.6916965246200562

('adam', 0.1, 64, 50)
train_loss: 0.6904813647270203, val_loss: 0.6705754399299622

('adam', 0.1, 64, 100)
train_loss: 0.6904575824737549, val_loss: 0.6723688840866089


---------
BEST MODEL
('adam', 0.1, 16, 10)
val_loss: 0.6657041907310486
---------

Run from 2023-04-10 12:52:51.726334
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6887062191963196, val_loss: 0.6944066286087036

('adam', 0.001, 8, 50)
train_loss: 0.6866685748100281, val_loss: 0.6923761367797852

('adam', 0.001, 8, 100)
train_loss: 0.6851981282234192, val_loss: 0.6917502880096436

('adam', 0.001, 16, 10)
train_loss: 0.6918520331382751, val_loss: 0.6925849914550781

('adam', 0.001, 16, 50)
train_loss: 0.7202338576316833, val_loss: 0.721372663974762

('adam', 0.001, 16, 100)
train_loss: 0.6851334571838379, val_loss: 0.6995774507522583

('adam', 0.001, 32, 10)
train_loss: 0.689375638961792, val_loss: 0.6936430931091309

('adam', 0.001, 32, 50)
train_loss: 0.7031630873680115, val_loss: 0.70168536901474

('adam', 0.001, 32, 100)
train_loss: 0.701123833656311, val_loss: 0.6995817422866821

('adam', 0.001, 64, 10)
train_loss: 0.7273761630058289, val_loss: 0.7307894825935364

('adam', 0.001, 64, 50)
train_loss: 0.692218542098999, val_loss: 0.6932134628295898

('adam', 0.001, 64, 100)
train_loss: 0.686360776424408, val_loss: 0.6918503046035767

('adam', 0.01, 8, 10)
train_loss: 0.6879809498786926, val_loss: 0.6916577816009521

('adam', 0.01, 8, 50)
train_loss: 0.682297945022583, val_loss: 0.6920995712280273

('adam', 0.01, 8, 100)
train_loss: 0.6794838905334473, val_loss: 0.6955291032791138

('adam', 0.01, 16, 10)
train_loss: 0.6843242645263672, val_loss: 0.7006579637527466

('adam', 0.01, 16, 50)
train_loss: 0.6786757707595825, val_loss: 0.695917546749115

('adam', 0.01, 16, 100)
train_loss: 0.6840736865997314, val_loss: 0.6920477151870728

('adam', 0.01, 32, 10)
train_loss: 0.6907138824462891, val_loss: 0.6923111081123352

('adam', 0.01, 32, 50)
train_loss: 0.6846086382865906, val_loss: 0.6914508938789368

('adam', 0.01, 32, 100)
train_loss: 0.689195990562439, val_loss: 0.6919697523117065

('adam', 0.01, 64, 10)
train_loss: 0.6862761974334717, val_loss: 0.698862612247467

('adam', 0.01, 64, 50)
train_loss: 0.7078121304512024, val_loss: 0.7043912410736084

('adam', 0.01, 64, 100)
train_loss: 0.6784403920173645, val_loss: 0.6970479488372803

('adam', 0.1, 8, 10)
train_loss: 0.6824326515197754, val_loss: 0.69908607006073

('adam', 0.1, 8, 50)
train_loss: 0.6812500953674316, val_loss: 0.7043614983558655

('adam', 0.1, 8, 100)
train_loss: 0.6786052584648132, val_loss: 0.6987890005111694

('adam', 0.1, 16, 10)
train_loss: 0.6827706098556519, val_loss: 0.6923912763595581

('adam', 0.1, 16, 50)
train_loss: 0.6841820478439331, val_loss: 0.7084970474243164

('adam', 0.1, 16, 100)
train_loss: 0.6789090037345886, val_loss: 0.7051730155944824

('adam', 0.1, 32, 10)
train_loss: 0.687183678150177, val_loss: 0.6914412975311279

('adam', 0.1, 32, 50)
train_loss: 0.679093599319458, val_loss: 0.7007614374160767

('adam', 0.1, 32, 100)
train_loss: 0.6789390444755554, val_loss: 0.7020447254180908

('adam', 0.1, 64, 10)
train_loss: 0.679716169834137, val_loss: 0.6947954893112183

('adam', 0.1, 64, 50)
train_loss: 0.6778370141983032, val_loss: 0.701781690120697

('adam', 0.1, 64, 100)
train_loss: 0.677818238735199, val_loss: 0.7026643753051758


---------
BEST MODEL
('adam', 0.1, 32, 10)
val_loss: 0.6914412975311279
---------

Run from 2023-04-10 14:11:07.985075
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7170018553733826, val_loss: 0.7113330364227295

('adam', 0.001, 8, 50)
train_loss: 0.6909785270690918, val_loss: 0.6914730072021484

('adam', 0.001, 8, 100)
train_loss: 0.6876988410949707, val_loss: 0.6889089345932007

('adam', 0.001, 16, 10)
train_loss: 0.7141048908233643, val_loss: 0.7093781232833862

('adam', 0.001, 16, 50)
train_loss: 0.6912142038345337, val_loss: 0.6917754411697388

('adam', 0.001, 16, 100)
train_loss: 0.689978837966919, val_loss: 0.6908471584320068

('adam', 0.001, 32, 10)
train_loss: 0.6931240558624268, val_loss: 0.6914582848548889

('adam', 0.001, 32, 50)
train_loss: 0.6959800124168396, val_loss: 0.6920878887176514

('adam', 0.001, 32, 100)
train_loss: 0.6997441053390503, val_loss: 0.6982743740081787

('adam', 0.001, 64, 10)
train_loss: 0.6964749097824097, val_loss: 0.6962392330169678

('adam', 0.001, 64, 50)
train_loss: 0.7021520137786865, val_loss: 0.6997627019882202

('adam', 0.001, 64, 100)
train_loss: 0.6923399567604065, val_loss: 0.6937493085861206

('adam', 0.01, 8, 10)
train_loss: 0.7114503979682922, val_loss: 0.7047860622406006

('adam', 0.01, 8, 50)
train_loss: 0.6866278052330017, val_loss: 0.6899136304855347

('adam', 0.01, 8, 100)
train_loss: 0.6873115301132202, val_loss: 0.6917538642883301

('adam', 0.01, 16, 10)
train_loss: 0.6912121772766113, val_loss: 0.6894363164901733

('adam', 0.01, 16, 50)
train_loss: 0.7057846784591675, val_loss: 0.701301097869873

('adam', 0.01, 16, 100)
train_loss: 0.6864749193191528, val_loss: 0.6921788454055786

('adam', 0.01, 32, 10)
train_loss: 0.7019398212432861, val_loss: 0.6991896629333496

('adam', 0.01, 32, 50)
train_loss: 0.7043579816818237, val_loss: 0.700099766254425

('adam', 0.01, 32, 100)
train_loss: 0.6966143846511841, val_loss: 0.6958339214324951

('adam', 0.01, 64, 10)
train_loss: 0.6916177868843079, val_loss: 0.6933678388595581

('adam', 0.01, 64, 50)
train_loss: 0.7116101980209351, val_loss: 0.7039530277252197

('adam', 0.01, 64, 100)
train_loss: 0.6881881952285767, val_loss: 0.6914810538291931

('adam', 0.1, 8, 10)
train_loss: 0.6906553506851196, val_loss: 0.6900741457939148

('adam', 0.1, 8, 50)
train_loss: 0.6865944862365723, val_loss: 0.6883518695831299

('adam', 0.1, 8, 100)
train_loss: 0.6868602633476257, val_loss: 0.6891100406646729

('adam', 0.1, 16, 10)
train_loss: 0.685656726360321, val_loss: 0.6887688636779785

('adam', 0.1, 16, 50)
train_loss: 0.695335865020752, val_loss: 0.6886275410652161

('adam', 0.1, 16, 100)
train_loss: 0.6864719390869141, val_loss: 0.6887058615684509

('adam', 0.1, 32, 10)
train_loss: 0.6890801191329956, val_loss: 0.6886110901832581

('adam', 0.1, 32, 50)
train_loss: 0.6856057643890381, val_loss: 0.689334511756897

('adam', 0.1, 32, 100)
train_loss: 0.6863179802894592, val_loss: 0.6883156299591064

('adam', 0.1, 64, 10)
train_loss: 0.6899048686027527, val_loss: 0.6969283223152161

('adam', 0.1, 64, 50)
train_loss: 0.6855959296226501, val_loss: 0.6897116303443909

('adam', 0.1, 64, 100)
train_loss: 0.6855683922767639, val_loss: 0.6895727515220642


---------
BEST MODEL
('adam', 0.1, 32, 100)
val_loss: 0.6883156299591064
---------

Run from 2023-04-17 09:50:46.215403
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.698069155216217, val_loss: 0.724719226360321

('adam', 0.001, 8, 50)
train_loss: 0.6925965547561646, val_loss: 0.691644012928009

('adam', 0.001, 8, 100)
train_loss: 0.7057830095291138, val_loss: 0.7650339007377625

('adam', 0.001, 16, 10)
train_loss: 0.692385733127594, val_loss: 0.6726718544960022

('adam', 0.001, 16, 50)
train_loss: 0.6920326948165894, val_loss: 0.658657968044281

('adam', 0.001, 16, 100)
train_loss: 0.6910238862037659, val_loss: 0.6823956966400146

('adam', 0.001, 32, 10)
train_loss: 0.7127652168273926, val_loss: 0.7914403080940247

('adam', 0.001, 32, 50)
train_loss: 0.6921964287757874, val_loss: 0.6620243191719055

('adam', 0.001, 32, 100)
train_loss: 0.6896941661834717, val_loss: 0.665438175201416

('adam', 0.001, 64, 10)
train_loss: 0.6990272402763367, val_loss: 0.6658453941345215

('adam', 0.001, 64, 50)
train_loss: 0.6947279572486877, val_loss: 0.7061588168144226

('adam', 0.001, 64, 100)
train_loss: 0.6946037411689758, val_loss: 0.7054917216300964

('adam', 0.01, 8, 10)
train_loss: 0.688653290271759, val_loss: 0.6599776744842529

('adam', 0.01, 8, 50)
train_loss: 0.6868794560432434, val_loss: 0.650404155254364

('adam', 0.01, 8, 100)
train_loss: 0.6887232661247253, val_loss: 0.6676875948905945

('adam', 0.01, 16, 10)
train_loss: 0.6896105408668518, val_loss: 0.6687720417976379

('adam', 0.01, 16, 50)
train_loss: 0.6996414661407471, val_loss: 0.729309618473053

('adam', 0.01, 16, 100)
train_loss: 0.6875643134117126, val_loss: 0.6594929099082947

('adam', 0.01, 32, 10)
train_loss: 0.6890536546707153, val_loss: 0.6646646857261658

('adam', 0.01, 32, 50)
train_loss: 0.6997612714767456, val_loss: 0.7314882278442383

('adam', 0.01, 32, 100)
train_loss: 0.6920561790466309, val_loss: 0.6895595192909241

('adam', 0.01, 64, 10)
train_loss: 0.6941013336181641, val_loss: 0.7021589279174805

('adam', 0.01, 64, 50)
train_loss: 0.6997553706169128, val_loss: 0.732992947101593

('adam', 0.01, 64, 100)
train_loss: 0.6865978240966797, val_loss: 0.6496428847312927

('adam', 0.1, 8, 10)
train_loss: 0.688482403755188, val_loss: 0.6587299108505249

('adam', 0.1, 8, 50)
train_loss: 0.6871362328529358, val_loss: 0.6454704999923706

('adam', 0.1, 8, 100)
train_loss: 0.6903125643730164, val_loss: 0.6486315131187439

('adam', 0.1, 16, 10)
train_loss: 0.6947869658470154, val_loss: 0.6975016593933105

('adam', 0.1, 16, 50)
train_loss: 0.6878283023834229, val_loss: 0.6470698714256287

('adam', 0.1, 16, 100)
train_loss: 0.6878547668457031, val_loss: 0.644304096698761

('adam', 0.1, 32, 10)
train_loss: 0.6872581243515015, val_loss: 0.6543390154838562

('adam', 0.1, 32, 50)
train_loss: 0.686708390712738, val_loss: 0.6448174715042114

('adam', 0.1, 32, 100)
train_loss: 0.6869875192642212, val_loss: 0.6445444226264954

('adam', 0.1, 64, 10)
train_loss: 0.6905021667480469, val_loss: 0.676342248916626

('adam', 0.1, 64, 50)
train_loss: 0.6864343285560608, val_loss: 0.6431503891944885

('adam', 0.1, 64, 100)
train_loss: 0.6864096522331238, val_loss: 0.6440973281860352

('sgd', 0.001, 8, 10)
train_loss: 0.6952115893363953, val_loss: 0.6667184829711914

('sgd', 0.001, 8, 50)
train_loss: 0.6935309171676636, val_loss: 0.6969020366668701

('sgd', 0.001, 8, 100)
train_loss: 0.7045407295227051, val_loss: 0.7572379112243652

('sgd', 0.001, 16, 10)
train_loss: 0.7108404040336609, val_loss: 0.7831926345825195

('sgd', 0.001, 16, 50)
train_loss: 0.696204662322998, val_loss: 0.7145664095878601

('sgd', 0.001, 16, 100)
train_loss: 0.6920448541641235, val_loss: 0.6812959313392639

('sgd', 0.001, 32, 10)
train_loss: 0.6948623657226562, val_loss: 0.7062757611274719

('sgd', 0.001, 32, 50)
train_loss: 0.7010112404823303, val_loss: 0.7398233413696289

('sgd', 0.001, 32, 100)
train_loss: 0.6930002570152283, val_loss: 0.6697607040405273

('sgd', 0.001, 64, 10)
train_loss: 0.7124851942062378, val_loss: 0.7899057269096375

('sgd', 0.001, 64, 50)
train_loss: 0.6938177347183228, val_loss: 0.6690024733543396

('sgd', 0.001, 64, 100)
train_loss: 0.7010588645935059, val_loss: 0.6668758988380432

('sgd', 0.01, 8, 10)
train_loss: 0.7141634821891785, val_loss: 0.7988898158073425

('sgd', 0.01, 8, 50)
train_loss: 0.6922059059143066, val_loss: 0.6887092590332031

('sgd', 0.01, 8, 100)
train_loss: 0.6930476427078247, val_loss: 0.6961574554443359

('sgd', 0.01, 16, 10)
train_loss: 0.7103171348571777, val_loss: 0.7818345427513123

('sgd', 0.01, 16, 50)
train_loss: 0.7025904059410095, val_loss: 0.7485537528991699

('sgd', 0.01, 16, 100)
train_loss: 0.6979590058326721, val_loss: 0.7236784100532532

('sgd', 0.01, 32, 10)
train_loss: 0.6943056583404541, val_loss: 0.7020896077156067

('sgd', 0.01, 32, 50)
train_loss: 0.7040536999702454, val_loss: 0.7550988793373108

('sgd', 0.01, 32, 100)
train_loss: 0.6927907466888428, val_loss: 0.6933333873748779

('sgd', 0.01, 64, 10)
train_loss: 0.6952701807022095, val_loss: 0.7089172005653381

('sgd', 0.01, 64, 50)
train_loss: 0.6991767287254333, val_loss: 0.7307793498039246

('sgd', 0.01, 64, 100)
train_loss: 0.7046860456466675, val_loss: 0.758124828338623

('sgd', 0.1, 8, 10)
train_loss: 0.7032070755958557, val_loss: 0.7516831755638123

('sgd', 0.1, 8, 50)
train_loss: 0.6903470158576965, val_loss: 0.6643130779266357

('sgd', 0.1, 8, 100)
train_loss: 0.6897115111351013, val_loss: 0.6631383299827576

('sgd', 0.1, 16, 10)
train_loss: 0.6913922429084778, val_loss: 0.6810481548309326

('sgd', 0.1, 16, 50)
train_loss: 0.7068625688552856, val_loss: 0.7650416493415833

('sgd', 0.1, 16, 100)
train_loss: 0.6872735023498535, val_loss: 0.6485755443572998

('sgd', 0.1, 32, 10)
train_loss: 0.6904234886169434, val_loss: 0.6678550839424133

('sgd', 0.1, 32, 50)
train_loss: 0.6935983896255493, val_loss: 0.699683666229248

('sgd', 0.1, 32, 100)
train_loss: 0.6874593496322632, val_loss: 0.6566558480262756

('sgd', 0.1, 64, 10)
train_loss: 0.7045546770095825, val_loss: 0.757169783115387

('sgd', 0.1, 64, 50)
train_loss: 0.6983365416526794, val_loss: 0.7261223793029785

('sgd', 0.1, 64, 100)
train_loss: 0.6932632327079773, val_loss: 0.6976837515830994


---------
BEST MODEL
('adam', 0.1, 64, 50)
val_loss: 0.6431503891944885
---------

Run from 2023-04-17 16:24:42.637798
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.700545072555542, val_loss: 0.7247940301895142

('adam', 0.001, 8, 50)
train_loss: 0.6930398344993591, val_loss: 0.6353549957275391

('adam', 0.001, 8, 100)
train_loss: 0.6942850947380066, val_loss: 0.7013542652130127

('adam', 0.001, 16, 10)
train_loss: 0.6922012567520142, val_loss: 0.6732116937637329

('adam', 0.001, 16, 50)
train_loss: 0.7124983072280884, val_loss: 0.7569539546966553

('adam', 0.001, 16, 100)
train_loss: 0.6892291307449341, val_loss: 0.6419939398765564

('adam', 0.001, 32, 10)
train_loss: 0.6971282362937927, val_loss: 0.7134393453598022

('adam', 0.001, 32, 50)
train_loss: 0.7118949294090271, val_loss: 0.7551441192626953

('adam', 0.001, 32, 100)
train_loss: 0.693142831325531, val_loss: 0.6334724426269531

('adam', 0.001, 64, 10)
train_loss: 0.6970769762992859, val_loss: 0.6504455804824829

('adam', 0.001, 64, 50)
train_loss: 0.6964373588562012, val_loss: 0.6435165405273438

('adam', 0.001, 64, 100)
train_loss: 0.7046858072280884, val_loss: 0.7385480999946594

('adam', 0.01, 8, 10)
train_loss: 0.6876217126846313, val_loss: 0.6428513526916504

('adam', 0.01, 8, 50)
train_loss: 0.6950176358222961, val_loss: 0.7018070220947266

('adam', 0.01, 8, 100)
train_loss: 0.687823474407196, val_loss: 0.6526468992233276

('adam', 0.01, 16, 10)
train_loss: 0.6883066296577454, val_loss: 0.6453949213027954

('adam', 0.01, 16, 50)
train_loss: 0.6917789578437805, val_loss: 0.6889893412590027

('adam', 0.01, 16, 100)
train_loss: 0.6871246099472046, val_loss: 0.6496853828430176

('adam', 0.01, 32, 10)
train_loss: 0.6951305866241455, val_loss: 0.7050424814224243

('adam', 0.01, 32, 50)
train_loss: 0.6875360608100891, val_loss: 0.6589620113372803

('adam', 0.01, 32, 100)
train_loss: 0.6875476241111755, val_loss: 0.6581718325614929

('adam', 0.01, 64, 10)
train_loss: 0.69168621301651, val_loss: 0.650783896446228

('adam', 0.01, 64, 50)
train_loss: 0.6868792772293091, val_loss: 0.6491221189498901

('adam', 0.01, 64, 100)
train_loss: 0.6930554509162903, val_loss: 0.695745587348938

('adam', 0.1, 8, 10)
train_loss: 0.6931394934654236, val_loss: 0.6459402441978455

('adam', 0.1, 8, 50)
train_loss: 0.6879817843437195, val_loss: 0.6289631128311157

('adam', 0.1, 8, 100)
train_loss: 0.684667706489563, val_loss: 0.6402423977851868

('adam', 0.1, 16, 10)
train_loss: 0.688164472579956, val_loss: 0.6356823444366455

('adam', 0.1, 16, 50)
train_loss: 0.6872940063476562, val_loss: 0.633955717086792

('adam', 0.1, 16, 100)
train_loss: 0.688119113445282, val_loss: 0.6384025812149048

('adam', 0.1, 32, 10)
train_loss: 0.6874492168426514, val_loss: 0.6369351148605347

('adam', 0.1, 32, 50)
train_loss: 0.6872236132621765, val_loss: 0.6426374316215515

('adam', 0.1, 32, 100)
train_loss: 0.6866933703422546, val_loss: 0.637892484664917

('adam', 0.1, 64, 10)
train_loss: 0.6889424324035645, val_loss: 0.6527396440505981

('adam', 0.1, 64, 50)
train_loss: 0.6866641640663147, val_loss: 0.6418028473854065

('adam', 0.1, 64, 100)
train_loss: 0.6866447329521179, val_loss: 0.6393121480941772

('sgd', 0.001, 8, 10)
train_loss: 0.6931188106536865, val_loss: 0.6928392648696899

('sgd', 0.001, 8, 50)
train_loss: 0.696795642375946, val_loss: 0.7122085094451904

('sgd', 0.001, 8, 100)
train_loss: 0.7199101448059082, val_loss: 0.7709348797798157

('sgd', 0.001, 16, 10)
train_loss: 0.6925765872001648, val_loss: 0.6869803667068481

('sgd', 0.001, 16, 50)
train_loss: 0.7229570150375366, val_loss: 0.7748535871505737

('sgd', 0.001, 16, 100)
train_loss: 0.6929367184638977, val_loss: 0.692333459854126

('sgd', 0.001, 32, 10)
train_loss: 0.6940597891807556, val_loss: 0.6995246410369873

('sgd', 0.001, 32, 50)
train_loss: 0.6951496601104736, val_loss: 0.6571066379547119

('sgd', 0.001, 32, 100)
train_loss: 0.710602343082428, val_loss: 0.7501816749572754

('sgd', 0.001, 64, 10)
train_loss: 0.7202956080436707, val_loss: 0.7692630290985107

('sgd', 0.001, 64, 50)
train_loss: 0.7103466391563416, val_loss: 0.7493096590042114

('sgd', 0.001, 64, 100)
train_loss: 0.6923123598098755, val_loss: 0.6780886650085449

('sgd', 0.01, 8, 10)
train_loss: 0.6946760416030884, val_loss: 0.7028418779373169

('sgd', 0.01, 8, 50)
train_loss: 0.6892573833465576, val_loss: 0.6696868538856506

('sgd', 0.01, 8, 100)
train_loss: 0.6962383389472961, val_loss: 0.7087810039520264

('sgd', 0.01, 16, 10)
train_loss: 0.6925430297851562, val_loss: 0.6665326356887817

('sgd', 0.01, 16, 50)
train_loss: 0.7148028612136841, val_loss: 0.7626805305480957

('sgd', 0.01, 16, 100)
train_loss: 0.688758373260498, val_loss: 0.6367015242576599

('sgd', 0.01, 32, 10)
train_loss: 0.6992583274841309, val_loss: 0.6438619494438171

('sgd', 0.01, 32, 50)
train_loss: 0.6920223236083984, val_loss: 0.6582539081573486

('sgd', 0.01, 32, 100)
train_loss: 0.7060859799385071, val_loss: 0.7416709661483765

('sgd', 0.01, 64, 10)
train_loss: 0.6961786150932312, val_loss: 0.653687596321106

('sgd', 0.01, 64, 50)
train_loss: 0.7203385829925537, val_loss: 0.7708759307861328

('sgd', 0.01, 64, 100)
train_loss: 0.69442218542099, val_loss: 0.644551157951355

('sgd', 0.1, 8, 10)
train_loss: 0.6894055604934692, val_loss: 0.6422398686408997

('sgd', 0.1, 8, 50)
train_loss: 0.6886938214302063, val_loss: 0.6437819004058838

('sgd', 0.1, 8, 100)
train_loss: 0.6900915503501892, val_loss: 0.6697612404823303

('sgd', 0.1, 16, 10)
train_loss: 0.7057337760925293, val_loss: 0.7390265464782715

('sgd', 0.1, 16, 50)
train_loss: 0.6871311068534851, val_loss: 0.6486037373542786

('sgd', 0.1, 16, 100)
train_loss: 0.691129207611084, val_loss: 0.6780774593353271

('sgd', 0.1, 32, 10)
train_loss: 0.6936771869659424, val_loss: 0.6941730976104736

('sgd', 0.1, 32, 50)
train_loss: 0.6953339576721191, val_loss: 0.7056553363800049

('sgd', 0.1, 32, 100)
train_loss: 0.7059744596481323, val_loss: 0.7430120706558228

('sgd', 0.1, 64, 10)
train_loss: 0.7198907136917114, val_loss: 0.7709352374076843

('sgd', 0.1, 64, 50)
train_loss: 0.6877842545509338, val_loss: 0.6426870822906494

('sgd', 0.1, 64, 100)
train_loss: 0.6873326301574707, val_loss: 0.6554735898971558


---------
BEST MODEL
('adam', 0.1, 8, 50)
val_loss: 0.6289631128311157
---------
