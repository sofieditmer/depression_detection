
Run from 2023-04-10 10:02:27.306697
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7123003602027893, val_loss: 0.752021312713623

('adam', 0.001, 8, 50)
train_loss: 0.7025945782661438, val_loss: 0.7384077310562134

('adam', 0.001, 8, 100)
train_loss: 0.692674458026886, val_loss: 0.6921521425247192

('adam', 0.001, 16, 10)
train_loss: 0.81847083568573, val_loss: 0.825204610824585

('adam', 0.001, 16, 50)
train_loss: 0.7116235494613647, val_loss: 0.6916387677192688

('adam', 0.001, 16, 100)
train_loss: 0.692549467086792, val_loss: 0.6931222081184387

('adam', 0.001, 32, 10)
train_loss: 0.7672086954116821, val_loss: 0.7592094540596008

('adam', 0.001, 32, 50)
train_loss: 0.6943002343177795, val_loss: 0.7060750126838684

('adam', 0.001, 32, 100)
train_loss: 0.730499267578125, val_loss: 0.8065932989120483

('adam', 0.001, 64, 10)
train_loss: 0.7172582745552063, val_loss: 0.7006943225860596

('adam', 0.001, 64, 50)
train_loss: 0.7866289019584656, val_loss: 0.7825631499290466

('adam', 0.001, 64, 100)
train_loss: 0.7602381706237793, val_loss: 0.7460896968841553

('adam', 0.01, 8, 10)
train_loss: 0.6931801438331604, val_loss: 0.690245509147644

('adam', 0.01, 8, 50)
train_loss: 0.6919926404953003, val_loss: 0.6985037326812744

('adam', 0.01, 8, 100)
train_loss: 0.6928934454917908, val_loss: 0.6971040964126587

('adam', 0.01, 16, 10)
train_loss: 0.7000528573989868, val_loss: 0.7364891767501831

('adam', 0.01, 16, 50)
train_loss: 0.7000982165336609, val_loss: 0.6784505844116211

('adam', 0.01, 16, 100)
train_loss: 0.6938711404800415, val_loss: 0.6949476003646851

('adam', 0.01, 32, 10)
train_loss: 0.7823644280433655, val_loss: 0.9116804599761963

('adam', 0.01, 32, 50)
train_loss: 0.6966688632965088, val_loss: 0.6809918880462646

('adam', 0.01, 32, 100)
train_loss: 0.6920260787010193, val_loss: 0.7055143117904663

('adam', 0.01, 64, 10)
train_loss: 0.6950035095214844, val_loss: 0.6838298439979553

('adam', 0.01, 64, 50)
train_loss: 0.7129428386688232, val_loss: 0.6723780632019043

('adam', 0.01, 64, 100)
train_loss: 0.6946582794189453, val_loss: 0.6847723722457886

('adam', 0.1, 8, 10)
train_loss: 0.697928249835968, val_loss: 0.6955437660217285

('adam', 0.1, 8, 50)
train_loss: 0.6947373747825623, val_loss: 0.7039990425109863

('adam', 0.1, 8, 100)
train_loss: 0.6953788995742798, val_loss: 0.7092397809028625

('adam', 0.1, 16, 10)
train_loss: 0.6987786889076233, val_loss: 0.6983954310417175

('adam', 0.1, 16, 50)
train_loss: 0.6942954063415527, val_loss: 0.6910853385925293

('adam', 0.1, 16, 100)
train_loss: 0.6948310136795044, val_loss: 0.703006386756897

('adam', 0.1, 32, 10)
train_loss: 0.7024655938148499, val_loss: 0.6771804094314575

('adam', 0.1, 32, 50)
train_loss: 0.6921253204345703, val_loss: 0.7051141262054443

('adam', 0.1, 32, 100)
train_loss: 0.6953956484794617, val_loss: 0.6986660957336426

('adam', 0.1, 64, 10)
train_loss: 0.6955809593200684, val_loss: 0.6863527894020081

('adam', 0.1, 64, 50)
train_loss: 0.6919406652450562, val_loss: 0.6992014050483704

('adam', 0.1, 64, 100)
train_loss: 0.6919019818305969, val_loss: 0.7012659907341003


---------
BEST MODEL
('adam', 0.01, 64, 50)
val_loss: 0.6723780632019043
---------

Run from 2023-04-10 13:00:26.610101
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7194156646728516, val_loss: 0.6853383779525757

('adam', 0.001, 8, 50)
train_loss: 0.7169259786605835, val_loss: 0.6819036602973938

('adam', 0.001, 8, 100)
train_loss: 0.7294570803642273, val_loss: 0.7613006830215454

('adam', 0.001, 16, 10)
train_loss: 0.769940972328186, val_loss: 0.6955836415290833

('adam', 0.001, 16, 50)
train_loss: 0.6970604062080383, val_loss: 0.6875928640365601

('adam', 0.001, 16, 100)
train_loss: 0.7279560565948486, val_loss: 0.751958429813385

('adam', 0.001, 32, 10)
train_loss: 0.7545040845870972, val_loss: 0.6918845176696777

('adam', 0.001, 32, 50)
train_loss: 0.7060998678207397, val_loss: 0.7247918844223022

('adam', 0.001, 32, 100)
train_loss: 0.7297693490982056, val_loss: 0.7528637647628784

('adam', 0.001, 64, 10)
train_loss: 0.8481651544570923, val_loss: 0.8461345434188843

('adam', 0.001, 64, 50)
train_loss: 0.7032715082168579, val_loss: 0.7206441164016724

('adam', 0.001, 64, 100)
train_loss: 0.813839852809906, val_loss: 0.821622908115387

('adam', 0.01, 8, 10)
train_loss: 0.7234013080596924, val_loss: 0.6810532808303833

('adam', 0.01, 8, 50)
train_loss: 0.6936482191085815, val_loss: 0.6929406523704529

('adam', 0.01, 8, 100)
train_loss: 0.6926997900009155, val_loss: 0.6985331773757935

('adam', 0.01, 16, 10)
train_loss: 0.7261196374893188, val_loss: 0.746379017829895

('adam', 0.01, 16, 50)
train_loss: 0.6933615803718567, val_loss: 0.7027270793914795

('adam', 0.01, 16, 100)
train_loss: 0.6940699815750122, val_loss: 0.6904876828193665

('adam', 0.01, 32, 10)
train_loss: 0.7625514268875122, val_loss: 0.6878436803817749

('adam', 0.01, 32, 50)
train_loss: 0.6929038166999817, val_loss: 0.6945347785949707

('adam', 0.01, 32, 100)
train_loss: 0.6935724020004272, val_loss: 0.706189751625061

('adam', 0.01, 64, 10)
train_loss: 0.705930233001709, val_loss: 0.6838200688362122

('adam', 0.01, 64, 50)
train_loss: 0.6934810876846313, val_loss: 0.6923400163650513

('adam', 0.01, 64, 100)
train_loss: 0.692226231098175, val_loss: 0.6962370872497559

('adam', 0.1, 8, 10)
train_loss: 0.6950315833091736, val_loss: 0.6971257925033569

('adam', 0.1, 8, 50)
train_loss: 0.6953293085098267, val_loss: 0.6903584003448486

('adam', 0.1, 8, 100)
train_loss: 0.702251672744751, val_loss: 0.7108292579650879

('adam', 0.1, 16, 10)
train_loss: 0.6994293332099915, val_loss: 0.6882317066192627

('adam', 0.1, 16, 50)
train_loss: 0.692695140838623, val_loss: 0.6924165487289429

('adam', 0.1, 16, 100)
train_loss: 0.7218707203865051, val_loss: 0.7252625823020935

('adam', 0.1, 32, 10)
train_loss: 0.6946937441825867, val_loss: 0.7111865282058716

('adam', 0.1, 32, 50)
train_loss: 0.6923190951347351, val_loss: 0.6980727910995483

('adam', 0.1, 32, 100)
train_loss: 0.6919659376144409, val_loss: 0.6970418691635132

('adam', 0.1, 64, 10)
train_loss: 0.6926782727241516, val_loss: 0.6948437690734863

('adam', 0.1, 64, 50)
train_loss: 0.6925019025802612, val_loss: 0.6972609162330627

('adam', 0.1, 64, 100)
train_loss: 0.6922274827957153, val_loss: 0.6962593197822571


---------
BEST MODEL
('adam', 0.01, 8, 10)
val_loss: 0.6810532808303833
---------

Run from 2023-04-10 14:21:56.603093
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6855921149253845, val_loss: 0.7117606401443481

('adam', 0.001, 8, 50)
train_loss: 0.7248774766921997, val_loss: 0.8779202699661255

('adam', 0.001, 8, 100)
train_loss: 0.6815063953399658, val_loss: 0.7423330545425415

('adam', 0.001, 16, 10)
train_loss: 0.7203270792961121, val_loss: 0.8432169556617737

('adam', 0.001, 16, 50)
train_loss: 0.6891478896141052, val_loss: 0.768968939781189

('adam', 0.001, 16, 100)
train_loss: 0.6848353743553162, val_loss: 0.7131005525588989

('adam', 0.001, 32, 10)
train_loss: 0.7941973805427551, val_loss: 0.6962956190109253

('adam', 0.001, 32, 50)
train_loss: 0.6851896047592163, val_loss: 0.7131862640380859

('adam', 0.001, 32, 100)
train_loss: 0.6829421520233154, val_loss: 0.7230947017669678

('adam', 0.001, 64, 10)
train_loss: 0.692416250705719, val_loss: 0.7679877281188965

('adam', 0.001, 64, 50)
train_loss: 0.7809821367263794, val_loss: 0.6892426013946533

('adam', 0.001, 64, 100)
train_loss: 0.6956051588058472, val_loss: 0.7918291687965393

('adam', 0.01, 8, 10)
train_loss: 0.7393880486488342, val_loss: 0.6621177196502686

('adam', 0.01, 8, 50)
train_loss: 0.6805017590522766, val_loss: 0.7387574911117554

('adam', 0.01, 8, 100)
train_loss: 0.6794018745422363, val_loss: 0.7503519654273987

('adam', 0.01, 16, 10)
train_loss: 0.6833987236022949, val_loss: 0.7191325426101685

('adam', 0.01, 16, 50)
train_loss: 0.6864901781082153, val_loss: 0.7098087072372437

('adam', 0.01, 16, 100)
train_loss: 0.6975137591362, val_loss: 0.6933626532554626

('adam', 0.01, 32, 10)
train_loss: 0.686549961566925, val_loss: 0.7090336680412292

('adam', 0.01, 32, 50)
train_loss: 0.680483877658844, val_loss: 0.7773916721343994

('adam', 0.01, 32, 100)
train_loss: 0.6794103980064392, val_loss: 0.7463520765304565

('adam', 0.01, 64, 10)
train_loss: 0.6893436908721924, val_loss: 0.7019256353378296

('adam', 0.01, 64, 50)
train_loss: 0.679842472076416, val_loss: 0.7382636666297913

('adam', 0.01, 64, 100)
train_loss: 0.7126718759536743, val_loss: 0.6749507188796997

('adam', 0.1, 8, 10)
train_loss: 0.6866357326507568, val_loss: 0.7568800449371338

('adam', 0.1, 8, 50)
train_loss: 0.6883988976478577, val_loss: 0.7574844360351562

('adam', 0.1, 8, 100)
train_loss: 0.6807370781898499, val_loss: 0.7708693742752075

('adam', 0.1, 16, 10)
train_loss: 0.6806299090385437, val_loss: 0.7475006580352783

('adam', 0.1, 16, 50)
train_loss: 0.6814864277839661, val_loss: 0.7757406234741211

('adam', 0.1, 16, 100)
train_loss: 0.6844439506530762, val_loss: 0.7339634299278259

('adam', 0.1, 32, 10)
train_loss: 0.6856061816215515, val_loss: 0.756925106048584

('adam', 0.1, 32, 50)
train_loss: 0.679996132850647, val_loss: 0.7548192143440247

('adam', 0.1, 32, 100)
train_loss: 0.6789689660072327, val_loss: 0.7493333220481873

('adam', 0.1, 64, 10)
train_loss: 0.6801645755767822, val_loss: 0.7715415954589844

('adam', 0.1, 64, 50)
train_loss: 0.6792430877685547, val_loss: 0.7515395879745483

('adam', 0.1, 64, 100)
train_loss: 0.6792276501655579, val_loss: 0.7531701326370239


---------
BEST MODEL
('adam', 0.01, 8, 10)
val_loss: 0.6621177196502686
---------

Run from 2023-04-17 09:54:52.699256
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6964489221572876, val_loss: 0.7073724865913391

('adam', 0.001, 8, 50)
train_loss: 0.6926472187042236, val_loss: 0.692668616771698

('adam', 0.001, 8, 100)
train_loss: 0.6948146820068359, val_loss: 0.7016704082489014

('adam', 0.001, 16, 10)
train_loss: 0.7550452947616577, val_loss: 0.6869204640388489

('adam', 0.001, 16, 50)
train_loss: 0.7709025740623474, val_loss: 0.8571853041648865

('adam', 0.001, 16, 100)
train_loss: 0.7686768174171448, val_loss: 0.8570391535758972

('adam', 0.001, 32, 10)
train_loss: 0.751555323600769, val_loss: 0.6861475110054016

('adam', 0.001, 32, 50)
train_loss: 0.7085767388343811, val_loss: 0.740291178226471

('adam', 0.001, 32, 100)
train_loss: 0.7119683027267456, val_loss: 0.7490100860595703

('adam', 0.001, 64, 10)
train_loss: 0.6996740698814392, val_loss: 0.7176716923713684

('adam', 0.001, 64, 50)
train_loss: 0.72725909948349, val_loss: 0.6722301840782166

('adam', 0.001, 64, 100)
train_loss: 0.8337429165840149, val_loss: 0.9553056359291077

('adam', 0.01, 8, 10)
train_loss: 0.6915851831436157, val_loss: 0.6905118823051453

('adam', 0.01, 8, 50)
train_loss: 0.6917490363121033, val_loss: 0.6825719475746155

('adam', 0.01, 8, 100)
train_loss: 0.6884993314743042, val_loss: 0.6681382656097412

('adam', 0.01, 16, 10)
train_loss: 0.7336550354957581, val_loss: 0.7921084761619568

('adam', 0.01, 16, 50)
train_loss: 0.7000298500061035, val_loss: 0.7146188616752625

('adam', 0.01, 16, 100)
train_loss: 0.6941549181938171, val_loss: 0.6981717944145203

('adam', 0.01, 32, 10)
train_loss: 0.6962386965751648, val_loss: 0.7067812085151672

('adam', 0.01, 32, 50)
train_loss: 0.7110706567764282, val_loss: 0.7424388527870178

('adam', 0.01, 32, 100)
train_loss: 0.6884580254554749, val_loss: 0.6688993573188782

('adam', 0.01, 64, 10)
train_loss: 0.7233040928840637, val_loss: 0.6662167906761169

('adam', 0.01, 64, 50)
train_loss: 0.6879788041114807, val_loss: 0.6633700132369995

('adam', 0.01, 64, 100)
train_loss: 0.6879006624221802, val_loss: 0.6597154140472412

('adam', 0.1, 8, 10)
train_loss: 0.6956422924995422, val_loss: 0.6525184512138367

('adam', 0.1, 8, 50)
train_loss: 0.7004005908966064, val_loss: 0.651905357837677

('adam', 0.1, 8, 100)
train_loss: 0.6890570521354675, val_loss: 0.6570643782615662

('adam', 0.1, 16, 10)
train_loss: 0.6946293115615845, val_loss: 0.6897082328796387

('adam', 0.1, 16, 50)
train_loss: 0.6884157061576843, val_loss: 0.6581981182098389

('adam', 0.1, 16, 100)
train_loss: 0.6891956329345703, val_loss: 0.6585872173309326

('adam', 0.1, 32, 10)
train_loss: 0.6901241540908813, val_loss: 0.6514347195625305

('adam', 0.1, 32, 50)
train_loss: 0.6885219216346741, val_loss: 0.6598590612411499

('adam', 0.1, 32, 100)
train_loss: 0.6886833906173706, val_loss: 0.6595737934112549

('adam', 0.1, 64, 10)
train_loss: 0.7108598947525024, val_loss: 0.7153642773628235

('adam', 0.1, 64, 50)
train_loss: 0.6879134178161621, val_loss: 0.6596360206604004

('adam', 0.1, 64, 100)
train_loss: 0.687900722026825, val_loss: 0.6599425077438354

('sgd', 0.001, 8, 10)
train_loss: 0.816162109375, val_loss: 0.924584150314331

('sgd', 0.001, 8, 50)
train_loss: 0.7038072943687439, val_loss: 0.6683433651924133

('sgd', 0.001, 8, 100)
train_loss: 0.7116898894309998, val_loss: 0.7471628189086914

('sgd', 0.001, 16, 10)
train_loss: 0.699556291103363, val_loss: 0.672870397567749

('sgd', 0.001, 16, 50)
train_loss: 0.7206054925918579, val_loss: 0.6718565821647644

('sgd', 0.001, 16, 100)
train_loss: 0.6965481042861938, val_loss: 0.6723009943962097

('sgd', 0.001, 32, 10)
train_loss: 0.7715471386909485, val_loss: 0.6991317868232727

('sgd', 0.001, 32, 50)
train_loss: 0.6939539313316345, val_loss: 0.6981687545776367

('sgd', 0.001, 32, 100)
train_loss: 0.6977606415748596, val_loss: 0.6727339625358582

('sgd', 0.001, 64, 10)
train_loss: 0.7676118612289429, val_loss: 0.69687819480896

('sgd', 0.001, 64, 50)
train_loss: 0.8332966566085815, val_loss: 0.9510390162467957

('sgd', 0.001, 64, 100)
train_loss: 0.8455772399902344, val_loss: 0.9701923727989197

('sgd', 0.01, 8, 10)
train_loss: 0.6947826743125916, val_loss: 0.6721207499504089

('sgd', 0.01, 8, 50)
train_loss: 0.7246218323707581, val_loss: 0.7760711312294006

('sgd', 0.01, 8, 100)
train_loss: 0.7019339799880981, val_loss: 0.722136914730072

('sgd', 0.01, 16, 10)
train_loss: 0.6945679783821106, val_loss: 0.675586998462677

('sgd', 0.01, 16, 50)
train_loss: 0.7261492609977722, val_loss: 0.7786828875541687

('sgd', 0.01, 16, 100)
train_loss: 0.6942066550254822, val_loss: 0.64383465051651

('sgd', 0.01, 32, 10)
train_loss: 0.7183104753494263, val_loss: 0.7597200274467468

('sgd', 0.01, 32, 50)
train_loss: 0.726906418800354, val_loss: 0.778484046459198

('sgd', 0.01, 32, 100)
train_loss: 0.7236554622650146, val_loss: 0.6517471671104431

('sgd', 0.01, 64, 10)
train_loss: 0.7535374760627747, val_loss: 0.6873927712440491

('sgd', 0.01, 64, 50)
train_loss: 0.7732189297676086, val_loss: 0.6933097839355469

('sgd', 0.01, 64, 100)
train_loss: 0.6922465562820435, val_loss: 0.6775034070014954

('sgd', 0.1, 8, 10)
train_loss: 0.6909886598587036, val_loss: 0.6416345238685608

('sgd', 0.1, 8, 50)
train_loss: 0.6941616535186768, val_loss: 0.6818223595619202

('sgd', 0.1, 8, 100)
train_loss: 0.691134512424469, val_loss: 0.6687211394309998

('sgd', 0.1, 16, 10)
train_loss: 0.7021629214286804, val_loss: 0.7176916599273682

('sgd', 0.1, 16, 50)
train_loss: 0.6908063292503357, val_loss: 0.64056396484375

('sgd', 0.1, 16, 100)
train_loss: 0.694023847579956, val_loss: 0.6954392790794373

('sgd', 0.1, 32, 10)
train_loss: 0.7452379465103149, val_loss: 0.8111063838005066

('sgd', 0.1, 32, 50)
train_loss: 0.6951216459274292, val_loss: 0.7012602686882019

('sgd', 0.1, 32, 100)
train_loss: 0.703484058380127, val_loss: 0.724041759967804

('sgd', 0.1, 64, 10)
train_loss: 0.7209932208061218, val_loss: 0.662578284740448

('sgd', 0.1, 64, 50)
train_loss: 0.7359601855278015, val_loss: 0.798530101776123

('sgd', 0.1, 64, 100)
train_loss: 0.6993616223335266, val_loss: 0.7149372100830078


---------
BEST MODEL
('sgd', 0.1, 16, 50)
val_loss: 0.64056396484375
---------

Run from 2023-04-17 16:34:20.488023
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7209205031394958, val_loss: 0.8260021805763245

('adam', 0.001, 8, 50)
train_loss: 0.7022453546524048, val_loss: 0.8029497861862183

('adam', 0.001, 8, 100)
train_loss: 0.6854158639907837, val_loss: 0.7355781197547913

('adam', 0.001, 16, 10)
train_loss: 0.7214794754981995, val_loss: 0.8249459266662598

('adam', 0.001, 16, 50)
train_loss: 0.7289468050003052, val_loss: 0.85233074426651

('adam', 0.001, 16, 100)
train_loss: 0.7972367405891418, val_loss: 0.6350051164627075

('adam', 0.001, 32, 10)
train_loss: 0.6903197169303894, val_loss: 0.7011352777481079

('adam', 0.001, 32, 50)
train_loss: 0.769821286201477, val_loss: 0.6546640396118164

('adam', 0.001, 32, 100)
train_loss: 0.7426607608795166, val_loss: 0.6492975950241089

('adam', 0.001, 64, 10)
train_loss: 0.703193187713623, val_loss: 0.6790637373924255

('adam', 0.001, 64, 50)
train_loss: 0.6870657205581665, val_loss: 0.7224842309951782

('adam', 0.001, 64, 100)
train_loss: 0.6865147948265076, val_loss: 0.7327994108200073

('adam', 0.01, 8, 10)
train_loss: 0.6912346482276917, val_loss: 0.6999762654304504

('adam', 0.01, 8, 50)
train_loss: 0.6868279576301575, val_loss: 0.7219708561897278

('adam', 0.01, 8, 100)
train_loss: 0.686242938041687, val_loss: 0.7231190800666809

('adam', 0.01, 16, 10)
train_loss: 0.6847891807556152, val_loss: 0.732426643371582

('adam', 0.01, 16, 50)
train_loss: 0.6967704892158508, val_loss: 0.6872996687889099

('adam', 0.01, 16, 100)
train_loss: 0.6844503283500671, val_loss: 0.7391396760940552

('adam', 0.01, 32, 10)
train_loss: 0.6872228980064392, val_loss: 0.7171826362609863

('adam', 0.01, 32, 50)
train_loss: 0.6957269906997681, val_loss: 0.6885392665863037

('adam', 0.01, 32, 100)
train_loss: 0.6838136911392212, val_loss: 0.7486929893493652

('adam', 0.01, 64, 10)
train_loss: 0.6949514150619507, val_loss: 0.6879297494888306

('adam', 0.01, 64, 50)
train_loss: 0.697126030921936, val_loss: 0.8121773600578308

('adam', 0.01, 64, 100)
train_loss: 0.6837773323059082, val_loss: 0.7515276670455933

('adam', 0.1, 8, 10)
train_loss: 0.6958361864089966, val_loss: 0.7619203925132751

('adam', 0.1, 8, 50)
train_loss: 0.6872567534446716, val_loss: 0.7600739002227783

('adam', 0.1, 8, 100)
train_loss: 0.6851068735122681, val_loss: 0.757790207862854

('adam', 0.1, 16, 10)
train_loss: 0.686732828617096, val_loss: 0.7250874042510986

('adam', 0.1, 16, 50)
train_loss: 0.6845510005950928, val_loss: 0.7478365898132324

('adam', 0.1, 16, 100)
train_loss: 0.6838761568069458, val_loss: 0.7540039420127869

('adam', 0.1, 32, 10)
train_loss: 0.6861076354980469, val_loss: 0.7355630993843079

('adam', 0.1, 32, 50)
train_loss: 0.6842168569564819, val_loss: 0.7511821389198303

('adam', 0.1, 32, 100)
train_loss: 0.6838729381561279, val_loss: 0.7537158727645874

('adam', 0.1, 64, 10)
train_loss: 0.7189951539039612, val_loss: 0.706659197807312

('adam', 0.1, 64, 50)
train_loss: 0.6837878227233887, val_loss: 0.750066339969635

('adam', 0.1, 64, 100)
train_loss: 0.6837770342826843, val_loss: 0.7512514591217041

('sgd', 0.001, 8, 10)
train_loss: 0.701950192451477, val_loss: 0.7796890139579773

('sgd', 0.001, 8, 50)
train_loss: 0.8362732529640198, val_loss: 0.6646263599395752

('sgd', 0.001, 8, 100)
train_loss: 0.7494632005691528, val_loss: 0.6541949510574341

('sgd', 0.001, 16, 10)
train_loss: 0.8968266248703003, val_loss: 0.6895080804824829

('sgd', 0.001, 16, 50)
train_loss: 0.6885579228401184, val_loss: 0.7100262641906738

('sgd', 0.001, 16, 100)
train_loss: 0.8307450413703918, val_loss: 0.6637486219406128

('sgd', 0.001, 32, 10)
train_loss: 0.7435213327407837, val_loss: 0.6645054817199707

('sgd', 0.001, 32, 50)
train_loss: 0.7856287360191345, val_loss: 0.6637557744979858

('sgd', 0.001, 32, 100)
train_loss: 0.7836028933525085, val_loss: 0.6618173122406006

('sgd', 0.001, 64, 10)
train_loss: 0.892077624797821, val_loss: 0.6890037059783936

('sgd', 0.001, 64, 50)
train_loss: 0.6961433291435242, val_loss: 0.7628021836280823

('sgd', 0.001, 64, 100)
train_loss: 0.8613450527191162, val_loss: 0.6779204607009888

('sgd', 0.01, 8, 10)
train_loss: 0.7015205025672913, val_loss: 0.7859801650047302

('sgd', 0.01, 8, 50)
train_loss: 0.7240320444107056, val_loss: 0.6426939964294434

('sgd', 0.01, 8, 100)
train_loss: 0.6840819716453552, val_loss: 0.7537678480148315

('sgd', 0.01, 16, 10)
train_loss: 0.6951997876167297, val_loss: 0.6890431642532349

('sgd', 0.01, 16, 50)
train_loss: 0.6872127056121826, val_loss: 0.7427670955657959

('sgd', 0.01, 16, 100)
train_loss: 0.707401692867279, val_loss: 0.6627861261367798

('sgd', 0.01, 32, 10)
train_loss: 0.729126513004303, val_loss: 0.8393660187721252

('sgd', 0.01, 32, 50)
train_loss: 0.7540737390518188, val_loss: 0.8980724811553955

('sgd', 0.01, 32, 100)
train_loss: 0.6918743252754211, val_loss: 0.6957833170890808

('sgd', 0.01, 64, 10)
train_loss: 0.7740092873573303, val_loss: 0.6628711819648743

('sgd', 0.01, 64, 50)
train_loss: 0.6984351873397827, val_loss: 0.7752645015716553

('sgd', 0.01, 64, 100)
train_loss: 0.6906737089157104, val_loss: 0.7535275220870972

('sgd', 0.1, 8, 10)
train_loss: 0.725541889667511, val_loss: 0.6485454440116882

('sgd', 0.1, 8, 50)
train_loss: 0.6844386458396912, val_loss: 0.7512485980987549

('sgd', 0.1, 8, 100)
train_loss: 0.6856701970100403, val_loss: 0.7480989694595337

('sgd', 0.1, 16, 10)
train_loss: 0.686853289604187, val_loss: 0.7606968879699707

('sgd', 0.1, 16, 50)
train_loss: 0.6847361326217651, val_loss: 0.761276125907898

('sgd', 0.1, 16, 100)
train_loss: 0.6847833395004272, val_loss: 0.7391451597213745

('sgd', 0.1, 32, 10)
train_loss: 0.6941176056861877, val_loss: 0.7772258520126343

('sgd', 0.1, 32, 50)
train_loss: 0.6884645223617554, val_loss: 0.7090295553207397

('sgd', 0.1, 32, 100)
train_loss: 0.684386670589447, val_loss: 0.767362654209137

('sgd', 0.1, 64, 10)
train_loss: 0.6880353689193726, val_loss: 0.7133066058158875

('sgd', 0.1, 64, 50)
train_loss: 0.702318549156189, val_loss: 0.6712780594825745

('sgd', 0.1, 64, 100)
train_loss: 0.6846368908882141, val_loss: 0.7330334186553955


---------
BEST MODEL
('adam', 0.001, 16, 100)
val_loss: 0.6350051164627075
---------
