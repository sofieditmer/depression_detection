
Run from 2023-04-02 11:09:49.972768
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.01, 50, 100)
train_loss: 0.6615201234817505, val_loss: 0.6681506633758545

('adam', 0.01, 50, 200)
train_loss: 0.6331361532211304, val_loss: 0.646743893623352

('adam', 0.01, 50, 300)
train_loss: 0.6404778361320496, val_loss: 0.6549683809280396

('adam', 0.01, 100, 100)
train_loss: 0.6593275666236877, val_loss: 0.6685616970062256

('adam', 0.01, 100, 200)
train_loss: 0.6073746085166931, val_loss: 0.628038763999939

('adam', 0.01, 100, 300)
train_loss: 0.669809877872467, val_loss: 0.6760584115982056

('adam', 0.01, 200, 100)
train_loss: 0.6852544546127319, val_loss: 0.6879737377166748

('adam', 0.01, 200, 200)
train_loss: 0.6524319052696228, val_loss: 0.6633273959159851

('adam', 0.01, 200, 300)
train_loss: 0.6671018004417419, val_loss: 0.6740362644195557

('adam', 0.1, 50, 100)
train_loss: 0.579010546207428, val_loss: 0.6058522462844849

('adam', 0.1, 50, 200)
train_loss: 0.5745623111724854, val_loss: 0.5953933000564575

('adam', 0.1, 50, 300)
train_loss: 0.5614755153656006, val_loss: 0.598976731300354

('adam', 0.1, 100, 100)
train_loss: 0.562089204788208, val_loss: 0.594326913356781

('adam', 0.1, 100, 200)
train_loss: 0.5570820569992065, val_loss: 0.5904848575592041

('adam', 0.1, 100, 300)
train_loss: 0.5570751428604126, val_loss: 0.5904873013496399

('adam', 0.1, 200, 100)
train_loss: 0.561005175113678, val_loss: 0.5935499668121338

('adam', 0.1, 200, 200)
train_loss: 0.557090699672699, val_loss: 0.5904822945594788

('adam', 0.1, 200, 300)
train_loss: 0.5570751428604126, val_loss: 0.5904873609542847

('adam', 0.2, 50, 100)
train_loss: 0.5684851408004761, val_loss: 0.5917931795120239

('adam', 0.2, 50, 200)
train_loss: 0.5603408217430115, val_loss: 0.5873448252677917

('adam', 0.2, 50, 300)
train_loss: 0.5584346652030945, val_loss: 0.591497004032135

('adam', 0.2, 100, 100)
train_loss: 0.5570812225341797, val_loss: 0.590522289276123

('adam', 0.2, 100, 200)
train_loss: 0.5570750832557678, val_loss: 0.5904873609542847

('adam', 0.2, 100, 300)
train_loss: 0.5570751428604126, val_loss: 0.5904872417449951

('adam', 0.2, 200, 100)
train_loss: 0.5570753812789917, val_loss: 0.5904206037521362

('adam', 0.2, 200, 200)
train_loss: 0.5570750832557678, val_loss: 0.5904876589775085

('adam', 0.2, 200, 300)
train_loss: 0.5570751428604126, val_loss: 0.5904873013496399


---------
BEST MODEL
('adam', 0.2, 50, 200)
val_loss: 0.5873448252677917
---------

Run from 2023-04-05 16:51:47.055928
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6830157041549683, val_loss: 0.682144045829773

('adam', 0.001, 8, 50)
train_loss: 0.6672295928001404, val_loss: 0.6702718734741211

('adam', 0.001, 8, 100)
train_loss: 0.6782300472259521, val_loss: 0.6812042593955994

('adam', 0.001, 16, 10)
train_loss: 0.7330926656723022, val_loss: 0.7295091152191162

('adam', 0.001, 16, 50)
train_loss: 0.7018376588821411, val_loss: 0.7013185024261475

('adam', 0.001, 16, 100)
train_loss: 0.6940589547157288, val_loss: 0.6941003799438477

('adam', 0.001, 32, 10)
train_loss: 0.6821531057357788, val_loss: 0.6827132701873779

('adam', 0.001, 32, 50)
train_loss: 0.6761859059333801, val_loss: 0.6779032349586487

('adam', 0.001, 32, 100)
train_loss: 0.68833988904953, val_loss: 0.6894283890724182

('adam', 0.001, 64, 10)
train_loss: 0.680610716342926, val_loss: 0.6808172464370728

('adam', 0.001, 64, 50)
train_loss: 0.7398611307144165, val_loss: 0.7349914312362671

('adam', 0.001, 64, 100)
train_loss: 0.7115377187728882, val_loss: 0.708738386631012

('adam', 0.01, 8, 10)
train_loss: 0.7035462856292725, val_loss: 0.7008755803108215

('adam', 0.01, 8, 50)
train_loss: 0.6610993146896362, val_loss: 0.6677007675170898

('adam', 0.01, 8, 100)
train_loss: 0.6299086213111877, val_loss: 0.6449411511421204

('adam', 0.01, 16, 10)
train_loss: 0.7334166169166565, val_loss: 0.7257498502731323

('adam', 0.01, 16, 50)
train_loss: 0.6571880578994751, val_loss: 0.6658713817596436

('adam', 0.01, 16, 100)
train_loss: 0.6205364465713501, val_loss: 0.6364453434944153

('adam', 0.01, 32, 10)
train_loss: 0.6759838461875916, val_loss: 0.6783220767974854

('adam', 0.01, 32, 50)
train_loss: 0.6463122963905334, val_loss: 0.656753659248352

('adam', 0.01, 32, 100)
train_loss: 0.6334031820297241, val_loss: 0.6470285654067993

('adam', 0.01, 64, 10)
train_loss: 0.6779507994651794, val_loss: 0.6771112084388733

('adam', 0.01, 64, 50)
train_loss: 0.6593682765960693, val_loss: 0.6676867604255676

('adam', 0.01, 64, 100)
train_loss: 0.6996821165084839, val_loss: 0.6986606121063232

('adam', 0.1, 8, 10)
train_loss: 0.6316479444503784, val_loss: 0.643649160861969

('adam', 0.1, 8, 50)
train_loss: 0.5634547472000122, val_loss: 0.5919560194015503

('adam', 0.1, 8, 100)
train_loss: 0.5615915060043335, val_loss: 0.5932766795158386

('adam', 0.1, 16, 10)
train_loss: 0.6314820647239685, val_loss: 0.6458953619003296

('adam', 0.1, 16, 50)
train_loss: 0.5762091875076294, val_loss: 0.6028845310211182

('adam', 0.1, 16, 100)
train_loss: 0.5682269930839539, val_loss: 0.6056308150291443

('adam', 0.1, 32, 10)
train_loss: 0.6297778487205505, val_loss: 0.6420842409133911

('adam', 0.1, 32, 50)
train_loss: 0.5790424942970276, val_loss: 0.607586145401001

('adam', 0.1, 32, 100)
train_loss: 0.5596809387207031, val_loss: 0.5910217761993408

('adam', 0.1, 64, 10)
train_loss: 0.6456490755081177, val_loss: 0.6578766703605652

('adam', 0.1, 64, 50)
train_loss: 0.5777625441551208, val_loss: 0.6059209108352661

('adam', 0.1, 64, 100)
train_loss: 0.5593953728675842, val_loss: 0.5920934677124023


---------
BEST MODEL
('adam', 0.1, 32, 100)
val_loss: 0.5910217761993408
---------

Run from 2023-04-10 10:39:13.283271
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.681878387928009, val_loss: 0.6813202500343323

('adam', 0.001, 8, 50)
train_loss: 0.6700287461280823, val_loss: 0.6730952858924866

('adam', 0.001, 8, 100)
train_loss: 0.7435671091079712, val_loss: 0.7347011566162109

('adam', 0.001, 16, 10)
train_loss: 0.6807466149330139, val_loss: 0.6805216073989868

('adam', 0.001, 16, 50)
train_loss: 0.7652750611305237, val_loss: 0.7566890716552734

('adam', 0.001, 16, 100)
train_loss: 0.6606980562210083, val_loss: 0.6649109721183777

('adam', 0.001, 32, 10)
train_loss: 0.6856728792190552, val_loss: 0.6861269474029541

('adam', 0.001, 32, 50)
train_loss: 0.7549168467521667, val_loss: 0.7480543255805969

('adam', 0.001, 32, 100)
train_loss: 0.6705432534217834, val_loss: 0.6735602617263794

('adam', 0.001, 64, 10)
train_loss: 0.6807912588119507, val_loss: 0.6808077096939087

('adam', 0.001, 64, 50)
train_loss: 0.7106841206550598, val_loss: 0.7085204124450684

('adam', 0.001, 64, 100)
train_loss: 0.6765220165252686, val_loss: 0.6788406372070312

('adam', 0.01, 8, 10)
train_loss: 0.7095363140106201, val_loss: 0.7054046392440796

('adam', 0.01, 8, 50)
train_loss: 0.6439507603645325, val_loss: 0.654900312423706

('adam', 0.01, 8, 100)
train_loss: 0.6309676766395569, val_loss: 0.6443465948104858

('adam', 0.01, 16, 10)
train_loss: 0.6984782814979553, val_loss: 0.6963465809822083

('adam', 0.01, 16, 50)
train_loss: 0.6377907991409302, val_loss: 0.6499525904655457

('adam', 0.01, 16, 100)
train_loss: 0.6400165557861328, val_loss: 0.6523292064666748

('adam', 0.01, 32, 10)
train_loss: 0.6989970803260803, val_loss: 0.69803786277771

('adam', 0.01, 32, 50)
train_loss: 0.6688860654830933, val_loss: 0.6745208501815796

('adam', 0.01, 32, 100)
train_loss: 0.6855764985084534, val_loss: 0.687096118927002

('adam', 0.01, 64, 10)
train_loss: 0.6889118552207947, val_loss: 0.6905600428581238

('adam', 0.01, 64, 50)
train_loss: 0.6732015013694763, val_loss: 0.6788702607154846

('adam', 0.01, 64, 100)
train_loss: 0.6333226561546326, val_loss: 0.646480917930603

('adam', 0.1, 8, 10)
train_loss: 0.6306740641593933, val_loss: 0.6407787799835205

('adam', 0.1, 8, 50)
train_loss: 0.5669591426849365, val_loss: 0.6000384092330933

('adam', 0.1, 8, 100)
train_loss: 0.5617738366127014, val_loss: 0.5919448137283325

('adam', 0.1, 16, 10)
train_loss: 0.6233834028244019, val_loss: 0.6361963748931885

('adam', 0.1, 16, 50)
train_loss: 0.5780012011528015, val_loss: 0.6027905344963074

('adam', 0.1, 16, 100)
train_loss: 0.5620496869087219, val_loss: 0.5886932611465454

('adam', 0.1, 32, 10)
train_loss: 0.6318978071212769, val_loss: 0.6451592445373535

('adam', 0.1, 32, 50)
train_loss: 0.5713680982589722, val_loss: 0.5999735593795776

('adam', 0.1, 32, 100)
train_loss: 0.5625193119049072, val_loss: 0.5986511707305908

('adam', 0.1, 64, 10)
train_loss: 0.7253440618515015, val_loss: 0.7133344411849976

('adam', 0.1, 64, 50)
train_loss: 0.5885535478591919, val_loss: 0.6142321825027466

('adam', 0.1, 64, 100)
train_loss: 0.5596334934234619, val_loss: 0.5924234390258789


---------
BEST MODEL
('adam', 0.1, 16, 100)
val_loss: 0.5886932611465454
---------

Run from 2023-04-10 13:27:47.636549
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7530981302261353, val_loss: 0.7925352454185486

('adam', 0.001, 8, 50)
train_loss: 0.6819902658462524, val_loss: 0.6634790897369385

('adam', 0.001, 8, 100)
train_loss: 0.6721740961074829, val_loss: 0.6406205296516418

('adam', 0.001, 16, 10)
train_loss: 0.6910171508789062, val_loss: 0.655075192451477

('adam', 0.001, 16, 50)
train_loss: 0.6820592284202576, val_loss: 0.654737114906311

('adam', 0.001, 16, 100)
train_loss: 0.6962082982063293, val_loss: 0.7005245685577393

('adam', 0.001, 32, 10)
train_loss: 0.7618237137794495, val_loss: 0.8043420314788818

('adam', 0.001, 32, 50)
train_loss: 0.6896366477012634, val_loss: 0.6457558870315552

('adam', 0.001, 32, 100)
train_loss: 0.6846145987510681, val_loss: 0.6737399101257324

('adam', 0.001, 64, 10)
train_loss: 0.6934069395065308, val_loss: 0.6561335325241089

('adam', 0.001, 64, 50)
train_loss: 0.7496456503868103, val_loss: 0.7887254953384399

('adam', 0.001, 64, 100)
train_loss: 0.6829332113265991, val_loss: 0.6633155941963196

('adam', 0.01, 8, 10)
train_loss: 0.680230975151062, val_loss: 0.6638243198394775

('adam', 0.01, 8, 50)
train_loss: 0.6985884308815002, val_loss: 0.7032010555267334

('adam', 0.01, 8, 100)
train_loss: 0.6444960832595825, val_loss: 0.5829113721847534

('adam', 0.01, 16, 10)
train_loss: 0.6982157826423645, val_loss: 0.7040321826934814

('adam', 0.01, 16, 50)
train_loss: 0.7097775340080261, val_loss: 0.7261596322059631

('adam', 0.01, 16, 100)
train_loss: 0.6591178178787231, val_loss: 0.6212329268455505

('adam', 0.01, 32, 10)
train_loss: 0.6851297616958618, val_loss: 0.6738560199737549

('adam', 0.01, 32, 50)
train_loss: 0.7093971967697144, val_loss: 0.7252854108810425

('adam', 0.01, 32, 100)
train_loss: 0.6675592660903931, val_loss: 0.6397563219070435

('adam', 0.01, 64, 10)
train_loss: 0.7543014883995056, val_loss: 0.7945582270622253

('adam', 0.01, 64, 50)
train_loss: 0.700993537902832, val_loss: 0.709705650806427

('adam', 0.01, 64, 100)
train_loss: 0.6573778390884399, val_loss: 0.6160520911216736

('adam', 0.1, 8, 10)
train_loss: 0.682006299495697, val_loss: 0.6526196002960205

('adam', 0.1, 8, 50)
train_loss: 0.6227447986602783, val_loss: 0.4897177815437317

('adam', 0.1, 8, 100)
train_loss: 0.6187809705734253, val_loss: 0.47277596592903137

('adam', 0.1, 16, 10)
train_loss: 0.6554938554763794, val_loss: 0.6072938442230225

('adam', 0.1, 16, 50)
train_loss: 0.6371279358863831, val_loss: 0.5311099290847778

('adam', 0.1, 16, 100)
train_loss: 0.6227750778198242, val_loss: 0.4959324598312378

('adam', 0.1, 32, 10)
train_loss: 0.6587477326393127, val_loss: 0.6124919056892395

('adam', 0.1, 32, 50)
train_loss: 0.6346001029014587, val_loss: 0.5493413209915161

('adam', 0.1, 32, 100)
train_loss: 0.6181904673576355, val_loss: 0.4788227081298828

('adam', 0.1, 64, 10)
train_loss: 0.7271558046340942, val_loss: 0.7418617606163025

('adam', 0.1, 64, 50)
train_loss: 0.6221309304237366, val_loss: 0.5060564875602722

('adam', 0.1, 64, 100)
train_loss: 0.6169179677963257, val_loss: 0.4637477993965149


---------
BEST MODEL
('adam', 0.1, 64, 100)
val_loss: 0.4637477993965149
---------

Run from 2023-04-10 14:58:50.339037
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7768431901931763, val_loss: 0.7881858348846436

('adam', 0.001, 8, 50)
train_loss: 0.6758070588111877, val_loss: 0.6658011674880981

('adam', 0.001, 8, 100)
train_loss: 0.664207398891449, val_loss: 0.6516460180282593

('adam', 0.001, 16, 10)
train_loss: 0.6880316138267517, val_loss: 0.6667921543121338

('adam', 0.001, 16, 50)
train_loss: 0.6778091788291931, val_loss: 0.6597026586532593

('adam', 0.001, 16, 100)
train_loss: 0.6752206087112427, val_loss: 0.6677345037460327

('adam', 0.001, 32, 10)
train_loss: 0.6876093745231628, val_loss: 0.6684379577636719

('adam', 0.001, 32, 50)
train_loss: 0.7411797642707825, val_loss: 0.7510324120521545

('adam', 0.001, 32, 100)
train_loss: 0.7437732219696045, val_loss: 0.7538893818855286

('adam', 0.001, 64, 10)
train_loss: 0.7782043814659119, val_loss: 0.7900221347808838

('adam', 0.001, 64, 50)
train_loss: 0.6887053847312927, val_loss: 0.6875223517417908

('adam', 0.001, 64, 100)
train_loss: 0.7136945128440857, val_loss: 0.719632625579834

('adam', 0.01, 8, 10)
train_loss: 0.6698590517044067, val_loss: 0.6592985987663269

('adam', 0.01, 8, 50)
train_loss: 0.6971349120140076, val_loss: 0.6969809532165527

('adam', 0.01, 8, 100)
train_loss: 0.662344217300415, val_loss: 0.6546283960342407

('adam', 0.01, 16, 10)
train_loss: 0.6775984168052673, val_loss: 0.6677355766296387

('adam', 0.01, 16, 50)
train_loss: 0.6616224646568298, val_loss: 0.6511157751083374

('adam', 0.01, 16, 100)
train_loss: 0.6544163823127747, val_loss: 0.6446479558944702

('adam', 0.01, 32, 10)
train_loss: 0.6749556660652161, val_loss: 0.6579384803771973

('adam', 0.01, 32, 50)
train_loss: 0.6545091867446899, val_loss: 0.6444261074066162

('adam', 0.01, 32, 100)
train_loss: 0.6461843252182007, val_loss: 0.6341798305511475

('adam', 0.01, 64, 10)
train_loss: 0.6795005798339844, val_loss: 0.6630561947822571

('adam', 0.01, 64, 50)
train_loss: 0.6926112771034241, val_loss: 0.6935408115386963

('adam', 0.01, 64, 100)
train_loss: 0.7183932065963745, val_loss: 0.7235103249549866

('adam', 0.1, 8, 10)
train_loss: 0.6729716062545776, val_loss: 0.6585122346878052

('adam', 0.1, 8, 50)
train_loss: 0.5885193347930908, val_loss: 0.5439532995223999

('adam', 0.1, 8, 100)
train_loss: 0.5845704674720764, val_loss: 0.5366498231887817

('adam', 0.1, 16, 10)
train_loss: 0.6858135461807251, val_loss: 0.6832279562950134

('adam', 0.1, 16, 50)
train_loss: 0.5994645357131958, val_loss: 0.5669242143630981

('adam', 0.1, 16, 100)
train_loss: 0.5883745551109314, val_loss: 0.5279572606086731

('adam', 0.1, 32, 10)
train_loss: 0.6461029052734375, val_loss: 0.6329467296600342

('adam', 0.1, 32, 50)
train_loss: 0.5991758108139038, val_loss: 0.5707497596740723

('adam', 0.1, 32, 100)
train_loss: 0.5894972085952759, val_loss: 0.5436598062515259

('adam', 0.1, 64, 10)
train_loss: 0.6927449703216553, val_loss: 0.6897532343864441

('adam', 0.1, 64, 50)
train_loss: 0.6088399291038513, val_loss: 0.5836224555969238

('adam', 0.1, 64, 100)
train_loss: 0.583577573299408, val_loss: 0.5364811420440674


---------
BEST MODEL
('adam', 0.1, 16, 100)
val_loss: 0.5279572606086731
---------

Run from 2023-04-17 10:09:27.507806
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7377272844314575, val_loss: 0.7492731213569641

('adam', 0.001, 8, 50)
train_loss: 0.703977644443512, val_loss: 0.707177460193634

('adam', 0.001, 8, 100)
train_loss: 0.6641238331794739, val_loss: 0.6559249758720398

('adam', 0.001, 16, 10)
train_loss: 0.7230936884880066, val_loss: 0.7317276000976562

('adam', 0.001, 16, 50)
train_loss: 0.6898632049560547, val_loss: 0.6891065239906311

('adam', 0.001, 16, 100)
train_loss: 0.668476939201355, val_loss: 0.6596455574035645

('adam', 0.001, 32, 10)
train_loss: 0.6858484745025635, val_loss: 0.6753215193748474

('adam', 0.001, 32, 50)
train_loss: 0.7713731527328491, val_loss: 0.7890650629997253

('adam', 0.001, 32, 100)
train_loss: 0.6779817938804626, val_loss: 0.6730052828788757

('adam', 0.001, 64, 10)
train_loss: 0.6901952028274536, val_loss: 0.6775113940238953

('adam', 0.001, 64, 50)
train_loss: 0.686364471912384, val_loss: 0.6842482686042786

('adam', 0.001, 64, 100)
train_loss: 0.6853011846542358, val_loss: 0.6710615158081055

('adam', 0.01, 8, 10)
train_loss: 0.6608453392982483, val_loss: 0.6486756205558777

('adam', 0.01, 8, 50)
train_loss: 0.6549554467201233, val_loss: 0.6464446187019348

('adam', 0.01, 8, 100)
train_loss: 0.6474246978759766, val_loss: 0.6362696290016174

('adam', 0.01, 16, 10)
train_loss: 0.6766763925552368, val_loss: 0.6721654534339905

('adam', 0.01, 16, 50)
train_loss: 0.68299800157547, val_loss: 0.6806797981262207

('adam', 0.01, 16, 100)
train_loss: 0.6528527736663818, val_loss: 0.6435415148735046

('adam', 0.01, 32, 10)
train_loss: 0.7278550267219543, val_loss: 0.734887421131134

('adam', 0.01, 32, 50)
train_loss: 0.7247874736785889, val_loss: 0.7286853790283203

('adam', 0.01, 32, 100)
train_loss: 0.6809737086296082, val_loss: 0.678736686706543

('adam', 0.01, 64, 10)
train_loss: 0.7137920260429382, val_loss: 0.7186185717582703

('adam', 0.01, 64, 50)
train_loss: 0.6771820187568665, val_loss: 0.6749171614646912

('adam', 0.01, 64, 100)
train_loss: 0.6467243432998657, val_loss: 0.6353002190589905

('adam', 0.1, 8, 10)
train_loss: 0.6471014618873596, val_loss: 0.6307557821273804

('adam', 0.1, 8, 50)
train_loss: 0.5987440347671509, val_loss: 0.545746922492981

('adam', 0.1, 8, 100)
train_loss: 0.5931394100189209, val_loss: 0.5319531559944153

('adam', 0.1, 16, 10)
train_loss: 0.6524326205253601, val_loss: 0.6391969323158264

('adam', 0.1, 16, 50)
train_loss: 0.5978893041610718, val_loss: 0.5529194474220276

('adam', 0.1, 16, 100)
train_loss: 0.5930442810058594, val_loss: 0.5348806381225586

('adam', 0.1, 32, 10)
train_loss: 0.698016345500946, val_loss: 0.6944535374641418

('adam', 0.1, 32, 50)
train_loss: 0.6004135012626648, val_loss: 0.5598981380462646

('adam', 0.1, 32, 100)
train_loss: 0.5947453379631042, val_loss: 0.5384742617607117

('adam', 0.1, 64, 10)
train_loss: 0.6586970686912537, val_loss: 0.6518178582191467

('adam', 0.1, 64, 50)
train_loss: 0.609664797782898, val_loss: 0.5785092711448669

('adam', 0.1, 64, 100)
train_loss: 0.5930317640304565, val_loss: 0.5343652367591858

('sgd', 0.001, 8, 10)
train_loss: 0.7520959377288818, val_loss: 0.7668318748474121

('sgd', 0.001, 8, 50)
train_loss: 0.6852503418922424, val_loss: 0.6812410950660706

('sgd', 0.001, 8, 100)
train_loss: 0.7497330904006958, val_loss: 0.7636516690254211

('sgd', 0.001, 16, 10)
train_loss: 0.7209308743476868, val_loss: 0.7293059229850769

('sgd', 0.001, 16, 50)
train_loss: 0.685238778591156, val_loss: 0.6805999875068665

('sgd', 0.001, 16, 100)
train_loss: 0.6841293573379517, val_loss: 0.6791197657585144

('sgd', 0.001, 32, 10)
train_loss: 0.7401235103607178, val_loss: 0.7526402473449707

('sgd', 0.001, 32, 50)
train_loss: 0.6962602138519287, val_loss: 0.6975555419921875

('sgd', 0.001, 32, 100)
train_loss: 0.6866814494132996, val_loss: 0.6752148270606995

('sgd', 0.001, 64, 10)
train_loss: 0.6905058026313782, val_loss: 0.6781414151191711

('sgd', 0.001, 64, 50)
train_loss: 0.6870850920677185, val_loss: 0.6764058470726013

('sgd', 0.001, 64, 100)
train_loss: 0.6880162358283997, val_loss: 0.6763972640037537

('sgd', 0.01, 8, 10)
train_loss: 0.7415850162506104, val_loss: 0.7531901001930237

('sgd', 0.01, 8, 50)
train_loss: 0.6660093069076538, val_loss: 0.6583700180053711

('sgd', 0.01, 8, 100)
train_loss: 0.6990326642990112, val_loss: 0.7002725005149841

('sgd', 0.01, 16, 10)
train_loss: 0.7483208775520325, val_loss: 0.7618937492370605

('sgd', 0.01, 16, 50)
train_loss: 0.6715705990791321, val_loss: 0.662211537361145

('sgd', 0.01, 16, 100)
train_loss: 0.7153695225715637, val_loss: 0.720698356628418

('sgd', 0.01, 32, 10)
train_loss: 0.7409356236457825, val_loss: 0.7533769607543945

('sgd', 0.01, 32, 50)
train_loss: 0.7152249217033386, val_loss: 0.7217867970466614

('sgd', 0.01, 32, 100)
train_loss: 0.6710659265518188, val_loss: 0.6612679958343506

('sgd', 0.01, 64, 10)
train_loss: 0.7275269627571106, val_loss: 0.7373228073120117

('sgd', 0.01, 64, 50)
train_loss: 0.7669525146484375, val_loss: 0.7840602993965149

('sgd', 0.01, 64, 100)
train_loss: 0.6789973974227905, val_loss: 0.671346127986908

('sgd', 0.1, 8, 10)
train_loss: 0.6933780908584595, val_loss: 0.6923444867134094

('sgd', 0.1, 8, 50)
train_loss: 0.6976751685142517, val_loss: 0.6968104839324951

('sgd', 0.1, 8, 100)
train_loss: 0.6379586458206177, val_loss: 0.6229085326194763

('sgd', 0.1, 16, 10)
train_loss: 0.7433418035507202, val_loss: 0.7508916854858398

('sgd', 0.1, 16, 50)
train_loss: 0.6463742852210999, val_loss: 0.6343153119087219

('sgd', 0.1, 16, 100)
train_loss: 0.6692459583282471, val_loss: 0.6615581512451172

('sgd', 0.1, 32, 10)
train_loss: 0.7339642643928528, val_loss: 0.7430801391601562

('sgd', 0.1, 32, 50)
train_loss: 0.6558428406715393, val_loss: 0.6476597189903259

('sgd', 0.1, 32, 100)
train_loss: 0.6499227285385132, val_loss: 0.640285313129425

('sgd', 0.1, 64, 10)
train_loss: 0.6806575655937195, val_loss: 0.6664892435073853

('sgd', 0.1, 64, 50)
train_loss: 0.7132396697998047, val_loss: 0.717792809009552

('sgd', 0.1, 64, 100)
train_loss: 0.6868705749511719, val_loss: 0.6862275004386902


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5319531559944153
---------

Run from 2023-04-17 17:08:03.221147
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7431333661079407, val_loss: 0.7789357900619507

('adam', 0.001, 8, 50)
train_loss: 0.6767939329147339, val_loss: 0.6469172835350037

('adam', 0.001, 8, 100)
train_loss: 0.6699903607368469, val_loss: 0.6391155123710632

('adam', 0.001, 16, 10)
train_loss: 0.6892186403274536, val_loss: 0.6823026537895203

('adam', 0.001, 16, 50)
train_loss: 0.742415726184845, val_loss: 0.7804684638977051

('adam', 0.001, 16, 100)
train_loss: 0.6739981174468994, val_loss: 0.6400516033172607

('adam', 0.001, 32, 10)
train_loss: 0.7422268390655518, val_loss: 0.7777430415153503

('adam', 0.001, 32, 50)
train_loss: 0.6908156275749207, val_loss: 0.6883756518363953

('adam', 0.001, 32, 100)
train_loss: 0.6802188754081726, val_loss: 0.6476305723190308

('adam', 0.001, 64, 10)
train_loss: 0.7310174107551575, val_loss: 0.7607361078262329

('adam', 0.001, 64, 50)
train_loss: 0.6920575499534607, val_loss: 0.6919991970062256

('adam', 0.001, 64, 100)
train_loss: 0.6825868487358093, val_loss: 0.662807047367096

('adam', 0.01, 8, 10)
train_loss: 0.7017272114753723, val_loss: 0.7100717425346375

('adam', 0.01, 8, 50)
train_loss: 0.6746569275856018, val_loss: 0.6549937725067139

('adam', 0.01, 8, 100)
train_loss: 0.6479080319404602, val_loss: 0.5963476896286011

('adam', 0.01, 16, 10)
train_loss: 0.675778329372406, val_loss: 0.6538238525390625

('adam', 0.01, 16, 50)
train_loss: 0.6581450700759888, val_loss: 0.620818018913269

('adam', 0.01, 16, 100)
train_loss: 0.6535660028457642, val_loss: 0.6093157529830933

('adam', 0.01, 32, 10)
train_loss: 0.682622492313385, val_loss: 0.6651833057403564

('adam', 0.01, 32, 50)
train_loss: 0.6632211804389954, val_loss: 0.6315029859542847

('adam', 0.01, 32, 100)
train_loss: 0.6688193082809448, val_loss: 0.6438381671905518

('adam', 0.01, 64, 10)
train_loss: 0.6933070421218872, val_loss: 0.6500194072723389

('adam', 0.01, 64, 50)
train_loss: 0.6699805855751038, val_loss: 0.6464489102363586

('adam', 0.01, 64, 100)
train_loss: 0.7182345986366272, val_loss: 0.7401580810546875

('adam', 0.1, 8, 10)
train_loss: 0.6584354639053345, val_loss: 0.6140539050102234

('adam', 0.1, 8, 50)
train_loss: 0.6303265690803528, val_loss: 0.5176275372505188

('adam', 0.1, 8, 100)
train_loss: 0.6261502504348755, val_loss: 0.4952947497367859

('adam', 0.1, 16, 10)
train_loss: 0.653866708278656, val_loss: 0.6035040020942688

('adam', 0.1, 16, 50)
train_loss: 0.6335063576698303, val_loss: 0.5499070286750793

('adam', 0.1, 16, 100)
train_loss: 0.6253187656402588, val_loss: 0.5065486431121826

('adam', 0.1, 32, 10)
train_loss: 0.6577969789505005, val_loss: 0.6173137426376343

('adam', 0.1, 32, 50)
train_loss: 0.62897127866745, val_loss: 0.5374928116798401

('adam', 0.1, 32, 100)
train_loss: 0.6234990358352661, val_loss: 0.5044606924057007

('adam', 0.1, 64, 10)
train_loss: 0.6950051784515381, val_loss: 0.692682683467865

('adam', 0.1, 64, 50)
train_loss: 0.6528707146644592, val_loss: 0.6067125201225281

('adam', 0.1, 64, 100)
train_loss: 0.6226722002029419, val_loss: 0.49750813841819763

('sgd', 0.001, 8, 10)
train_loss: 0.6879927515983582, val_loss: 0.6708912253379822

('sgd', 0.001, 8, 50)
train_loss: 0.7387410402297974, val_loss: 0.7731009721755981

('sgd', 0.001, 8, 100)
train_loss: 0.7204790711402893, val_loss: 0.7448129057884216

('sgd', 0.001, 16, 10)
train_loss: 0.7694750428199768, val_loss: 0.8164872527122498

('sgd', 0.001, 16, 50)
train_loss: 0.729529082775116, val_loss: 0.7586890459060669

('sgd', 0.001, 16, 100)
train_loss: 0.6905176639556885, val_loss: 0.6865635514259338

('sgd', 0.001, 32, 10)
train_loss: 0.6903676390647888, val_loss: 0.6649637818336487

('sgd', 0.001, 32, 50)
train_loss: 0.7497699856758118, val_loss: 0.7887676358222961

('sgd', 0.001, 32, 100)
train_loss: 0.7098332643508911, val_loss: 0.7265185117721558

('sgd', 0.001, 64, 10)
train_loss: 0.6957399845123291, val_loss: 0.6616921424865723

('sgd', 0.001, 64, 50)
train_loss: 0.6984300017356873, val_loss: 0.6605970859527588

('sgd', 0.001, 64, 100)
train_loss: 0.7076331973075867, val_loss: 0.6604874730110168

('sgd', 0.01, 8, 10)
train_loss: 0.6957589387893677, val_loss: 0.6482364535331726

('sgd', 0.01, 8, 50)
train_loss: 0.7307978272438049, val_loss: 0.7639915943145752

('sgd', 0.01, 8, 100)
train_loss: 0.6979964375495911, val_loss: 0.7032111883163452

('sgd', 0.01, 16, 10)
train_loss: 0.7608094811439514, val_loss: 0.8048886060714722

('sgd', 0.01, 16, 50)
train_loss: 0.67757248878479, val_loss: 0.6467506885528564

('sgd', 0.01, 16, 100)
train_loss: 0.6860353946685791, val_loss: 0.6791182160377502

('sgd', 0.01, 32, 10)
train_loss: 0.6875093579292297, val_loss: 0.6697839498519897

('sgd', 0.01, 32, 50)
train_loss: 0.6867651343345642, val_loss: 0.6766447424888611

('sgd', 0.01, 32, 100)
train_loss: 0.6960781216621399, val_loss: 0.7001116275787354

('sgd', 0.01, 64, 10)
train_loss: 0.6885554194450378, val_loss: 0.6783826351165771

('sgd', 0.01, 64, 50)
train_loss: 0.7180887460708618, val_loss: 0.7406473159790039

('sgd', 0.01, 64, 100)
train_loss: 0.7152066826820374, val_loss: 0.7360774278640747

('sgd', 0.1, 8, 10)
train_loss: 0.6930253505706787, val_loss: 0.6885892152786255

('sgd', 0.1, 8, 50)
train_loss: 0.6665611267089844, val_loss: 0.6366770267486572

('sgd', 0.1, 8, 100)
train_loss: 0.652336835861206, val_loss: 0.6049249172210693

('sgd', 0.1, 16, 10)
train_loss: 0.7292551398277283, val_loss: 0.7597540020942688

('sgd', 0.1, 16, 50)
train_loss: 0.7115173935890198, val_loss: 0.7272599935531616

('sgd', 0.1, 16, 100)
train_loss: 0.6597846746444702, val_loss: 0.6206721663475037

('sgd', 0.1, 32, 10)
train_loss: 0.731426477432251, val_loss: 0.7628005743026733

('sgd', 0.1, 32, 50)
train_loss: 0.6725645661354065, val_loss: 0.6520451307296753

('sgd', 0.1, 32, 100)
train_loss: 0.6753775477409363, val_loss: 0.6586092710494995

('sgd', 0.1, 64, 10)
train_loss: 0.6900497078895569, val_loss: 0.6459687948226929

('sgd', 0.1, 64, 50)
train_loss: 0.7185486555099487, val_loss: 0.7424241304397583

('sgd', 0.1, 64, 100)
train_loss: 0.701700747013092, val_loss: 0.7105164527893066


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.4952947497367859
---------
