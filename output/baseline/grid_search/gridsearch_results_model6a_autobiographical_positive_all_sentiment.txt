
Run from 2023-04-10 10:44:49.947212
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6918200254440308, val_loss: 0.6822915077209473

('adam', 0.001, 8, 50)
train_loss: 0.6927335262298584, val_loss: 0.6939122676849365

('adam', 0.001, 8, 100)
train_loss: 0.6899540424346924, val_loss: 0.6874071359634399

('adam', 0.001, 16, 10)
train_loss: 0.729418158531189, val_loss: 0.7169613838195801

('adam', 0.001, 16, 50)
train_loss: 0.6886330246925354, val_loss: 0.6772778034210205

('adam', 0.001, 16, 100)
train_loss: 0.6957234144210815, val_loss: 0.6756589412689209

('adam', 0.001, 32, 10)
train_loss: 0.7744910717010498, val_loss: 0.8061636686325073

('adam', 0.001, 32, 50)
train_loss: 0.7013491988182068, val_loss: 0.7085591554641724

('adam', 0.001, 32, 100)
train_loss: 0.7174664735794067, val_loss: 0.7007200717926025

('adam', 0.001, 64, 10)
train_loss: 0.7297008037567139, val_loss: 0.7461966276168823

('adam', 0.001, 64, 50)
train_loss: 0.6897293925285339, val_loss: 0.6797130107879639

('adam', 0.001, 64, 100)
train_loss: 0.6901566982269287, val_loss: 0.6777430772781372

('adam', 0.01, 8, 10)
train_loss: 0.693831741809845, val_loss: 0.6969712376594543

('adam', 0.01, 8, 50)
train_loss: 0.6916142106056213, val_loss: 0.6918888092041016

('adam', 0.01, 8, 100)
train_loss: 0.6712087392807007, val_loss: 0.6330803632736206

('adam', 0.01, 16, 10)
train_loss: 0.759145200252533, val_loss: 0.797319233417511

('adam', 0.01, 16, 50)
train_loss: 0.681246817111969, val_loss: 0.6619849801063538

('adam', 0.01, 16, 100)
train_loss: 0.6982493996620178, val_loss: 0.7097395658493042

('adam', 0.01, 32, 10)
train_loss: 0.734665036201477, val_loss: 0.7586126327514648

('adam', 0.01, 32, 50)
train_loss: 0.6762682199478149, val_loss: 0.6488841772079468

('adam', 0.01, 32, 100)
train_loss: 0.6844443678855896, val_loss: 0.6722347736358643

('adam', 0.01, 64, 10)
train_loss: 0.758090078830719, val_loss: 0.7862910032272339

('adam', 0.01, 64, 50)
train_loss: 0.6905362606048584, val_loss: 0.6887726187705994

('adam', 0.01, 64, 100)
train_loss: 0.6741771697998047, val_loss: 0.6426738500595093

('adam', 0.1, 8, 10)
train_loss: 0.6846466064453125, val_loss: 0.6536046266555786

('adam', 0.1, 8, 50)
train_loss: 0.6634997129440308, val_loss: 0.607079029083252

('adam', 0.1, 8, 100)
train_loss: 0.6492974162101746, val_loss: 0.5539296865463257

('adam', 0.1, 16, 10)
train_loss: 0.6782754063606262, val_loss: 0.6469348669052124

('adam', 0.1, 16, 50)
train_loss: 0.6711317300796509, val_loss: 0.6251581907272339

('adam', 0.1, 16, 100)
train_loss: 0.6559466123580933, val_loss: 0.5804113149642944

('adam', 0.1, 32, 10)
train_loss: 0.6994494199752808, val_loss: 0.7106925249099731

('adam', 0.1, 32, 50)
train_loss: 0.6813900470733643, val_loss: 0.6625111699104309

('adam', 0.1, 32, 100)
train_loss: 0.6585594415664673, val_loss: 0.590374231338501

('adam', 0.1, 64, 10)
train_loss: 0.6788102388381958, val_loss: 0.6519906520843506

('adam', 0.1, 64, 50)
train_loss: 0.6587161421775818, val_loss: 0.5906746983528137

('adam', 0.1, 64, 100)
train_loss: 0.6542763113975525, val_loss: 0.5743533968925476


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5539296865463257
---------

Run from 2023-04-10 13:32:30.461016
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7563145756721497, val_loss: 0.7712526917457581

('adam', 0.001, 8, 50)
train_loss: 0.6837212443351746, val_loss: 0.6771829128265381

('adam', 0.001, 8, 100)
train_loss: 0.6925171613693237, val_loss: 0.6939207911491394

('adam', 0.001, 16, 10)
train_loss: 0.7377113699913025, val_loss: 0.7486286163330078

('adam', 0.001, 16, 50)
train_loss: 0.6914204955101013, val_loss: 0.6912736892700195

('adam', 0.001, 16, 100)
train_loss: 0.7186301350593567, val_loss: 0.7324241399765015

('adam', 0.001, 32, 10)
train_loss: 0.7817045450210571, val_loss: 0.8000569343566895

('adam', 0.001, 32, 50)
train_loss: 0.8114410042762756, val_loss: 0.8375199437141418

('adam', 0.001, 32, 100)
train_loss: 0.712614893913269, val_loss: 0.7224589586257935

('adam', 0.001, 64, 10)
train_loss: 0.7199448943138123, val_loss: 0.7088453769683838

('adam', 0.001, 64, 50)
train_loss: 0.7546576261520386, val_loss: 0.7411694526672363

('adam', 0.001, 64, 100)
train_loss: 0.7831671833992004, val_loss: 0.8051040172576904

('adam', 0.01, 8, 10)
train_loss: 0.6763291954994202, val_loss: 0.6529229879379272

('adam', 0.01, 8, 50)
train_loss: 0.6966637969017029, val_loss: 0.7001274824142456

('adam', 0.01, 8, 100)
train_loss: 0.6565456986427307, val_loss: 0.6171765327453613

('adam', 0.01, 16, 10)
train_loss: 0.679317057132721, val_loss: 0.6632199287414551

('adam', 0.01, 16, 50)
train_loss: 0.688740074634552, val_loss: 0.6857675909996033

('adam', 0.01, 16, 100)
train_loss: 0.6613364815711975, val_loss: 0.6286158561706543

('adam', 0.01, 32, 10)
train_loss: 0.7499102354049683, val_loss: 0.7651466131210327

('adam', 0.01, 32, 50)
train_loss: 0.7205057144165039, val_loss: 0.7454538345336914

('adam', 0.01, 32, 100)
train_loss: 0.7050765156745911, val_loss: 0.7188444137573242

('adam', 0.01, 64, 10)
train_loss: 0.7992924451828003, val_loss: 0.8205642700195312

('adam', 0.01, 64, 50)
train_loss: 0.7461639046669006, val_loss: 0.7723324298858643

('adam', 0.01, 64, 100)
train_loss: 0.7114109396934509, val_loss: 0.7305254340171814

('adam', 0.1, 8, 10)
train_loss: 0.670461118221283, val_loss: 0.6367844343185425

('adam', 0.1, 8, 50)
train_loss: 0.6326754689216614, val_loss: 0.5425304174423218

('adam', 0.1, 8, 100)
train_loss: 0.6257177591323853, val_loss: 0.5053364038467407

('adam', 0.1, 16, 10)
train_loss: 0.681873619556427, val_loss: 0.6619619131088257

('adam', 0.1, 16, 50)
train_loss: 0.6459921598434448, val_loss: 0.5897655487060547

('adam', 0.1, 16, 100)
train_loss: 0.6296735405921936, val_loss: 0.5135722160339355

('adam', 0.1, 32, 10)
train_loss: 0.6696833372116089, val_loss: 0.6425303220748901

('adam', 0.1, 32, 50)
train_loss: 0.6388024687767029, val_loss: 0.5686926245689392

('adam', 0.1, 32, 100)
train_loss: 0.6284710764884949, val_loss: 0.537707507610321

('adam', 0.1, 64, 10)
train_loss: 0.6731827259063721, val_loss: 0.6522377729415894

('adam', 0.1, 64, 50)
train_loss: 0.6481656432151794, val_loss: 0.5958853363990784

('adam', 0.1, 64, 100)
train_loss: 0.6274471282958984, val_loss: 0.5314713716506958


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5053364038467407
---------

Run from 2023-04-10 15:04:00.743797
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.8618469834327698, val_loss: 0.8663616180419922

('adam', 0.001, 8, 50)
train_loss: 0.68723064661026, val_loss: 0.688278317451477

('adam', 0.001, 8, 100)
train_loss: 0.6716153025627136, val_loss: 0.6773442029953003

('adam', 0.001, 16, 10)
train_loss: 0.8093382716178894, val_loss: 0.8110179305076599

('adam', 0.001, 16, 50)
train_loss: 0.7068766951560974, val_loss: 0.706739068031311

('adam', 0.001, 16, 100)
train_loss: 0.6757969260215759, val_loss: 0.6781268119812012

('adam', 0.001, 32, 10)
train_loss: 0.7078603506088257, val_loss: 0.7214533686637878

('adam', 0.001, 32, 50)
train_loss: 0.7173408269882202, val_loss: 0.734254002571106

('adam', 0.001, 32, 100)
train_loss: 0.6848545670509338, val_loss: 0.6860262155532837

('adam', 0.001, 64, 10)
train_loss: 0.6985386610031128, val_loss: 0.6980975866317749

('adam', 0.001, 64, 50)
train_loss: 0.714311957359314, val_loss: 0.73011314868927

('adam', 0.001, 64, 100)
train_loss: 0.6807393431663513, val_loss: 0.6818476915359497

('adam', 0.01, 8, 10)
train_loss: 0.6750150918960571, val_loss: 0.674968957901001

('adam', 0.01, 8, 50)
train_loss: 0.6807675957679749, val_loss: 0.6814711093902588

('adam', 0.01, 8, 100)
train_loss: 0.6508675217628479, val_loss: 0.6452236175537109

('adam', 0.01, 16, 10)
train_loss: 0.7324603796005249, val_loss: 0.7308136820793152

('adam', 0.01, 16, 50)
train_loss: 0.6602450013160706, val_loss: 0.655829668045044

('adam', 0.01, 16, 100)
train_loss: 0.64991295337677, val_loss: 0.644241452217102

('adam', 0.01, 32, 10)
train_loss: 0.6861187219619751, val_loss: 0.6874697208404541

('adam', 0.01, 32, 50)
train_loss: 0.6807531118392944, val_loss: 0.6817518472671509

('adam', 0.01, 32, 100)
train_loss: 0.6941562294960022, val_loss: 0.6969576478004456

('adam', 0.01, 64, 10)
train_loss: 0.680792510509491, val_loss: 0.6841305494308472

('adam', 0.01, 64, 50)
train_loss: 0.6661072969436646, val_loss: 0.6676159501075745

('adam', 0.01, 64, 100)
train_loss: 0.6612322926521301, val_loss: 0.6582304835319519

('adam', 0.1, 8, 10)
train_loss: 0.6867495775222778, val_loss: 0.6840174198150635

('adam', 0.1, 8, 50)
train_loss: 0.6146985292434692, val_loss: 0.5989293456077576

('adam', 0.1, 8, 100)
train_loss: 0.5923621654510498, val_loss: 0.5555142760276794

('adam', 0.1, 16, 10)
train_loss: 0.7027453184127808, val_loss: 0.7056155204772949

('adam', 0.1, 16, 50)
train_loss: 0.615304708480835, val_loss: 0.5949748158454895

('adam', 0.1, 16, 100)
train_loss: 0.5988222360610962, val_loss: 0.5765610933303833

('adam', 0.1, 32, 10)
train_loss: 0.6717646718025208, val_loss: 0.6659287810325623

('adam', 0.1, 32, 50)
train_loss: 0.6420204639434814, val_loss: 0.6340627670288086

('adam', 0.1, 32, 100)
train_loss: 0.5964194536209106, val_loss: 0.5671238899230957

('adam', 0.1, 64, 10)
train_loss: 0.6715271472930908, val_loss: 0.6613114476203918

('adam', 0.1, 64, 50)
train_loss: 0.6032012104988098, val_loss: 0.5772120952606201

('adam', 0.1, 64, 100)
train_loss: 0.6075548529624939, val_loss: 0.5852434635162354


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5555142760276794
---------

Run from 2023-04-17 10:11:28.991276
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.6851037740707397, val_loss: 0.686790943145752

('adam', 0.001, 8, 50)
train_loss: 0.7471997737884521, val_loss: 0.7434170842170715

('adam', 0.001, 8, 100)
train_loss: 0.6774195432662964, val_loss: 0.6767986416816711

('adam', 0.001, 16, 10)
train_loss: 0.7190581560134888, val_loss: 0.7207756042480469

('adam', 0.001, 16, 50)
train_loss: 0.6935672163963318, val_loss: 0.6940639615058899

('adam', 0.001, 16, 100)
train_loss: 0.6838299036026001, val_loss: 0.6824423670768738

('adam', 0.001, 32, 10)
train_loss: 0.6974462866783142, val_loss: 0.6969590783119202

('adam', 0.001, 32, 50)
train_loss: 0.8575283288955688, val_loss: 0.8435123562812805

('adam', 0.001, 32, 100)
train_loss: 0.6881936192512512, val_loss: 0.6885105967521667

('adam', 0.001, 64, 10)
train_loss: 0.6879279017448425, val_loss: 0.6900915503501892

('adam', 0.001, 64, 50)
train_loss: 0.7511183619499207, val_loss: 0.7522571682929993

('adam', 0.001, 64, 100)
train_loss: 0.696367084980011, val_loss: 0.6977230906486511

('adam', 0.01, 8, 10)
train_loss: 0.6979281306266785, val_loss: 0.6995390057563782

('adam', 0.01, 8, 50)
train_loss: 0.7070785760879517, val_loss: 0.7116885185241699

('adam', 0.01, 8, 100)
train_loss: 0.6679357290267944, val_loss: 0.6643849015235901

('adam', 0.01, 16, 10)
train_loss: 0.7341082096099854, val_loss: 0.729557991027832

('adam', 0.01, 16, 50)
train_loss: 0.6759507060050964, val_loss: 0.6745495796203613

('adam', 0.01, 16, 100)
train_loss: 0.6592350006103516, val_loss: 0.6534934043884277

('adam', 0.01, 32, 10)
train_loss: 0.7168482542037964, val_loss: 0.7126886248588562

('adam', 0.01, 32, 50)
train_loss: 0.6654766798019409, val_loss: 0.6613799929618835

('adam', 0.01, 32, 100)
train_loss: 0.6564486026763916, val_loss: 0.6499056220054626

('adam', 0.01, 64, 10)
train_loss: 0.6889057755470276, val_loss: 0.6896103024482727

('adam', 0.01, 64, 50)
train_loss: 0.6700843572616577, val_loss: 0.6671831011772156

('adam', 0.01, 64, 100)
train_loss: 0.6739424467086792, val_loss: 0.6714861989021301

('adam', 0.1, 8, 10)
train_loss: 0.6727460622787476, val_loss: 0.6700236201286316

('adam', 0.1, 8, 50)
train_loss: 0.6191908717155457, val_loss: 0.5901579856872559

('adam', 0.1, 8, 100)
train_loss: 0.5955215692520142, val_loss: 0.546720564365387

('adam', 0.1, 16, 10)
train_loss: 0.6712706685066223, val_loss: 0.6575801968574524

('adam', 0.1, 16, 50)
train_loss: 0.6193692684173584, val_loss: 0.5928017497062683

('adam', 0.1, 16, 100)
train_loss: 0.598605215549469, val_loss: 0.5555529594421387

('adam', 0.1, 32, 10)
train_loss: 0.6630041599273682, val_loss: 0.6567860245704651

('adam', 0.1, 32, 50)
train_loss: 0.6232227683067322, val_loss: 0.6009965538978577

('adam', 0.1, 32, 100)
train_loss: 0.6060197353363037, val_loss: 0.5701242089271545

('adam', 0.1, 64, 10)
train_loss: 0.6728647947311401, val_loss: 0.66524338722229

('adam', 0.1, 64, 50)
train_loss: 0.6038011908531189, val_loss: 0.5671821236610413

('adam', 0.1, 64, 100)
train_loss: 0.5936611294746399, val_loss: 0.538356363773346

('sgd', 0.001, 8, 10)
train_loss: 0.7045854926109314, val_loss: 0.7031927108764648

('sgd', 0.001, 8, 50)
train_loss: 0.8334715962409973, val_loss: 0.8214396834373474

('sgd', 0.001, 8, 100)
train_loss: 0.7941718697547913, val_loss: 0.7857539653778076

('sgd', 0.001, 16, 10)
train_loss: 0.8351677060127258, val_loss: 0.8227233290672302

('sgd', 0.001, 16, 50)
train_loss: 0.8091543912887573, val_loss: 0.7988632321357727

('sgd', 0.001, 16, 100)
train_loss: 0.7795988917350769, val_loss: 0.771883487701416

('sgd', 0.001, 32, 10)
train_loss: 0.8235332369804382, val_loss: 0.8120079040527344

('sgd', 0.001, 32, 50)
train_loss: 0.7168553471565247, val_loss: 0.7142651081085205

('sgd', 0.001, 32, 100)
train_loss: 0.7858203649520874, val_loss: 0.7774022221565247

('sgd', 0.001, 64, 10)
train_loss: 0.7221429347991943, val_loss: 0.7247692942619324

('sgd', 0.001, 64, 50)
train_loss: 0.7596006393432617, val_loss: 0.7614612579345703

('sgd', 0.001, 64, 100)
train_loss: 0.7326852679252625, val_loss: 0.7349047660827637

('sgd', 0.01, 8, 10)
train_loss: 0.8011229038238525, val_loss: 0.7892894148826599

('sgd', 0.01, 8, 50)
train_loss: 0.6725385189056396, val_loss: 0.6707306504249573

('sgd', 0.01, 8, 100)
train_loss: 0.6966267824172974, val_loss: 0.6985408663749695

('sgd', 0.01, 16, 10)
train_loss: 0.8248516321182251, val_loss: 0.8121338486671448

('sgd', 0.01, 16, 50)
train_loss: 0.6790252327919006, val_loss: 0.6791327595710754

('sgd', 0.01, 16, 100)
train_loss: 0.7346706390380859, val_loss: 0.7359294295310974

('sgd', 0.01, 32, 10)
train_loss: 0.7017684578895569, val_loss: 0.7038564682006836

('sgd', 0.01, 32, 50)
train_loss: 0.8181830048561096, val_loss: 0.80753093957901

('sgd', 0.01, 32, 100)
train_loss: 0.6916932463645935, val_loss: 0.6921495795249939

('sgd', 0.01, 64, 10)
train_loss: 0.685678243637085, val_loss: 0.6870343685150146

('sgd', 0.01, 64, 50)
train_loss: 0.6948179602622986, val_loss: 0.696577250957489

('sgd', 0.01, 64, 100)
train_loss: 0.8010431528091431, val_loss: 0.7920913100242615

('sgd', 0.1, 8, 10)
train_loss: 0.6701696515083313, val_loss: 0.6642189621925354

('sgd', 0.1, 8, 50)
train_loss: 0.6617220640182495, val_loss: 0.6553482413291931

('sgd', 0.1, 8, 100)
train_loss: 0.6704155206680298, val_loss: 0.662369430065155

('sgd', 0.1, 16, 10)
train_loss: 0.6824122071266174, val_loss: 0.6821908950805664

('sgd', 0.1, 16, 50)
train_loss: 0.6569934487342834, val_loss: 0.649188220500946

('sgd', 0.1, 16, 100)
train_loss: 0.6588720679283142, val_loss: 0.6514921188354492

('sgd', 0.1, 32, 10)
train_loss: 0.6950290203094482, val_loss: 0.6943857073783875

('sgd', 0.1, 32, 50)
train_loss: 0.683339536190033, val_loss: 0.6833521723747253

('sgd', 0.1, 32, 100)
train_loss: 0.6542127728462219, val_loss: 0.6462395191192627

('sgd', 0.1, 64, 10)
train_loss: 0.7970464825630188, val_loss: 0.7843306660652161

('sgd', 0.1, 64, 50)
train_loss: 0.6612100005149841, val_loss: 0.656013548374176

('sgd', 0.1, 64, 100)
train_loss: 0.6563428044319153, val_loss: 0.6499046087265015


---------
BEST MODEL
('adam', 0.1, 64, 100)
val_loss: 0.538356363773346
---------

Run from 2023-04-17 17:12:34.143737
Results of Grid Search for logistic_regression: 
Order of Parameters: optimizer, learning_rate, batch_size, epochs 

('adam', 0.001, 8, 10)
train_loss: 0.7029293775558472, val_loss: 0.7062826752662659

('adam', 0.001, 8, 50)
train_loss: 0.7385472059249878, val_loss: 0.7534809112548828

('adam', 0.001, 8, 100)
train_loss: 0.7275215983390808, val_loss: 0.7467979192733765

('adam', 0.001, 16, 10)
train_loss: 0.7563886642456055, val_loss: 0.7369984984397888

('adam', 0.001, 16, 50)
train_loss: 0.6893971562385559, val_loss: 0.6876809597015381

('adam', 0.001, 16, 100)
train_loss: 0.690333366394043, val_loss: 0.6700019240379333

('adam', 0.001, 32, 10)
train_loss: 0.8274768590927124, val_loss: 0.8441741466522217

('adam', 0.001, 32, 50)
train_loss: 0.6858459115028381, val_loss: 0.6808614134788513

('adam', 0.001, 32, 100)
train_loss: 0.6974737644195557, val_loss: 0.6812530755996704

('adam', 0.001, 64, 10)
train_loss: 0.7471195459365845, val_loss: 0.7299779653549194

('adam', 0.001, 64, 50)
train_loss: 0.6939070224761963, val_loss: 0.6951041221618652

('adam', 0.001, 64, 100)
train_loss: 0.7008517980575562, val_loss: 0.6868208646774292

('adam', 0.01, 8, 10)
train_loss: 0.7126975655555725, val_loss: 0.7253578305244446

('adam', 0.01, 8, 50)
train_loss: 0.6777642369270325, val_loss: 0.6642327308654785

('adam', 0.01, 8, 100)
train_loss: 0.6877375841140747, val_loss: 0.6845727562904358

('adam', 0.01, 16, 10)
train_loss: 0.6807045340538025, val_loss: 0.6700841188430786

('adam', 0.01, 16, 50)
train_loss: 0.6754143238067627, val_loss: 0.659548819065094

('adam', 0.01, 16, 100)
train_loss: 0.6883702874183655, val_loss: 0.6854202747344971

('adam', 0.01, 32, 10)
train_loss: 0.687167227268219, val_loss: 0.6730837821960449

('adam', 0.01, 32, 50)
train_loss: 0.7175623774528503, val_loss: 0.7390403747558594

('adam', 0.01, 32, 100)
train_loss: 0.6710674166679382, val_loss: 0.6507235169410706

('adam', 0.01, 64, 10)
train_loss: 0.733035683631897, val_loss: 0.7413097620010376

('adam', 0.01, 64, 50)
train_loss: 0.6905041933059692, val_loss: 0.6897900104522705

('adam', 0.01, 64, 100)
train_loss: 0.6631367206573486, val_loss: 0.6333007216453552

('adam', 0.1, 8, 10)
train_loss: 0.6706401705741882, val_loss: 0.6402860283851624

('adam', 0.1, 8, 50)
train_loss: 0.6398276686668396, val_loss: 0.5734496712684631

('adam', 0.1, 8, 100)
train_loss: 0.6377322673797607, val_loss: 0.5225604772567749

('adam', 0.1, 16, 10)
train_loss: 0.6821233034133911, val_loss: 0.6707677245140076

('adam', 0.1, 16, 50)
train_loss: 0.6384764313697815, val_loss: 0.5674726963043213

('adam', 0.1, 16, 100)
train_loss: 0.6300992369651794, val_loss: 0.5393988490104675

('adam', 0.1, 32, 10)
train_loss: 0.7235475778579712, val_loss: 0.7371150255203247

('adam', 0.1, 32, 50)
train_loss: 0.6499338150024414, val_loss: 0.5998886823654175

('adam', 0.1, 32, 100)
train_loss: 0.6307497024536133, val_loss: 0.5463750958442688

('adam', 0.1, 64, 10)
train_loss: 0.7142956852912903, val_loss: 0.7248672246932983

('adam', 0.1, 64, 50)
train_loss: 0.6273332238197327, val_loss: 0.5310535430908203

('adam', 0.1, 64, 100)
train_loss: 0.6401891112327576, val_loss: 0.5760202407836914

('sgd', 0.001, 8, 10)
train_loss: 0.6924289464950562, val_loss: 0.6843972206115723

('sgd', 0.001, 8, 50)
train_loss: 0.7057638168334961, val_loss: 0.7095708847045898

('sgd', 0.001, 8, 100)
train_loss: 0.6861513257026672, val_loss: 0.680487871170044

('sgd', 0.001, 16, 10)
train_loss: 0.7337767481803894, val_loss: 0.741348385810852

('sgd', 0.001, 16, 50)
train_loss: 0.8701488375663757, val_loss: 0.8914157152175903

('sgd', 0.001, 16, 100)
train_loss: 0.695590078830719, val_loss: 0.6850014925003052

('sgd', 0.001, 32, 10)
train_loss: 0.7164245843887329, val_loss: 0.7035415172576904

('sgd', 0.001, 32, 50)
train_loss: 0.6890456080436707, val_loss: 0.6825778484344482

('sgd', 0.001, 32, 100)
train_loss: 0.7776308655738831, val_loss: 0.7909609079360962

('sgd', 0.001, 64, 10)
train_loss: 0.7476454377174377, val_loss: 0.7568399906158447

('sgd', 0.001, 64, 50)
train_loss: 0.7026042938232422, val_loss: 0.7051796913146973

('sgd', 0.001, 64, 100)
train_loss: 0.6895431280136108, val_loss: 0.6827737092971802

('sgd', 0.01, 8, 10)
train_loss: 0.7439768314361572, val_loss: 0.754396915435791

('sgd', 0.01, 8, 50)
train_loss: 0.6737801432609558, val_loss: 0.6507110595703125

('sgd', 0.01, 8, 100)
train_loss: 0.7134866118431091, val_loss: 0.7320760488510132

('sgd', 0.01, 16, 10)
train_loss: 0.6868483424186707, val_loss: 0.681787371635437

('sgd', 0.01, 16, 50)
train_loss: 0.686075747013092, val_loss: 0.6689261794090271

('sgd', 0.01, 16, 100)
train_loss: 0.6733030080795288, val_loss: 0.6495679020881653

('sgd', 0.01, 32, 10)
train_loss: 0.7240834832191467, val_loss: 0.7305677533149719

('sgd', 0.01, 32, 50)
train_loss: 0.7022064328193665, val_loss: 0.7059236764907837

('sgd', 0.01, 32, 100)
train_loss: 0.7070329189300537, val_loss: 0.7142753601074219

('sgd', 0.01, 64, 10)
train_loss: 0.6998293995857239, val_loss: 0.689517617225647

('sgd', 0.01, 64, 50)
train_loss: 0.7059376835823059, val_loss: 0.6919085383415222

('sgd', 0.01, 64, 100)
train_loss: 0.6891416907310486, val_loss: 0.6873021125793457

('sgd', 0.1, 8, 10)
train_loss: 0.6888105869293213, val_loss: 0.6829604506492615

('sgd', 0.1, 8, 50)
train_loss: 0.6619716882705688, val_loss: 0.6182558536529541

('sgd', 0.1, 8, 100)
train_loss: 0.6740167737007141, val_loss: 0.6551910042762756

('sgd', 0.1, 16, 10)
train_loss: 0.6834691166877747, val_loss: 0.6729452610015869

('sgd', 0.1, 16, 50)
train_loss: 0.6721227765083313, val_loss: 0.6516776084899902

('sgd', 0.1, 16, 100)
train_loss: 0.6894956231117249, val_loss: 0.6875730752944946

('sgd', 0.1, 32, 10)
train_loss: 0.6911892294883728, val_loss: 0.6694475412368774

('sgd', 0.1, 32, 50)
train_loss: 0.6831814050674438, val_loss: 0.6727107167243958

('sgd', 0.1, 32, 100)
train_loss: 0.6770746111869812, val_loss: 0.6604083776473999

('sgd', 0.1, 64, 10)
train_loss: 0.6971915364265442, val_loss: 0.6989541053771973

('sgd', 0.1, 64, 50)
train_loss: 0.6884753108024597, val_loss: 0.6860339641571045

('sgd', 0.1, 64, 100)
train_loss: 0.7122183442115784, val_loss: 0.7307729125022888


---------
BEST MODEL
('adam', 0.1, 8, 100)
val_loss: 0.5225604772567749
---------
